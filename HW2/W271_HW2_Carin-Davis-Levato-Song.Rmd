---
title: "**W271**-2 -- Spring 2016 -- **HW 2**"
author: "***Juanjo Carin, Kevin Davis, Ashley Levato, Minghu Song***"
date: "*February 10, 2016*"
output:
   pdf_document:
     fig_caption: yes
     toc: yes
numbersections: false
geometry: margin=1in
options: width=30
fontsize: 10pt
linkcolor: blue
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[LO,LE]{Carin, Davis, Levato, Song}
- \fancyhead[CE,CO]{W271 -- HW2 -- \leftmark}
- \fancyhead[RE,RO]{\rightmark}
- \fancyfoot[LO,LE]{UC Berkeley -- MIDS}
- \fancyfoot[CO,CE]{Spring semester 2016}
- \fancyfoot[RE,RO]{\thepage}
- \renewcommand{\headrulewidth}{0.5pt}
- \renewcommand{\footrulewidth}{0.5pt}


---

**********

# Data

**In the United States, a 401K is a type of retirement savings plan that is tied to a worker's place of employment. Employees that put money into a 401K enjoy certain tax benefits. Moreover, many employers have a policy of promoting 401K use, by matching some percentage of an employee's contributions. If an employer matches at, say, 50%, for every dollar that an employee puts into a 401K, the employer will put in another 50 cents.**

**The file `401k_w271.RData` contains data on 401k contributions that were filed with the IRS on form 5500. It was collected by Professor L. E. Papke and may have been further modified by the instructors to test your proficiency.**



**********

\pagebreak

# Exercises

**Complete the following exercises, following the best practices outlined in class. Place your answers in a written report (pdf, word, or jupyter notebook format) along with relevant R statements and output.**

**Load the `401k_w271.RData` dataset and look at the value of the function `desc()` to see what variables are included.**

```{r, echo = FALSE}
require(knitr, quietly = TRUE)
read_chunk('code/W271_HW2_Carin-Davis-Levato-Song.R')
opts_chunk$set(message = FALSE, warning = FALSE)
opts_chunk$set(fig.width = 4, fig.height = 3)
# Set path to data here (don't use setwd() inside a chunk!!!)
opts_knit$set(root.dir = './data')
```

```{r Libraries-Functions-Constants, echo = FALSE}
```

```{r Load_Data, echo = -c(1:4)}
```

## Question 1

**Your dependent variable will be `prate`, representing the fraction of a company's employees participating in its 401k plan. Because this variable is bounded between 0 and 1, a linear model without any transformations may not be the most ideal way to analyze the data, but we can still learn a lot from it. Examine the `prate` variable and comment on the shape of its distribution.**

```{r Question1-1, echo = -c(1:2)}
```

Based on the R output above, `prate` is one of the `r dim(desc)[1]` variables contained in the dataset. There are **`r dim(data)[1]` observations** of `prate`, and `r sum(is.na(data$prate))` of them correspond to `NA` values.

Its minimum and **maximum** values are `r frmt(min(data$prate), 1)` and **`r frmt(max(data$prate), 1)`**, respectively: the latter must correspond to an error, since a rate cannot be greater than $100\%$. A further analysis reveals that there are **`r length(data$prate[data$prate > 100])` observations** in which `prate` exceeds 100, so we should and shall **discard** them from our analysis.

Its **mean** and **median** values are `r frmt(mean(data$prate), 1)` and `r frmt(median(data$prate), 1)`, respectively.

The last two values in the output of the `stat.desc()` function used above show the results of a Shapiro-Wilk test, which indicates non-normality (but that test is not very conlusive with such a large sample).

The **excess kurtosis** (the kurtosis minus 3) is positive (`r frmt(kurtosis(data$prate), 2)`), which indicates that the distribution of the sample is **leptokurtic** (a more acute peak around the mean and fatter tails than the normal distribution). Its skewness is negative (`r frmt(skewness(data$prate), 2)`), which indicates that the distribution of the sample is **left-skewed** (it has a long left tail). The following Figures confirm both aspects.

```{r Question1-2, echo = FALSE, fig.width = 6, fig.height = 4.5, fig.cap = "Histogram of participation rate ($\\%$) in 401K plans of a company's employees (bin width = 5)"}
```

\label{hist-Q1}

Though hard to see (roughly $`r frmt(100*length(data$prate[data$prate == 100])/length(data$prate), 1)`\%$ of the observations correspond to the exact value of 100%), the histogram above reveals the one observation with a value of `r frmt(data$prate[data$prate > 100 & data$prate < 200], 1)` and the two observations with a value of `r frmt(max(data$prate), 1)`.

\pagebreak

```{r Question1-3, echo = FALSE, fig.width = 3.8, fig.height = 3/(4/3.8), fig.cap = "Approximate density plot of participation rate ($\\%$) in 401K plans of a company's employees "}
```

\label{density1-Q1}

If we omit the `r length(data$prate[data$prate > 100])` anomalous observations (which we'll do from now on), the (approximate) density plot looks like this (**not normal** at all: neither it is symmetrically distributed about the mean (`r frmt(mean(data$prate[data$prate <= 100]), digits = 1)`) nor it can take any positive or value):

```{r Question1-4, echo = FALSE, fig.cap = "Approximate density plot of participation rate ($\\%$) in 401K plans of a company's employees (excluding wrong values, higher than 100%)"}
```

\label{density2-Q1}



**********

\pagebreak

## Question 2

**Your independent variable will be `mrate`, the rate at which a company matches employee 401K contributions. Examine this variable and comment on the shape of its distribution.**

```{r Question2-1, echo = -c(1:2)}
```

As with `prate`, there are no `NA` values of `mrate`.

Its minimum and maximum values are `r frmt(min(data2$mrate), 1)` and `r frmt(max(data2$mrate), 1)` (these values, as those of `prate`, correspond to percentages).

Its **mean** and **median** values are `r frmt(mean(data2$mrate))` and `r frmt(median(data2$mrate))`, respectively.

Again, a Shapiro-Wilk test (see the last two values in the output of the `stat.desc()` function) indicates that the distribution is far from normal, but this kind of test is not conclusive whatsoever with with such a large sample.

The **excess kurtosis** (the kurtosis minus 3) is positive (`r frmt(kurtosis(data2$mrate), 2)`), which indicates that the distribution of the sample is **leptokurtic** (a more acute peak around the mean and fatter tails than the normal distribution). Its skewness is positive (`r frmt(skewness(data2$mrate), 2)`), which indicates that the distribution of the sample is **right-skewed** (it has a long right tail). The following Figures confirm both aspects.

\pagebreak

```{r Question2-2, echo = FALSE, fig.cap = "Histogram of companies' match rate ($\\%$) to their employees' 401K contributions (bin width = 0.1)"}
```

\label{hist-Q2}

```{r Question2-3, echo = FALSE, fig.cap = "Approximate density plot of companies' match rate ($\\%$) to their employees' 401K contributions"}
```

\label{density-Q2}



**********

\pagebreak

## Question 3 \label{Q3}

**Generate a scatterplot of `prate` against `mrate`. Then estimate the linear regression of `prate` on `mrate`. What slope coefficient did you get?**

```{r Question3-1, echo = FALSE, fig.cap = "Scatterplot of the participation rate ($\\%$) to 401K plans of a company's employees against the match rate ($\\%$) of that company to their employees' contributions"}
```

\label{scatter-Q3}

While a low match rate of a company may correspond to almost the whole range of employees' participation rates, higher match rates correspond to high participation rates, which seems to indicate the positive relationship between both variables.

```{r Question3-2, echo = c(1:3)}
```

\label{table-Q3}

As shown in `r I(pasteLabel("Table", tableCount, "table-Q3"))` above, **the slope coefficient is** $\mathbf{`r frmt(summary(model)$coefficients[2, 1])` \ (`r frmt(summary(model)$coefficients[2, 2])`)}$: a 1 percentage point increase in the match rate would correspond to an increase of almost `r frmt(summary(model)$coefficients[2, 1], digits = 0)` percentage points in the participation rate to 401K plans of the employees, which could indicate that more employees are willing to make this kind of investment when their companies promote 401K plans by matching a higher percentage of their own contributions.



**********

\pagebreak

## Question 4

**Is the assumption of zero-conditional mean realistic? Explain your evidence. What are the implications for your OLS coefficients?**

One of the ways to check this assumption is by plotting the residuals ($\hat{u}$) against the fitted values ($\hat{y}$), or even against the regressor when only one is used. That plot (below) shows that the residuals change with the fitted values (see the blue line; the smoother is far from flat), which suggests that the assumption may **not** be **realistic** (as well as a possible non-linear relationship between `mrate` and `prate`).

```{r Question4-1, echo = FALSE, fig.cap = "Scatterplot of fitted values of the regressand against the residuals"}
```

\label{fitted-residuals-Q4}

The implication is that the OLS coefficients are **biased**. But though the assumption of zero-conditional mean ($E(u|x)=0$) is not met, the assumption of zero mean ($E(u)=0$) and zero correlation ($Cov(x,u)=0$) seems realistic (i.e., the **exogeneity** assumption is met), so they may be **consistent** at least:

```{r Question4-2}
```



**********

\pagebreak

## Question 5

**Is the assumption of homoskedasticity realistic? Provide at least two pieces of evidence to support your conclusion. What are the implications for your OLS analysis?**

The funnel shape of `r I(pasteLabel("Figure", figCount, "fitted-residuals-Q4"))` suggests that the assumption of homoskedasticity is **not realistic**. based on the shape of the scatterplot: the variance of the residuals is much greater for lower values of $\hat{y}$ than it is for greater values.

\small

> The fact that about $`r frmt(100*length(data2$mrate[data2$mrate<1]) / length(data2$mrate), digits = 0)`\%$ of the data points in `r I(pasteLabel("Figure", figCount, "fitted-residuals-Q4"))` are concentrated in about one fifth of the x-axis (corresponding to `mrate` < 1) may make it look like there's more variation than is actually occurring. But there are more evidence of homoskedasticity...

\normalsize

A second piece of evidence comes from the analysis of the scale-location plot:

```{r Question5-1, echo = FALSE, fig.cap = "Scale-location plot of the regression model"}
```

\label{scale-location-Q5}

The smoother (the blue line in the figure above) should be flat, which is not the case.

\small

> We might also run a Breusch-Pagan test, but with such a large dataset it is very likely that the result will be significant even if the assumption of homoskedasticity were realistic.

> ```{r Question5-2}
> ```

\normalsize

The implications are that:

1. we cannot conclude that the OLS estimators have the smallest variance among *all linear*, unbiased estimators, and
2. we cannot compute the variance of our estimates of the coefficients, so we don't have a clear idea of their precision.

The use of heteroskedasticity robust standard errors (which we use in [Question 7](#Q7) or weighted least squares regression may be able to aid in estimating variance under the heteroskedasticity case; however they may necessitate other assumption to be met. Alternatively a transform (i.e., log or square root) could be applied to the variable to try and address the heteroskedasticity in running the OLS regression. 


**********

## Question 6

**Is the assumption of normal errors realistic? Provide at least two pieces of evidence to support your conclusion. What are the implications for your OLS analysis?**

```{r Question6-1, echo = FALSE, fig.cap = "Q-Q plot of the residuals of the regression"}
```

\label{QQplot-Q6}

<!--
```{r, echo = FALSE, fig.align = 'center'}
plot(model, which = 2, main = "Q-Q plot of the Standardized Residuals", sub = "", caption = "")
```
-->

The left tail of the Q-Q plot twists counterclockwise, while the right tail twists clockwise, which indicates that the distribution is left-skewed (and not normal).

Let's also check some descriptive statistics of the residuals. As seen below, the **excess kurtosis** (the kurtosis minus 3) is positive (`r frmt(kurtosis(model$residuals), 2)`), which indicates that the distribution of the sample is **leptokurtic** (a more acute peak around the mean and fatter tails than the normal distribution), and the skewness is negative (`r frmt(skewness(model$residuals), 2)`), which indicates that the distribution of the sample is **left-skewed** (it has a long left tail). Both details indicate a non-normal distribution.

```{r Question6-2, echo = TRUE}
```

\pagebreak

Let's plot the approximate distribution of the residuals:

```{r Question6-3, echo = FALSE, fig.cap = "Density plot of the residuals of the regression"}
```

\label{density-Q6}

\small

> As expected, the density plot of the residuals has a shape similar to the explained variable; see `r I(pasteLabel("Figure", figCount, "density2-Q1"))`.

\normalsize

Finally, the last two values in the output of the `stat.desc()` function used in the previous page showed the results of a Shapiro-Wilk test, which indicates non-normality (but that test is not very conlusive with such a large sample). We can confirm that result using the specific R function:

```{r Question6-4, echo = TRUE}
```

The implication of non-normality of the errors is two-fold:

1. the OLS estimators wiil not have the smallest variance among *all* (linear or not) unbiased estimators (assuming the Gauss-Markov assumptions are met, which is not the case here), and 
2. the *t* and *F* statistics do not have *t* and *F* distributions, respectively, which makes hypothesis testing impossible.

But if the Gauss-Markov assumptions were met (which is not the case; for example, the zero conditional mean and the homoskedasticity assumptions are broken), we could still use assume that the (standardized) OLS estimators are *asymptotically normally distributed*. I.e., we might use the $t_{n-k-1}$ distribution for hypothesis testing, since our sample is quite large (`r dim(data2)[1]` observations after discarding the `r length(data$prate[data$prate > 100])` ones): the actual significance level we may get will be close to the one we chose (5% or any other).



**********

\pagebreak

## Question 7 \label{Q7}

**Based on the above considerations, what is the standard error of your slope coefficient?**

The standard error of the slope coefficient (which is `r frmt(summary(model)$coefficients[2, 1])`) is shown in `r I(pasteLabel("Table", tableCount, "table-Q3"))`: **`r frmt(summary(model)$coefficients[2, 2])`**.

```{r Question7-1, echo = 3}
```

But the variance of the residuals is not constant, so we should use **robust standard errors** instead (which is actually smaller for the slope coefficient: **`r frmt(coeftest(model, vcovHC(model))[2, 2])`**.

```{r Question7-2, echo = 1}
```

\label{table-Q7}



**********

## Question 8

**Is the effect you find statistically significant, and is it practically significant?**

As shown in `r I(pasteLabel("Table", tableCount, "table-Q7"))`, the effect of `mrate` on `prate` is highly **statistically significante**: the exact *p*-value is $p = `r formatC(coeftest(model, vcovHC(model))[2, 4], format = "e", digits = 1)`$ (lower than what we got in [Question 3](#Q3): according to `r I(pasteLabel("Table", tableCount, "table-Q3"))`, $p = `r formatC(summary(model)$coefficient[2, 4], format = "e", digits = 1)`$ when we do not use heteroskedasticity-robust standard errors).

If a company matches one percentage point more of its employees' contributions, the contribution of those employees to 401k is, on average, about `r frmt(summary(model)$coefficients[2, 1], digits = 1)` percentage points higher, which is also a **practically significant** effect.

<!-- See `r I(pasteLabel("Figure", figCount, "density2-Q1"))` -->

**********

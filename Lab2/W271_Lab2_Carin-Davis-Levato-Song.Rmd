---
title: "**W271**-2 -- Spring 2016 -- **Lab 2**"
author: "***Juanjo Carin, Kevin Davis, Ashley Levato, Minghu Song***"
date: "*March 7, 2016*"
output:
   pdf_document:
     fig_caption: yes
     toc: yes
numbersections: false
geometry: margin=1in
options: width=30
fontsize: 10pt
linkcolor: blue
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[LO,LE]{Carin, Davis, Levato, Song}
- \fancyhead[CE,CO]{W271 -- Lab 2}
- \fancyhead[RE,RO]{\leftmark}
- \fancyfoot[LO,LE]{UC Berkeley -- MIDS}
- \fancyfoot[CO,CE]{Spring semester 2016}
- \fancyfoot[RE,RO]{\thepage}
- \renewcommand{\headrulewidth}{0.5pt}
- \renewcommand{\footrulewidth}{0.5pt}
---

**********

\pagebreak

```{r, echo = FALSE}
require(knitr, quietly = TRUE)
read_chunk('code/W271_Lab2_Carin-Davis-Levato-Song.R')
opts_chunk$set(message = FALSE, warning = FALSE)
opts_chunk$set(fig.width = 4, fig.height = 3)
# Set path to data here (don't use setwd() inside a chunk!!!)
opts_knit$set(root.dir = './data')
```

```{r Libraries-Functions-Constants, echo = FALSE}
```

# Question 1: Broken Rulers

**You have a ruler of length 1 and you choose a place to break it using a uniform probability distribution. Let random variable $\mathbf{X}$ represent the length of the left piece of the ruler. $\mathbf{X}$ is distributed uniformly in $\mathbf{[0, 1]}$. You take the left piece of the ruler and once again choose a place to break it using a uniform probability distribution. Let random variable $\mathbf{Y}$ be the length of the left piece from the second break.**

1. **Find the conditional expectation of $\mathbf{Y}$ given $\mathbf{X}$, $\mathbf{E(Y|X)}$.**

$f_X = U(0,1)$ and $f_{Y|X}=U(0,X)$ (because the maximum length of the second left piece cannot be greater than the length of the first left piece). As we know, the probability density function for a variable $Z$ that follows a uniform distribution $U(a,b)$ is:

$$f_Z(z) = \begin{cases}
\cfrac{1}{b-a} & a \leq z \leq b \\
0 & \text{otherwise}
\end{cases}$$

So:

$$f_{Y|X}(y|x) = \begin{cases}
\cfrac{1}{x} & 0 \leq z \leq x \\
0 & \text{otherwise}
\end{cases}$$

And:

$$\mathbf{\color{red}{E(Y|X)}} = \displaystyle\int_\mathbb{Y} y \cdot f_{Y|X}(y|x) \cdot dy = \displaystyle\int_{y=0}^x y \cdot \cfrac{1}{x} \cdot dy = \cfrac{1}{x}\left[ \cfrac{y^2}{2}\right]_{y=0}^x = \cfrac{x^2}{2x} = \mathbf{\color{red}{\cfrac{x}{2}}}$$

We'll make use of some simulations through this Question to confirm the results.

\small

> ```{r Question1-1-1, echo = -c(1:2), fig.cap = "Histogram and approximate pdf of $X$ and $Y$"}
> ```

\pagebreak

> ```{r Question1-1-2, echo = TRUE, fig.cap = "Histogram and approximate pdf of $Y$ conditional on $X$ for a given value of $X$ (0.2)"}
> ```

\normalsize


\pagebreak

2. **Find the unconditional expectation of $\mathbf{Y}$. One way to do this is to apply the law of iterated expectations, which states that $\mathbf{E(Y) = E(E(Y|X))}$. The inner expectation is the conditional expectation computed above, which is a function of $\mathbf{X}$. The outer expectation finds the expected value of this function.**

$$\mathbf{\color{red}{E(Y)}} = E\left[E(Y|X)\right] = \displaystyle\int_\mathbb{X} E(Y|X) \cdot f_X(x) \cdot dx = \int_{x=0}^1 \cfrac{x}{2} \cdot 1 \cdot dx = \left[ \cfrac{x^2}{4}\right]_{x=0}^1 = \mathbf{\color{red}{\cfrac{1}{4} = 0.25}}$$


3. **Write down an expression for the joint probability density function of $\mathbf{X}$ and $\mathbf{Y}$, $\mathbf{f_{X,Y}(x,y)}$.**

$\mathbf{\color{red}{f_{X,Y}(x,y)}} = f_{Y|X}(y|x) \cdot f_X(x) = \mathbf{\color{red}{\begin{cases}
\mathbf{\cfrac{1}{x}} & \mathbf{x \in (0,1), y \in (0,x)} \\
\mathbf{0} & \textbf{otherwise}
\end{cases}}}$

Let's check that this is a valid joint *pdf*:

$$\displaystyle\int_{\mathbb{X}} \displaystyle\int_{\mathbb{Y}} f_{X,Y}(x,y) \cdot dx \cdot dy = \displaystyle\int_{x=0}^1 \displaystyle\int_{y=0}^x\cfrac{1}{x} \cdot dy \cdot dx = \displaystyle\int_{x=0}^1 \left[\cfrac{y}{x}\right]_{y=0}^x dx = \displaystyle\int_{x=0}^1 dx = \left[x\right]_{x=0}^1 = 1$$

Simulations:

\small

> ```{r Question1-3, echo = TRUE, fig.width = 6, fig.height = 4.5, fig.cap = "Approximate joint pdf of $X$ and $Y$"}
> ```

\normalsize


\pagebreak

4. **Find the conditional probability density function of $\mathbf{X}$ given $\mathbf{Y}$, $\mathbf{f_{X|Y}}$.**

In order to find $f_{X|Y}$ we need the marginal pdf of $Y$.

$$f_Y(y) = \displaystyle\int_{\mathbb{X}} f_{X,Y}(x,y) \cdot dx = \displaystyle\int_{y=0}^x \cfrac{1}{x} \cdot dx = \displaystyle\int_{x=y}^1 \cfrac{dx}{x} = \left[\log(x)\right]_{x=y}^1 dx = -\log(y)=\log\left(\cfrac{1}{y}\right)$$

This result confirms what the shape of $f_Y(y)$ in Figure 1 suggested.

\small

> ```{r Question1-4, echo = TRUE, fig.cap = "Approximate pdf of $Y$ conditional on $X$ for two values of $X$"}
> ```

\normalsize

$$\mathbf{\color{red}{f_{X|Y}(x|y)}}=\cfrac{f_{X,Y}(x,y)}{f_Y(y)} = \mathbf{\color{red}{\cfrac{-1}{x \cdot \log(y)}}}$$


\pagebreak

5. **Find the expectation of $\mathbf{X}$, given that $\mathbf{Y}$ is $\mathbf{1/2}$, $\mathbf{E(X|Y = 1/2)}$.**

$$\begin{aligned}\mathbf{\color{red}{E(X|Y=1/2)}} &= \displaystyle\int_\mathbb{X} x \cdot f_{X|Y}(x|y=1/2) \cdot dx = \displaystyle\int_{x=1/2}^1 x \cdot \left(\cfrac{-1}{x \cdot \log(1/2)}\right) \cdot dx \\
& = \cfrac{1}{\log(2)} \displaystyle\int_{x=1/2}^1 dx = \cfrac{1}{\log(2)}\left[x \right]_{x=1/2}^1 = \mathbf{\color{red}{\cfrac{1}{2\cdot\log(2)} = `r frmt(1/(2*log(2)))`}}\end{aligned}$$

\small

> ```{r Question1-5, echo = TRUE}
> ```

\normalsize



**********

\pagebreak

# Question 2: Investing

**Suppose that you are planning an investment in three different companies. The payoff per unit you invest in each company is represented by a random variable. $\mathbf{A}$ represents the payoff per unit invested in the first company, $\mathbf{B}$ in the second, and $\mathbf{C}$ in the third. $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ are independent of each other. Furthermore, $\mathbf{Var(A) = 2Var(B) = 3Var(C)}$.**

**You plan to invest a total of one unit in all three companies. You will invest amount $\mathbf{a}$ in the first company, $\mathbf{b}$ in the second, and $\mathbf{c}$ in the third, where $\mathbf{a,b, c \in [0,1]}$ and $\mathbf{a + b + c = 1}$. Find, the values of $\mathbf{a}$, $\mathbf{b}$, and $\mathbf{c}$ that minimize the variance of your total payoff.**

Let's call $P$ the total payoff:

$$Var(P) = Var(aA+bB+cC) = a^2Var(A) + b^2Var(B) + c^2Var(C)$$

because $A$, $B$, and $C$ are independent of each other. And since $Var(A) = 2Var(B) = 3Var(C)$, we can derive that:

$$Var(P) = Var(A)\left(a^2 + \cfrac{b^2}{2} + \cfrac{c^2}{3}\right)$$

We want to:

$$\begin{matrix}
\text{minimize} & P(a,b,c)\\ 
\text{subject to} & g(a,b,c) = 0
\end{matrix}$$

where $g(a,b,c)=a+b+c-1$, so $g(a,b,c)=0$ is our constraint.

Using the Lagrange multiplier method, we can define:

$$\mathcal{L}(a,b,c,\lambda) = P(a,b,c)-\lambda \cdot g(a,b,c)$$

So to find our local minima we need to solve:

$$\nabla_{a,b,c,\lambda}\mathcal{L} = 0$$

$$\left( \cfrac{\partial\mathcal{L}}{\partial a}, \cfrac{\partial\mathcal{L}}{\partial b}, \cfrac{\partial\mathcal{L}}{\partial c}, \cfrac{\partial\mathcal{L}}{\partial \lambda}\right) = \left(2a-\lambda,b-\lambda,\cfrac{2}{3}c-\lambda,-(a+b+c-1)\right)=\mathbf{0}$$

$$\Rightarrow \left\{\begin{matrix}
2a-\lambda=0\\ 
b-\lambda=0\\ 
2c/3-\lambda=0\\ 
a+b+c-1=0
\end{matrix}\right.
\Rightarrow \left\{\begin{matrix}
a=\lambda/2\\ 
b=\lambda\\ 
c=3\lambda/2\\ 
\cfrac{\lambda}{2}+\lambda+\cfrac{3\lambda}{2}=3\lambda=1
\end{matrix}\right.
\Rightarrow \left\{\begin{matrix}
\mathbf{\color{red}{a=\cfrac{1}{6}=`r formatC(1/6)`}}\\ 
\mathbf{\color{red}{b=\cfrac{1}{3}=`r formatC(1/3)`}}\\ 
\mathbf{\color{red}{c=\cfrac{1}{2}=`r formatC(1/2)`}}
\end{matrix}\right.$$

Let's prove the result in R:

\small

```{r Question2, echo = -c(1:2)}
```

\normalsize



**********

\pagebreak

# Question 3: Turtles

**Next, suppose that the lifespan of a species of turtle follows a uniform distribution over $\mathbf{[0, \theta]}$. Here, parameter $\mathbf{\theta}$ represents the unknown maximum lifespan. You have a random sample of $\mathbf{n}$ individuals, and measure the lifespan of each individual $i$ to be $\mathbf{y_i}$.**

1. **Write down the likelihood function, $\mathbf{l(\theta)}$ in terms of $\mathbf{y_1, y_2, \dots, y_n}$.**

<!--
http://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/MLE.pdf
http://math.stackexchange.com/questions/49543/maximum-estimator-method-more-known-as-mle-of-a-uniform-distribution
https://en.wikipedia.org/wiki/Maximum_likelihood
-->

$$l(\theta;y_1,\dots,y_n)=f(y_1,\dots,y_n|\theta)=\displaystyle\prod_{i=1}^{n}f(y_i|\theta)=
\begin{cases}
\displaystyle\prod_{i=1}^{n}\cfrac{1}{\theta}=\theta^{-n} & 0\leq y_i \leq \theta \\
0 & \text{otherwise}
\end{cases}$$

<!--We can also use the **log-likelihood** function (if the MLE exists, it will be the same regardless of whether we maximize the likelihood or the log-likelihood function):

$$\ln l(\theta;y_1,\dots,y_n)=f(y_1,\dots,y_n|\theta)=\displaystyle\sum_{i=1}^{n}\ln f(x_i|\theta)=
\begin{cases}
n \cdot \ln \left(\cfrac{1}{\theta} \right)=-n \cdot \ln \theta & 0\leq y_i \leq \theta \\
0 & \text{otherwise}
\end{cases}$$-->


2. **Based on the previous result, what is the maximum-likelihood estimator for $\mathbf{\theta}$?**

The MLE of $\theta$ must be a value of $\theta$ for which $\theta \geq y_i$ for $i=1,\dots,n$ and which maximizes $1/\theta^n$ among all such values. I.e., the maximum value of $y_i$ within the sample.

$$\hat{\theta}_{ml} = \text{arg}\max\limits_{\theta \in \Theta}\ \hat{l}(\theta;y_1,\dots,y_n) = \max\{y_1,\dots,y_n\}$$


3. **Let $\mathbf{\hat{\theta}_{ml}}$ be the maximum likelihood estimator above. For the simple case that $\mathbf{n = 1}$, what is the expectation of $\mathbf{\hat{\theta}_{ml}}$, given $\mathbf{\theta}$?**

$$E\left(\hat{\theta}_{ml} \mid \theta\right) = E(y_1) = E(y) = \displaystyle\int_{y=0}^{\theta}\cfrac{y}{\theta}\cdot dy = \left[\cfrac{y^2}{2\theta}\right]_{y=0}^{\theta} = \cfrac{\theta}{2}$$


4. **Is the maximum likelihood estimator biased?**

Yes, it is:

$$E\left(\hat{\theta}_{ml}\right) - \theta = \cfrac{\theta}{2} \neq 0$$


5. **For the more general case that $\mathbf{n \geq 1}$, what is the expectation of $\mathbf{\hat{\theta}_{ml}}$?**

<!--https://www.physicsforums.com/threads/expectation-of-an-uniform-distribution-maximum-likelihood-estimator.380389/-->

Without loss of generality, let's suppose the individual $n$ is the one with the maximum lifespan among the sample, i.e., $y_n \geq y_i$ for $i=1,\dots,n-1$. Call $z$ that maximum value of $y_i$.

$$E[max\{y_1,\dots,y_n\}]=E(y_n)=E(z)=\displaystyle\int_{z=0}^{\theta}z \cdot f(z) dz$$

But what is the distribution of $z$?

$$F(z) = \Pr(y_n \leq z) = \Pr(y_1\leq z \cap \dots \cap y_n \leq z) \stackrel{\text{i.i.d}}{=} \displaystyle\prod_{i=1}^n\Pr(y_i\leq z)=\left(\cfrac{z}{\theta}\right)^n \Rightarrow f(z) = \cfrac{nz^{n-1}}{\theta^n}$$

$$E[max\{y_1,\dots,y_n\}] = \cfrac{n}{\theta^n}\displaystyle\int_{z=0}^\theta z^ndz=\cfrac{n}{\theta^n}\left[\cfrac{z^{n+1}}{n+1}\right]_{z=0}^\theta=\cfrac{n}{n+1}\cdot\theta$$

(Which confirms the previous result, for $n=1$.)

6. **Is the maximum likelihood estimator consistent?**

It is:

$$\Pr\left(|\hat{\theta}_{ml} - \theta|>\varepsilon\right) = \Pr\left(\cfrac{\theta}{n+1}>\varepsilon\right) \to 0 \text{ as } n \to \infty$$

<!--(Even though $\Pr\left(max\{y_1, \dots, y_n\} < \theta\right) = 1$.)-->

\small

> ```{r Question3, echo = c(2:11), fig.cap = "Trend of the MLE of $\\theta$ depending on the sample size"}
> ```

\normalsize



**********

\pagebreak

# Question 4: CLM 1

## Background

**The file `WageData2.csv` contains a dataset that has been used to quantify the impact of education on wage. One of the reasons we are proving another wage-equation exercise is that this area by far has the most (and most well-known) applications of instrumental variable techniques, the endogenity problem is obvious in this context, and the datasets are easy to obtain.**

## The Data

**You are given a sample of 1000 individuals with their wage, education level, age, working experience, race (as an indicator), father's and mother's education level, whether the person lived in a rural area, whether the person lived in a city, IQ score, and two potential instruments, called $\mathbf{z1}$ and $\mathbf{z2}$.**

**The dependent variable of interest is `wage` (or its transformation), and we are interested in measuring "return" to education, where return is measured in the increase (hopefully) in wage with an additional year of education.**

## Question 4.1

**Conduct an univariate analysis (using tables, graphs, and descriptive statistics found in the last 7 lectures) of all of the variables in the dataset.**

**Also, create two variables: (1) natural log of wage (name it `logWage`) (2) square of experience (name it `experienceSquare`)**

```{r Question4-1-1, echo = F}
```

```{r Question4-1-1a, results = 'asis', echo = F, fig.cap = "Summary Statistics of Wage Data"}
```

\pagebreak

```{r Question4-1-2, echo = F, fig.width = 8, fig.height = 3.5, fig.cap = "Histogram of Wage , IQ Score, and log(Wage)"}
```

```{r Question4-1-3, echo = F, fig.width = 8, fig.height = 3.5, fig.cap = "Histogram of Race , Rural, City, Z1, and Z2"}
```

```{r Question4-1-4, echo = F, fig.width = 8, fig.height = 10, fig.cap = "Histogram of Education, Expereince, Age, Father's Education, and Mother's Education"}
```

\pagebreak


## Question 4.2

**Conduct a bivariate analysis (using tables, graphs, descriptive statistics found in the last 7 lectures) of `wage` and `logWage` and all the other variables in the datasets.**

```{r Question4-2-1, results = 'asis', echo = F, fig.cap = "Correlation Matrix for Wage Variables"}
```
    

```{r Question4-2-2, echo = F, fig.width = 8, fig.height = 10, fig.cap = "Scatterplot of Wage Against Variables of Interest"}
```

```{r Question4-2-3, echo = F, fig.width = 8, fig.height = 10, fig.cap = "Scatterplot of log(Wage) Against Variables of Interest"}
```

\pagebreak


## Question 4.3

**Regress *log(wage)* on education, experience, age, and raceColor.**

1. **Report all the estimated coefficients, their standard errors, t-statistics, F-statistic of the regression, $\mathbf{R^2}$, $\mathbf{R_{adj}^2}$ , and degrees of freedom.**

```{r Question4-3-1, results='asis', echo = F, fig.cap = "Regression Output for Question 4.3"}
```

2. **Explain why the degrees of freedom takes on the specific value you observe in the regression output.**

The overall degrees of freedom (F(`r round(summary(model)$fstatistic[2],0)`, `r round(summary(model)$fstatistic[3],0)`) is one smaller than we might expect otherwise. This is because the parameter for age is a linearly dependent combination of the other parameters and it's coefficient cannot be estimated. R automatically removes this variable from the model, and thus our regression output reflects the degrees of freedom for a model with 3 parameters rather than 4. Thus estimating this model is equivalent to estimating $log(wage) = education + experience + raceColor$.


```{r Question4-3-2, results='asis', echo = F, fig.cap = "Equivalent Regression Output for Question 4.3"}
```


```{r Question4-3-3, echo = F, fig.width = 8, fig.height = 6, fig.cap = "Regression Diagnostic Plots for Question 4.3"}
```

\pagebreak

3. **Describe any unexpected results from your regression and how you would resolve them (if the intent is to estimate return to education, condition on race and experience).**

The inability to estimate the age coefficient is an unexpected result. Looking at the correlation matrix from above, we don't see that any variable is perfectly correlated with $log(wage)$. However, if we take the correlation of $age$ with the sum of $education$ and $experience$, we see that $age$ is perfectly correlated with the two variables. Since the model already incorporates all of the information contained in the age variable, we can simply remove it from the model and not lose anything. 

4. **Interpret the coefficient estimate associated with education.**

Holding other covariates constant, a one year increase in $education$ was associated with a statistically significant increase in $log(wage)$, ($\beta$ = `r formatC(round(model2$coefficients[2], 3), format = 'f', digits = 3)`, std. error = `r round(coeftest(model2, vcov = vcovHC)[6],3)`,  t = `r round(coeftest(model2, vcov=vcovHC)[10],3)`, p < .001). This effect also represents a practically significant wage return on education. 


5. **Interpret the coefficient estimate associated with experience.**

Holding other covariates constant, a one year increase in $experience$ was associated with a statistically significant increase in $log(wage)$, ($\beta$ = `r formatC(round(coeftest(model2, vcov = vcovHC)[3],3), format = 'f', digits = 3)`, std. error = `r round(coeftest(model2, vcov = vcovHC)[7],3)`,  t = `r round(coeftest(model2, vcov=vcovHC)[11],3)`, p < .001). This effect also represents a practically significant wage return on experience. 


## Question 4.4

**Regress *log(wage)* on education, experience, experienceSquare, and race-Color.**

```{r Question4-4-1, results = 'asis', echo = F, fig.cap = "Regression Output for Question 4.4"}
```

1. **Plot a graph of the estimated effect of experience on wage.**

```{r Question4-4-2, echo = F, fig.cap = "Estimated Effect of Experience on Wages"}
```

2. **What is the estimated effect of experience on wage when experience is 10 years?**

The estimated effect of experience at 10 years on wages is 10 $\cdot$ `r round(coeftest(model3, vcovHC)[3], 3)` + 10^2 $\cdot$ `r round(coeftest(model3, vcovHC)[4],3)` = `r round(10 * coeftest(model3, vcovHC)[3] + 10**2*coeftest(model3, vcovHC)[4], 3)`

\pagebreak

## Question 4.5 {#question45}

**Regress `logWage` on education, experience, `experienceSquare`, `raceColor`, `dad_education`, `mom_education`, rural, city.**

```{r Question4-5-1, results = 'asis', echo = F, fig.cap = "Regression Output for Question 4.5"}
```

1. **What are the number of observations used in this regression? Are missing values a problem? Analyze the missing values, if any, and see if there is any discernible pattern with wage, education, experience, and raceColor.**

There are 723 observations in the regression. A comparison of the original data and the observations in the regression reveals that the missing observations tend to have lower wages and education and are less likely to live in the city. The missing observations have more experience, and are more likely to be non-white and live in rural areas. 

```{r Question4-5-2, results = 'asis', echo = F, fig.cap="Summary Statistics for Missing Observations"}
```

2. **Do you just want to "throw away" these observations?**

The clear pattern of missing values representing more rural, non-white observations with lower education indicates that these values should not simply be discarded from the model. In order to accurately assess the effect of covariates these values are important for the regression. 

3. **How about blindly replace all of the missing values with the average of the observed values of the corresponding variable? Rerun the original regression using all of the observations?**

```{r Question4-5-3, results = 'asis', echo = c(1:5), fig.cap= "Regression Output for Mean Imputation"}
```

4. **How about regress the variable(s) with missing values on education, experience, and raceColor, and use this regression(s) to predict (i.e., "impute") the missing values and then rerun the original regression using all of the observations?**

```{r Question4-5-4, results = 'asis', echo = c(1:7), fig.cap= "Regression Output for Value Imputation"}
```

\pagebreak

5. **Compare the results of all of these regressions. Which one, if at all, would you prefer?**

```{r Question4-5-5, results = 'asis', echo = F, fig.cap= "Comparing Regression Model with Imputed Models"}
```

Comparing the regression results, it's worth noting that the magnitude and significance of the coefficients are broadly similar. However, given that the values that we imputed are for covariates that are neither statistically nor practically significant, I would prefer a model that used all of the observations and did not include the covariates for parent's education. 


## Question 4.6

1. **Consider using $\mathbf{z_1}$ as the instrumental variable (IV) for education. What assumptions are needed on $\mathbf{z_1}$ and the error term (call it, $\mathbf{u}$)?**

Formally, we need to assume that the instrumental variable is uncorrelated with the error term. Ie.

$$ Cov(z_{1j}, u) = 0 \ \forall j=1,2,\dots,k $$

2. **Suppose $\mathbf{z_1}$ is an indicator representing whether or not an individual lives in an area in which there was a recent policy change to promote the importance of education. Could $\mathbf{z_1}$ be correlated with other unobservables captured in the error term?**

Yes. Living in an area with recent education policy changes could correlate to a number of variables that relate to the earnings in that area, such as a person's trust in educational and governmental institutions, or the willingness of people to pay taxes to support education spending. As with most instrumental variables, the assumption that it is truly uncorrelated with the error term can always be questioned, although it may represent an improvement in estimating coefficients over straightforward OLS.  

3. **Using the same specification as that in [Question 4.5](#question45), estimate the equation by 2SLS, using both $\mathbf{z_1}$ and $\mathbf{z_2}$ as instrument variables. Interpret the results. How does the coefficient estimate on education change?**

```{r Question4-6-1, results = 'asis', echo = c(1:4), fig.cap = "First Stage Regression Output"}
```


Note that $z_1$ is not correlated with education, and is thus not an appropriate instrumental variable to estimate the effect of education on wages. 

\pagebreak

```{r Question4-6-3, results = 'asis', echo = F, fig.cap = "Second Stage Regression Output"}
```


The results of the regression show that the magnitude of the covariate coefficients is similar using the instrumental variable approach. The coefficient for education has dramatically decreased and is no longer statistically significant. 


**********

\pagebreak

# Question 5: CLM 2

**The dataset, `wealthy candidates.csv`, contains candidate level electoral data from a developing country. Politically, each region (which is a subset of the country) is divided in to smaller electoral districts where the candidate with the most votes wins the seat. This dataset has data on the financial wealth and electoral performance (voteshare) of electoral candidates. We are interested in understanding whether or not wealth is an electoral advantage. In other words, do wealthy candidates fare better in elections than their less wealthy peers?**

1. **Begin with a parsimonious, yet appropriate, specification. Why did you choose this model? Are your results statistically significant? Based on these results, how would you answer the research question? Is there a linear relationship between wealth and electoral performance?**

In order to decide on a parsimonious model and discover any potential issues with data quality or variable distributions, we will first explore summary statistics and conduct a univariate analysis. 

```{r Question5-1-1, results = 'asis', echo = F}
```

\pagebreak

```{r Question5-1-2, echo = F, fig.width = 8, fig.height = 6, fig.cap = "Histogram of Urban, Literacy, Voteshar, and Wealth Variables"}
```

```{r Question5-1-3, echo = F, fig.cap="Bar Plot of Wealth and No Wealth"}
```

```{r Question5-1-4, echo = F, fig.cap="Bar Plot of Region"}
```

```{r Question5-1-5, echo = F, fig.width = 5, fig.cap="Histogram of log(Wealth) Among Those With Wealth"}
```

```{r Question5-1-6, echo = F, fig.width = 8, fig.height = 6, fig.cap= "Scatterplots of Voteshare against Urban, Literacy, Absolute Wealth, and log(Absolute Wealth)"}
```

Examining the absolute_wealth and log(absolute_wealth) variables reveals that a large number of observations have the same value. It seems likely that this value codes for having zero absolute wealth. Because we interested in the effect of wealth on voteshare, it seems reasonable to look at this effect among those with wealth greater than zero. By creating a factor variable representing wealth greater than zero, the equation for our parsimonious model will be

$$voteshare = \beta_0 + \beta_1 log(wealth) + \beta_2 hasWealth$$

\pagebreak

```{r Question5-1-7, results='asis', echo = F}
```

The parsimonious model reveals a very weak positive effect of wealth on voteshare. The effect is not of practical significance and suggests that there may not be a linear relationship between wealth and voteshare. 

2. **A team-member suggests adding a quadratic term to your regression. Based on your prior model, is such an addition warranted? Add this term and interpret the results. Do wealthier candidates fare better in elections?**

Theoretically, the addition of a quadratic seems questionable because it suggests there is a point at which the returns to wealth stop increasing. Even if the quadratic model resulted in an improvement in the R^2 value, it may be difficult to justify the inclusion of a quadratic term without a theoretical reason. 

```{r Question5-1-8, results='asis', echo = F}
```

\pagebreak

3. **Another team member suggests that it is important to take into account the fact that different regions have different electoral contexts. In particular, the relationship between candidate wealth and electoral performance might be different across states. Modify your model and report your results. Test the hypothesis that this addition is not needed.**

First, we will look at how the variables of interest are distributed across regions. 

```{r Question5-3-1, echo = F, fig.width = 6, fig.height = 5, fig.cap="Histograms of Wealth By Region"}
```

\pagebreak


```{r Question5-3-2, echo = F, fig.width = 6, fig.height = 4, fig.cap="Histograms of Voteshare By Region"}
```

The histograms of wealth by region and voteshare by region do seem to reveal some genuine structural differences between the regions. The first region has much greater wealth inequality, with a large share of the observations having no wealth and greater average wealth among the wealthy than regions 2 and 3. The voteshare histogram for region one also suggests a difference in election structure, as voteshare percentages tend to be clustered between 15% and 40%, while regions two and three have relatively uniform voteshare distributions between 5% and 60%. 

\pagebreak


```{r Question5-3-3, echo = F, fig.width = 6, fig.height = 4, fig.cap="Scatterplots of Voteshare Against Wealth By Region"}
```

```{r Question5-3-4, echo = F, fig.width = 6, fig.height = 4, fig.cap="Scatterplots of Voteshare Against Having Wealth By Region"}
```

The scatter plots of voteshare against the wealth variables reveal that is all regions, there does not seem to be a strong linear relationship between wealth and voteshare. There seems to be a level of $log(wealth)$ around 10, below which there are few observations, but once that floor is cleared, there doesn't seem to be a strong relationship between additional wealth and additional voteshare. 

\pagebreak


```{r Question5-3-5, results='asis', echo = F}
```

```{r Question5-3-6, results='asis', echo = F}
```

The regression output shows that the model including regions accounts for a greater share of the overall variation than the parsimonious model, with the difference between the two models being practically and statistically significant (F(`r model_comp[2,3]`, `r model_comp[2,1]`) = `r round(model_comp[2,5],3)`, p < 0.001). This supports the notion that the structural differences in voteshare and wealth between the regions are important to take into account when estimating the effect of wealth on voteshare.  

4. **Return to your parsimonious model. Do you think you have found a causal and unbiased estimate? Please state the conditions under which you would have an unbiased and causal estimates. Do these conditions hold?**

In order for OLS to make unbiased estimates of the OLS parameters, 4 conditions must hold:

1. The parameters have a linear relationship
2. Observations are observed through random sampling
3. There is no perfect collinearity among the independent variables
4. There is zero conditional mean of the error term and zero correlation of the error with any of the independent variables

And in order for our coefficients to represent causal effects, changes in the independent variable must not change the expected value of the error term. Ie.

$$\frac{\partial u}{\partial x} = 0$$

Two primary issues that lead me to conclude that we have not found a causal and unbiased estimate. The first is the lack of apparent linearity in the relationship between voteshare and wealth. This is supported by the low explanatory power of our model, as well as the scatter plot of wealth and voteshare that seems to imply non linear factors may be more important than linear. The second issue is that wealth is probably not exogenous, but are in fact correlated with a host of measures that are contained in the error term. For example, having more money may be correlated with being a well-known public figure, which may influence voteshare. 



5. **Someone proposes a difference in difference design. Please write the equation for such a model. Under what circumstances would this design yield a causal effect?**

Supposing we had data collected at two different time periods, with $\lambda_t$ being an indicator for the fixed time effect, $a_i$ representing the time-fixed effect of being individual i, and $u_{it}$ representing the time-variable portion of the error term. Then our population model would look like: 

$$ voteshare_{it} = \beta_0 + \lambda_t + \beta_{1}log(absolute\_wealth)_{it} + \beta_{2}hasWealth_{it} + a_{i} + u_{it} $$

The first difference model would be obtained by subtracting the model at $t_2$ from $t_1$ and assuming parallel trends, ie that trends would have continued in the same way at $t_2$ as in $t_1$ in the absence of treatment.

$$ \Delta voteshare_{i} = \lambda_t + \beta_{1} \Delta log(absolute\_wealth)_i + \beta_{2} \Delta hasWealth_{i} + \Delta u_{i}$$

In order for the first difference model to be causal, changes in the difference in the independent variables must not change the expectation of changes in the error term. 

$$\frac{\partial \Delta u}{\partial \Delta x} = 0$$



**********

\pagebreak

# Question 6: CLM 3

**Your analytics team has been tasked with analyzing aggregate revenue, cost and sales data, which have been provided to you in the R workspace/data frame `retailSales.Rdata`.**

**Your task is two fold. First, your team is to develop a model for predicting (forecasting) revenues. Part of the model development documentation is a backtesting exercise where you train your model using data from the first two years and evaluate the model's forecasts using the last two years of data.**

**Second, management is equally interested in understanding variables that might affect revenues in support of management adjustments to operations and revenue forecasts. You are also to identify factors that affect revenues, and discuss how useful management's planned revenue is for forecasting revenues.**

**Your analysis should address the following:**

+ **Exploratory Data Analysis: focus on bivariate and multivariate relationships.**

+ **Be sure to assess conditions and identify unusual observations.**

First we explore the whole dataset.

> ```{r Question6-1, echo = -c(1:2)}
> ```

The dataset contains `r frmt(dim(data)[1], 0)` observations of `r dim(data)[2]` variables. `r sum(sapply(data, is.factor))` of them are categorical (``r paste(names(data)[sapply(data, is.factor)], collapse=", ")``), and `Year` should also be considered as categorical, since there are data from only `r max(data$Year) - min(data$Year) + 1` years (from `r min(data$Year)` to `r max(data$Year)`).

> ```{r Question6-2}
> ```

We also notice (from the output of `summary`) that some of the variables (all of them numerical) has a high number of `NA`s, the same in all cases (`r sum(is.na(data$Revenue))`, i.e., $`r frmt(sum(is.na(data$Revenue))/dim(data)[1]*100, 2)`\%$ of the total number of observations). Do the `NA`s appear in the same observations for all those variables? Yes, they do.

> ```{r Question6-3}
> ```

And the amount of `NA`s per category is roughly the same for all categorical values (or at least there are non-missing data for all categories; below we just show the percentage per category for three of the numerical variables).

> ```{r Question6-4, echo = c(1:3)}
> ```

So we can ommit all those missing observations (reducing our sample size to `r dim(data)[1] - sum(is.na(data$Revenue))`), and continue with a further analysis of the numerical variables:

> ```{r Question6-5}
> ```

All numerical variables are right-skewed, with long right tails (i.e., with several observations more than 2 standard deviations far from the mean), especially the ones corresponding to aggregate---non-unit---results.

> ```{r Question6-6, echo = FALSE, fig.width = 6, fig.height = 4.5, fig.cap = "Histogram of all non-categorical variables in the dataset"}
> ```

Below we show the correlation matrix of the numerical variables, as well as two different representations of the scatterplot matrix (where we've used a sample of the data of size 500 because the plotting functions consume a lot of resources; that's why the correlations shown in the second Figure, only approximate, differ from the ones shown right below). As we might have expected, the correlations between `Revenue`, `Planned.revenue`, `Product.cost`, and `Gross.profit` (i.e., the aggregate values), as well as those between `Unit.cost`, `Unit.price`, and `Unit.sale.price` (i.e., the values per unit), are positive and very high. `Quantity` is negatively correlated with the unitary variables (but that correlation is negligible in absolute value), and is moderately correlated ($\rho \simeq 0.5$) with the aggregate values.

> ```{r Question6-7, echo = 1, fig.width = 6, fig.height = 6, fig.cap = "Scatterplot matrix of a sample of the dataset"}
> ```

> ```{r Question6-8, echo = FALSE, fig.width = 6, fig.height = 6, fig.cap = "Scatterplot matrix of a sample of the dataset (with correlations)"}
> ```

\pagebreak

We have to divide the dataset into two separate ones, to train and evaluate the model. Now we can convert `Year` back to a numerical variable (subtracting 2004 so the baseline is 0; that will make the intercept more intuitive when including `Year` in the regression model).

> ```{r Question6-9}
> ```

Not all products appear in both periods so some re-factoring is needed, otherwise the `predict.lm` function will not work when it encounters a category not present before:

> ```{r Question6-10}
> ```

There are some variables that are calculated from `Revenue` (or vice versa) so including them in any regression model would lead to a perfect fit. In particular, `Gross.profit` = `Revenue` - `Product.cost`. And `Revenue` should be equal to `Unit.sale.price` times `Quantity`, though this is not always the case, and there are differences in many cases ($`r frmt(100*sum(data$Unit.sale.price * data$Quantity != data$Revenue) / dim(data)[1], 1)` \%$ of the total number of observations).

> ```{r Question6-11}
> ```

But not only that: **in order to make predictions with our models, we need to use the values of the independent variables for which we want our predictions to be made; it seems very unlikely that the company will know in advance any of the numerical variables, same as the revenues are unknown. The only exception is `Planned.revenue`, which already is a forecast (so we will use it alone, with no other variable, to evaluate how useful it is forecasting).** (There might be other exceptions, such as `Unit.cost` and `Unit.price`, which could already be set by the beginning of the year and not vary over the year, but just in case we won't use them.)

> ```{r Question6-12}
> ```

Let's continue the EDA, now with the categorical variables (the ones that can be used for forecasting). We explored several combinations, but for the sake of brevity we'll include here only the most interesting ones, starting with `Year` and `Order.method.type`:

\pagebreak

> ```{r Question6-13, echo = FALSE}
> ```

> ```{r Question6-14, echo = FALSE, fig.height = 4.5, fig.width = 6, fig.cap = "Bar chart of aggregate revenues per Order method type and Year"}
> ```

> ```{r Question6-15}
> ```

> ```{r Question6-15-1, results='asis'}
> ```

We could use both the *Bayesian Information Criterion* (BIC) and the *Akaike Information Criterion* (AIC) to compare different models (preferring the one with the lowest BIC or AIC). Both metrics introduce a penalty term for the number of parameters in the model (larger in BIC), though they don't answer exactly the same question: the AIC tries to select the model that most adequately describes an unknown, high dimensional reality, i.e., it is aimed for situations where we don't necessarily think there's a model so much as a bunch of effects of different sizes, and we want to get good prediction errors. On the contrary, the BIC tries to find the "true" model among the set of candidates. For that reason, the AIC may be better for our objective.

But these metrics evaluate the models (based on the data used to build them), not their predictions: to evaluate their accuracy, we'll use the RMSE and the MAE (*Mean Absolute Error*; the MAPE cannot be used here because `Revenue` = 0 in some cases). The greater the difference between them, the greater the variance in the prediction errors in the test set.

> ```{r Question6-16}
> ```

\pagebreak

Let's continue exploring `Retailer.country` and `Product.line`:

> ```{r Question6-17, echo = FALSE, fig.height = 4.5, fig.width = 6, fig.cap = "Bar chart of aggregate revenues per Retailer country (only 6 shown) and Product Line"}
> ```

The revenue per product line seems quite similar for all countries, so the interaction may be light: we'll explore both models anyway.

> ```{r Question6-18}
> ```


> ```{r Question6-19}
> ```

> ```{r Question6-20}
> ```

The BIC is larger in the 3rd model (including the interaction terms) because the reduction in the errors is not compensated by the extra complexity introduced by adding the interaction terms. Regarding to the forecasts, the RMSE has kept decreasing with each new model, but the MAE has increased. This means that the prediction errors are higher on average, but their variance is smaller (the RMSE penalizes observations that differ largely from the real value).

+ **Is the change in the average revenue different from 95 cents when the planned revenue increases by $1?**

> ```{r Question6-21, echo = c(1:5), fig.cap = "Planned Revenue vs. Revenue (observed and predicted) in 2006 and 2007"}
> ```

> ```{r Question6-22}
> ```

Based on its RMSE and MAE, this model is similar or even better than the ones we have built so far.

> ```{r Question6-23}
> ```

As shown below, the change in the average revenue is significantly different from $0.95 when the revenue increases by $1 (while the *F* statistic of the exact value, which is quite close to $0.95, has a *p* value equal to 1):

+ **Explain what interaction terms in your model mean in context supported by data visualizations.**

For example, in the 1st model (where the baseline is 2004 and orders arrived by e-mail), an additional year increases the revenues, on average, by about $2,871, and an order that arrived by fax is, also on average, about $4,038 higher. But if both conditions happen at the same time (i.e., an order by fax in 2005), the effect of the subsequent year disappears (because the interaction term estimate is $-\$2,875$). Similarly, in the 2nd model, where the baseline is Australia and Camping Equipment, the revenue from Germany is, on average, about $2,513 lower, and one for Golf Equipment is $5,623 lower. Putting both conditions together would decrease revenue by $8,136, but when the interaction term is included (in the 3rd model), the mentioned estimates change to $-\$4,472$ and $-\$1,925$, respectively, while the interaction term is $-\$6,077$; i.e., the decrease is revenue is much larger, almost by $12,500.

+ **Give two reasons why the OLS model coefficients may be biased and/or not consistent, be specific.**

+ **Propose (but do not actually implement) a plan for an IV approach to improve your forecasting model.**

Let's say we are able to know in advance the quantity of each product that will be purchased in the coming years, and thus incorporate that variable in our model and make predictions based on that new model. Another variable that would certainly help forecast the revenue is the **demand** (i.e., not what is actually sold but what customers are willing to buy, but may end up not doing so for a number of reasons: they prefer other alternatives, they don't have the money, they don't even know our product, etc.). Demand cannot be included as a explanatory variable (i.e., it's omitted from our model), so it's included in the error term. Since the actual quantity that is bought is correlated with demand, our estimates may be biased. A possible IV (that has no effect on ` revenue`---after controlling for `quantity`---, is not correlated with demand, and is correlated with `quantity`) could be a metric of the **brand awareness**. Or an indicator of the economic situation.

**********

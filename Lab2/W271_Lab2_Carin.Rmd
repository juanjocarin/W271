---
title: "**W271**-2 -- Spring 2016 -- **Lab 2**"
author: "***Juanjo Carin, Kevin Davis, Ashley Levato, Minghu Song***"
date: "*March 7, 2016*"
output:
   pdf_document:
     fig_caption: yes
     toc: yes
numbersections: false
geometry: margin=1in
options: width=30
fontsize: 10pt
linkcolor: blue
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[LO,LE]{Carin, Davis, Levato, Song}
- \fancyhead[CE,CO]{W271 -- Lab 2}
- \fancyhead[RE,RO]{\leftmark}
- \fancyfoot[LO,LE]{UC Berkeley -- MIDS}
- \fancyfoot[CO,CE]{Spring semester 2016}
- \fancyfoot[RE,RO]{\thepage}
- \renewcommand{\headrulewidth}{0.5pt}
- \renewcommand{\footrulewidth}{0.5pt}
---

**********

\pagebreak

```{r, echo = FALSE}
require(knitr, quietly = TRUE)
read_chunk('code/W271_Lab2_Carin.R')
opts_chunk$set(message = FALSE, warning = FALSE)
opts_chunk$set(fig.width = 4, fig.height = 3)
# Set path to data here (don't use setwd() inside a chunk!!!)
opts_knit$set(root.dir = './data')
```

```{r Libraries-Functions-Constants, echo = FALSE}
```

# Question 1: Broken Rulers

**You have a ruler of length 1 and you choose a place to break it using a uniform probability distribution. Let random variable $\mathbf{X}$ represent the length of the left piece of the ruler. $\mathbf{X}$ is distributed uniformly in $\mathbf{[0, 1]}$. You take the left piece of the ruler and once again choose a place to break it using a uniform probability distribution. Let random variable $\mathbf{Y}$ be the length of the left piece from the second break.**

1. **Find the conditional expectation of $\mathbf{Y}$ given $\mathbf{X}$, $\mathbf{E(Y|X)}$.**

$f_X = U(0,1)$ and $f_{Y|X}=U(0,X)$ (because the maximum length of the second left piece cannot be greater than the length of the first left piece). As we know, the probability density function for a variable $Z$ that follows a uniform distribution $U(a,b)$ is:

$$f_Z(z) = \begin{cases}
\cfrac{1}{b-a} & a \leq z \leq b \\
0 & \text{otherwise}
\end{cases}$$

So:

$$f_{Y|X}(y|x) = \begin{cases}
\cfrac{1}{x} & 0 \leq z \leq x \\
0 & \text{otherwise}
\end{cases}$$

And:

$\mathbf{\color{red}{E(Y|X)}} = \displaystyle\int_\mathbb{Y} y \cdot f_{Y|X}(y|x) \cdot dy = \displaystyle\int_{y=0}^x y \cdot \cfrac{1}{x} \cdot dy = \cfrac{1}{x}\left[ \cfrac{y^2}{2}\right]_{y=0}^x = \cfrac{x^2}{2x} = \mathbf{\color{red}{\cfrac{x}{2}}}$

We'll make use of some simulations through this Question to confirm the results.

\small

> ```{r Question1-1-1, echo = -c(1:2), fig.cap = "Histogram and approximate pdf of $X$ and $Y$"}
> ```

\pagebreak

> ```{r Question1-1-2, echo = TRUE, fig.cap = "Approximate pdf of $Y$ conditional on $X$ for a given value of $X$ (0.2)"}
> ```

\normalsize

\pagebreak

2. **Find the unconditional expectation of $\mathbf{Y}$. One way to do this is to apply the law of iterated expectations, which states that $\mathbf{E(Y) = E(E(Y|X))}$. The inner expectation is the conditional expectation computed above, which is a function of $\mathbf{X}$. The outer expectation finds the expected value of this function.**

$\mathbf{\color{red}{E(Y)}} = E\left[E(Y|X)\right] = \displaystyle\int_\mathbb{X} E(Y|X) \cdot f_X(x) \cdot dx = \int_{x=0}^1 \cfrac{x}{2} \cdot 1 \cdot dx = \left[ \cfrac{x^2}{4}\right]_{x=0}^1 = \mathbf{\color{red}{\cfrac{1}{4} = 0.25}}$


3. **Write down an expression for the joint probability density function of $\mathbf{X}$ and $\mathbf{Y}$, $\mathbf{f_{X,Y}(x,y)}$.**

$\mathbf{\color{red}{f_{X,Y}(x,y)}} = f_{Y|X}(y|x) \cdot f_X(x) = \mathbf{\color{red}{\begin{cases}
\mathbf{\cfrac{1}{x}} & \mathbf{x \in (0,1), y \in (0,x)} \\
\mathbf{0} & \textbf{otherwise}
\end{cases}}}$

Let's check that this is a valid joint *pdf*:

$\displaystyle\int_{\mathbb{X}} \displaystyle\int_{\mathbb{Y}} f_{X,Y}(x,y) \cdot dx \cdot dy = \displaystyle\int_{x=0}^1 \displaystyle\int_{y=0}^x\cfrac{1}{x} \cdot dy \cdot dx = \displaystyle\int_{x=0}^1 \left[\cfrac{y}{x}\right]_{y=0}^x dx = \displaystyle\int_{x=0}^1 dx = \left[x\right]_{x=0}^1 = 1$

Simulations:

\small

> ```{r Question1-3, echo = TRUE, fig.width = 6, fig.height = 4.5, fig.cap = "Approximate joint pdf of $X$ and $Y$"}
> ```

\normalsize

\pagebreak

4. **Find the conditional probability density function of $\mathbf{X}$ given $\mathbf{Y}$, $\mathbf{f_{X|Y}}$.**

In order to find $f_{X|Y}$ we need the marginal pdf of $Y$.

$f_Y(y) = \displaystyle\int_{\mathbb{X}} f_{X,Y}(x,y) \cdot dx = \displaystyle\int_{y=0}^x \cfrac{1}{x} \cdot dx = \displaystyle\int_{x=y}^1 \cfrac{dx}{x} = \left[\log(x)\right]_{x=y}^1 dx = -\log(y)=\log\left(\cfrac{1}{y}\right)$

This result confirms what the shape of $f_Y(y)$ in Figure 1 suggested.

\small

> ```{r Question1-4, echo = TRUE, fig.cap = "Approximate pdf of $Y$ conditional on $X$ for two values of $X$"}
> ```

\normalsize

$\mathbf{\color{red}{f_{X|Y}(x|y)}}=\cfrac{f_{X,Y}(x,y)}{f_Y(y)} = \mathbf{\color{red}{\cfrac{-1}{x \cdot \log(y)}}}$


\pagebreak

5. **Find the expectation of $\mathbf{X}$, given that $\mathbf{Y}$ is $\mathbf{1/2}$, $\mathbf{E(X|Y = 1/2)}$.**

$\begin{aligned}\mathbf{\color{red}{E(X|Y=1/2)}} &= \displaystyle\int_\mathbb{X} x \cdot f_{X|Y}(x|y=1/2) \cdot dx = \displaystyle\int_{x=1/2}^1 x \cdot \left(\cfrac{-1}{x \cdot \log(1/2)}\right) \cdot dx \\
& = \cfrac{1}{\log(2)} \displaystyle\int_{x=1/2}^1 dx = \cfrac{1}{\log(2)}\left[x \right]_{x=1/2}^1 = \mathbf{\color{red}{\cfrac{1}{2\cdot\log(2)} = `r frmt(1/(2*log(2)))`}}\end{aligned}$

\small

> ```{r Question1-5, echo = TRUE}
> ```

\normalsize



**********

\pagebreak

# Question 2: Investing

**Suppose that you are planning an investment in three different companies. The payoff per unit you invest in each company is represented by a random variable. $\mathbf{A}$ represents the payoff per unit invested in the first company, $\mathbf{B}$ in the second, and $\mathbf{C}$ in the third. $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ are independent of each other. Furthermore, $\mathbf{Var(A) = 2Var(B) = 3Var(C)}$.**

**You plan to invest a total of one unit in all three companies. You will invest amount $\mathbf{a}$ in the first company, $\mathbf{b}$ in the second, and $\mathbf{c}$ in the third, where $\mathbf{a,b, c \in [0,1]}$ and $\mathbf{a + b + c = 1}$. Find, the values of $\mathbf{a}$, $\mathbf{b}$, and $\mathbf{c}$ that minimize the variance of your total payoff.**

Let's call $P$ the total payoff:

$$Var(P) = Var(aA+bB+cC) = a^2Var(A) + b^2Var(B) + c^2Var(C)$$

because $A$, $B$, and $C$ are independent of each other. And since $Var(A) = 2Var(B) = 3Var(C)$, we can derive that:

$$Var(P) = Var(A)\left(a^2 + \cfrac{b^2}{2} + \cfrac{c^2}{3}\right)$$

We want to:

$$\begin{matrix}
\text{minimize} & P(a,b,c)\\ 
\text{subject to} & g(a,b,c) = 0
\end{matrix}$$

where $g(a,b,c)=a+b+c-1$, so $g(a,b,c)=0$ is our constraint.

Using the Lagrange multiplier method, we can define:

$$\mathcal{L}(a,b,c,\lambda) = P(a,b,c)-\lambda \cdot g(a,b,c)$$

So to find our local minima we need to solve:

$$\nabla_{a,b,c,\lambda}\mathcal{L} = 0$$

$$\left( \cfrac{\partial\mathcal{L}}{\partial a}, \cfrac{\partial\mathcal{L}}{\partial b}, \cfrac{\partial\mathcal{L}}{\partial c}, \cfrac{\partial\mathcal{L}}{\partial \lambda}\right) = \left(2a-\lambda,b-\lambda,\cfrac{2}{3}c-\lambda,-(a+b+c-1)\right)=\mathbf{0}$$

$$\Rightarrow \left\{\begin{matrix}
2a-\lambda=0\\ 
b-\lambda=0\\ 
2c/3-\lambda=0\\ 
a+b+c-1=0
\end{matrix}\right.
\Rightarrow \left\{\begin{matrix}
a=\lambda/2\\ 
b=\lambda\\ 
c=3\lambda/2\\ 
\cfrac{\lambda}{2}+\lambda+\cfrac{3\lambda}{2}=3\lambda=1
\end{matrix}\right.
\Rightarrow \left\{\begin{matrix}
\mathbf{\color{red}{a=\cfrac{1}{6}}}\\ 
\mathbf{\color{red}{b=\cfrac{1}{3}}}\\ 
\mathbf{\color{red}{c=\cfrac{1}{2}}}
\end{matrix}\right.$$

Let's prove the result in R:

\small

```{r Question2, echo = -c(1:2)}
```

\normalsize



**********

\pagebreak

# Question 3: Turtles

**Next, suppose that the lifespan of a species of turtle follows a uniform distribution over $\mathbf{[0, \theta]}$. Here, parameter $\mathbf{\theta}$ represents the unknown maximum lifespan. You have a random sample of $\mathbf{n}$ individuals, and measure the lifespan of each individual $i$ to be $\mathbf{y_i}$.**

1. **Write down the likelihood function, $\mathbf{l(\theta)}$ in terms of $\mathbf{y_1, y_2, \dots, y_n}$.**

2. **Based on the previous result, what is the maximum-likelihood estimator for $\mathbf{\theta}$?**

3. **Let $\mathbf{\hat{\theta_{ml}}}$ be the maximum likelihood estimator above. For the simple case that $\mathbf{n \geq 1}$, what is the expectation of $\mathbf{\hat{\theta_{ml}}}$, given $\mathbf{\theta}$?**

4. **Is the maximum likelihood estimator biased?**

5. **For the more general case that $\mathbf{n \geq 1}$, what is the expectation of $\mathbf{\hat{\theta_{ml}}}$?**

6. **Is the maximum likelihood estimator consistent?**



**********

\pagebreak

# Question 4: CLM 1

## Background

**The file `WageData2.csv` contains a dataset that has been used to quantify the impact of education on wage. One of the reasons we are proving another wage-equation exercise is that this area by far has the most (and most well-known) applications of instrumental variable techniques, the endogenity problem is obvious in this context, and the datasets are easy to obtain.**

## The Data

**You are given a sample of 1000 individuals with their wage, education level, age, working experience, race (as an indicator), father's and mother's education level, whether the person lived in a rural area, whether the person lived in a city, IQ score, and two potential instruments, called $\mathbf{z1}$ and $\mathbf{z2}$.**

**The dependent variable of interest is `wage` (or its transformation), and we are interested in measuring "return" to education, where return is measured in the increase (hopefully) in wage with an additional year of education.**

## Question 4.1

**Conduct an univariate analysis (using tables, graphs, and descriptive statistics found in the last 7 lectures) of all of the variables in the dataset.**

**Also, create two variables: (1) natural log of wage (name it `logWage`) (2) square of experience (name it `experienceSquare`)**

## Question 4.2

**Conduct a bivariate analysis (using tables, graphs, descriptive statistics found in the last 7 lectures) of `wage` and `logWage` and all the other variables in the datasets.**

## Question 4.3

**Regress *log(wage)* on education, experience, age, and raceColor.**

1. **Report all the estimated coefficients, their standard errors, t-statistics, F-statistic of the regression, $\mathbf{R^2}$, $\mathbf{R_{adj}^2}$ , and degrees of freedom.**

2. **Explain why the degrees of freedom takes on the specific value you observe in the regression output.**

3. **Describe any unexpected results from your regression and how you would resolve them (if the intent is to estimate return to education, condition on race and experience).**

4. **Interpret the coefficient estimate associated with education.**

5. **Interpret the coefficient estimate associated with experience.**

## Question 4.4

**Regress *log(wage)* on education, experience, experienceSquare, and race-Color.**

1. **Plot a graph of the estimated effect of experience on wage.**

2. **What is the estimated effect of experience on wage when experience is 10 years?**

## Question 4.5 {#question45}

**Regress `logWage` on education, experience, `experienceSquare`, `raceColor`, `dad_education`, `mom_education`, rural, city.**

1. **What are the number of observations used in this regression? Are missing values a problem? Analyze the missing values, if any, and see if there is any discernible pattern with wage, education, experience, and raceColor.**

2. **Do you just want to "throw away" these observations?**

3. **How about blindly replace all of the missing values with the average of the observed values of the corresponding variable? Rerun the original regression using all of the observations?**

4. **How about regress the variable(s) with missing values on education, experience, and raceColor, and use this regression(s) to predict (i.e., "impute") the missing values and then rerun the original regression using all of the observations?**

5. **Compare the results of all of these regressions. Which one, if at all, would you prefer?**

## Question 4.6

1. **Consider using $\mathbf{z_1}$ as the instrumental variable (IV) for education. What assumptions are needed on $\mathbf{z_1}$ and the error term (call it, $\mathbf{u}$)?**

2. **Suppose $\mathbf{z_1}$ is an indicator representing whether or not an individual lives in an area in which there was a recent policy change to promote the importance of education. Could $\mathbf{z_1}$ be correlated with other unobservables captured in the error term?**

3. **Using the same specification as that in [Question 4.5](#question45), estimate the equation by 2SLS, using both $\mathbf{z_1}$ and $\mathbf{z_2}$ as instrument variables. Interpret the results. How does the coefficient estimate on education change?**



**********

\pagebreak

# Question 5: CLM 2

**The dataset, `wealthy candidates.csv`, contains candidate level electoral data from a developing country. Politically, each region (which is a subset of the country) is divided in to smaller electoral districts where the candidate with the most votes wins the seat. This dataset has data on the financial wealth and electoral performance (voteshare) of electoral candidates. We are interested in understanding whether or not wealth is an electoral advantage. In other words, do wealthy candidates fare better in elections than their less wealthy peers?**

1. **Begin with a parsimonious, yet appropriate, specification. Why did you choose this model? Are your results statistically significant? Based on these results, how would you answer the research question? Is there a linear relationship between wealth and electoral performance?**

2. **A team-member suggests adding a quadratic term to your regression. Based on your prior model, is such an addition warranted? Add this term and interpret the results. Do wealthier candidates fare better in elections?**

3. **Another team member suggests that it is important to take into account the fact that different regions have different electoral contexts. In particular, the relationship between candidate wealth and electoral performance might be different across states. Modify your model and report your results. Test the hypothesis that this addition is not needed.**

4. **Return to your parsimonious model. Do you think you have found a causal and unbiased estimate? Please state the conditions under which you would have an unbiased and causal estimates. Do these conditions hold?**

5. **Someone proposes a difference in difference design. Please write the equation for such a model. Under what circumstances would this design yield a causal effect?**



**********

\pagebreak

# Question 6: CLM 3

**Your analytics team has been tasked with analyzing aggregate revenue, cost and sales data, which have been provided to you in the R workspace/data frame `retailSales.Rdata`.**

**Your task is two fold. First, your team is to develop a model for predicting (forecasting) revenues. Part of the model development documentation is a backtesting exercise where you train your model using data from the first two years and evaluate the model's forecasts using the last two years of data.**

**Second, management is equally interested in understanding variables that might affect revenues in support of management adjustments to operations and revenue forecasts. You are also to identify factors that affect revenues, and discuss how useful management's planned revenue is for forecasting revenues.**

**Your analysis should address the following:**

+ **Exploratory Data Analysis: focus on bivariate and multivariate relationships.**

+ **Be sure to assess conditions and identify unusual observations.**

+ **Is the change in the average revenue different from 95 cents when the planned revenue increases by $1?**

+ **Explain what interaction terms in your model mean in context supported by data visualizations.**

+ **Give two reasons why the OLS model coefficients may be biased and/or not consistent, be specific.**

+ **Propose (but do not actually implement) a plan for an IV approach to improve your forecasting model.**


**********

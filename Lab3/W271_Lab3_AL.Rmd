---
title: "**W271**-2 -- Spring 2016 -- **Lab 3**"
author: "***Juanjo Carin, Kevin Davis, Ashley Levato, Minghu Song***"
date: "*April 22, 2016*"
output:
   pdf_document:
     fig_caption: yes
     fig_height: 3
     fig_width: 4
     toc: yes
     toc_depth: 2
numbersections: false
geometry: margin=1in
options: width=30
fontsize: 10pt
linkcolor: blue
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[LO,LE]{Carin, Davis, Levato, Song}
- \fancyhead[CE,CO]{W271 -- Lab 3}
- \fancyhead[RE,RO]{\leftmark}
- \fancyfoot[LO,LE]{UC Berkeley -- MIDS}
- \fancyfoot[CO,CE]{Spring semester 2016}
- \fancyfoot[RE,RO]{\thepage}
- \renewcommand{\headrulewidth}{0.5pt}
- \renewcommand{\footrulewidth}{0.5pt}
---

**********

### Instructions

* Thoroughly analyze the given dataset or data series. Detect any anomalies in each of the variables. Examine if any of the variables that may appear to be top- or bottom-coded.
* Your report needs to include a comprehensive graphical analysis
* Your analysis needs to be accompanied by detailed narrative. Just printing a bunch of graphs and econometric results will likely receive a very low score.
* Your analysis needs to show that your models are valid (in statistical sense).
* Your rationale of using certian metrics to choose models need to be provided. Explain the validity / pros / cons of the metric you use to choose your "best" model.
* Your rationale of any decisions made in your modeling needs to be explained and supported with empirical evidence.
* All the steps to arrive at your final model need to be shown and explained clearly.
* All of the assumptions of your final model need to be thoroughly tested and explained and shown to be valid. Don't just write something like, "the plot looks reasonable", or "the plot looks good", as different people interpret vague terms like "reasonable" or "good" differently.

\pagebreak

```{r, echo = FALSE, warning = FALSE}
require(knitr, quietly = TRUE)
read_chunk('code/W271_Lab3_AL.R')
opts_chunk$set(message = FALSE, warning = FALSE)
opts_chunk$set(fig.width = 4, fig.height = 3)
# Set path to data here (don't use setwd() inside a chunk!!!)
opts_knit$set(root.dir = './data', global.par = TRUE)
```

```{r Libraries-Functions-Constants, echo = FALSE}
```

# Part 1

## Modeling House Values

**In Part 1, you will use the data set `houseValue.csv` to build a linear regression model, which includes the possible use of the instrumental variable approach, to answer a set of questions interested by a philanthropist group. You will also need to test hypotheses using these questions.**

**The philanthropist group hires a think tank to examine the relationship between the house values and neighborhood characteristics. For instance, they are interested in the extent to which houses in neighbhorhood with desirable features command higher values. They are specifically interested in environmental features, such as proximity to water body (i.e. lake, river, or ocean) or air quality of a region.**

**The think tank has collected information from tens of thousands of neighborhoods throughout the United States. They hire your group as contractors, and you are given a small sample and selected variables of the original data set collected to conduct an initial, proof-of-concept analysis. Many variables, in their original form or transfomed forms, that can explain the house values are included in the dataset. Analyze each of these variables as well as different combinations of them very carefully and use them (or a subset of them), in its original or transformed version, to build a linear regression model and test hypotheses to address the questions. Also address potential (statistical) issues that may be casued by omitted variables.**

**********

\pagebreak

**********

\pagebreak

# Part 2

## Modeling and Forecasting a Real-World Macroeconomic / Financial time series

**Build a time-series model for the series in `lab3_series02.csv`, which is extracted from a real-world macroeconomic/financial time series, and use it to perform a 36-step ahead forecast. The periodicity of the series is purposely not provided. Possible models include AR, MA, ARMA, ARIMA, Seasonal ARIMA, GARCH, ARIMA-GARCH, or Seasonal ARIMA-GARCH models.**


We argue that an seasonal-ARIMA-GARCH model is the best choice. We explored this timeseries data and evaluted possible model types including AR, MA, ARMA, ARIMA, Seasonal ARIMA, GARCH, ARIMA-GARCH, or Seasonal ARIMA-GARCH models. A sARIMA-GARCH model was identified as the best fitting model because the time-series is not stationary in the mean or variance and is conditional heteroskedasstic which makes modeling the data using AR, MA, ARMA, ARIMA, or SARIMA models not good fits by themselves. We also note a statistically significant lags and possible seasonal trends in the data around 14 or 16. If this data is daily data this lag could be approximately mid month or every two weeks.Therefore, the combination sARIMA-GARCH model should produe the best fit for the data. 

First we load the data and conduct some exploratory analysis. This includes plotting the timeseries and examining key characteristics that can help us elimate unlikly modeling choices instead of testing all possible model types.  

```{r ex2-load, echo = -c(1:2)}
```

The file has two columns but the first one is just an incremental index so we discard it. The second column (that is stored in a numeric vector called `DXCM.Close`) contains `r length(d)` observations. No information about the time scale is given so for now we will just index it from 1 to `r length(d)` (with frequency=1). The main descriptive statistics of the series, as well as its histogram and time-series plot are shown below. The histogram is positively skewed with a long tail and far from normal, but as typical with timeseries data the histrogram does not provide information about the dynamics of the series.  


```{r ex2-desc_stats}
```

```{r ex2-hist, echo = FALSE, fig.width = 5, fig.height = 3.75, fig.cap = "Histogram of the data."}
```

We look at the timeseries itself and some zoomed in plots covering approximately the last 1000, 365, 91, and 60 units which could possibly correspond to a year, quarter, and two months. These plots inform us that the time series is not (mean) stationary, as the mean depends on time with an increasing (then slighly decreasing) trend.

```{r ex2-time_plot, echo = 1, fig.width = 5, fig.height = 3.75, fig.cap = "Time-series plot"}
```

```{r ex2-time_plot_zoom, fig.width = 5, echo = FALSE, fig.cap = "Zoomed Time-series plots"}
```

We then take a look at the box and whisker plot of the timeseries in order to see if the variance is stationary over time. As we can see in the plot below the variance is not stationary in time and starts to increase over time especially after group 16 in the plot. Note since we do not know the unit of observation we choose to group the data into 22 equal bins of 106 observations in order to be able to identify any changes in variance. We can also plot the square of the timeseries to see if the variance is conditional on the prior and as we can see in the figure below there is a change at around obervation 1700.

```{r ex2-boxplot, echo = FALSE,  fig.width = 5, fig.height = 4, fig.cap = "Checking for Staionarity in Variance"}
```

We also examine the acf and pacf to see look for any statistically significant lags or seasonality. As we can see from the pacf that there may be some seasonality around 14 or 16 which as we noted above could be about every two weeks or mid month if the data is daily. 

```{r ex2-acf_pacf, echo = FALSE, fig.width=6, fig.height=4.5, fig.cap = "Autocorrelation Plot"}
```


From the variance analysis,  we choose to subset the data and only work with the data from about 1500 to 2332, becasue the data prior to 1500 is likely to not be useful in forcasting the data.

```{r ex2-subset}
```

```{r ex2-sub_time_plot, echo = 1, fig.width = 5, fig.height = 3.75, fig.cap = "Subset Time-series plot"}
```


Since we suspect some seasonality, we can decomponse the timeseries which varifies the increasing trend and the seasonality as see in the figure below. We use multiplicative decomposition because of the nonstationarity in the variance.  

```{r ex2-decompose, echo = FALSE,  fig.width = 6, fig.height = 5.6, fig.cap = "Decomposition of the time series."}
```

Since this data is not stationary in the mean or variance; MA, AR, and ARMA models will not be good models to forcast the data. In order to model this non-staionary data we need to difference the data. The plot of the differenced data looks stationary in the mean; however, we also plot the squared ts and notice the variance is still volitile. To confirm our suspicion of conditionally heteroskedastic we plot the ACF of the squared values with adjusted mean. The results in the plot below confirm our suspicion which indicatese that a GARCH style model would be better suited to model the data because they allow for conditional changes in variance. 


```{r ex2-first_diff}
```

```{r ex2-acf, echo = FALSE, fig.width=6, fig.height=4.5, fig.cap = "Autocorrelation Plot"}
```


At this point we know that an MA, AR, ARMA, ARIMA, and sARIMA models will not be able to fully capture the data; however, a simple GARCH model will not be able to capture the seasonal aspect of the data. Therefore, we use a seasonal-ARIMA-GARCH model to capture all of the elements of the data. The sARIMA model to capture the seasonal component and then run a GARCH on the residuals to model the conditional changes in variance. 

In order order to show that an ARIMA model does not capture the data, we run several ARIMA models to find the best coeficients and see that sARIMA(0,0,0)(0,1,2) are the best fit coefficients using AIC criteria showing a seasonality component to the model. Examining the acf and squared acf we note that the squared acf still has volitility which is evident by the squared residuals being significant at most lags and confirms our assumption that we need to run a GARCH on the residuals. We also see that the confidence intervals on the coeficients for the seasonal componets are significant and do not cross zero. 

```{r ex2-sarima}
```

```{r ex2-sarima_acf, echo = FALSE, fig.width=6, fig.height=4.5, fig.cap = "Autocorrelation Plots from sARIMA model"}
```

We now model the residuals using a GARCH model and look at the confidence intervals, acf, and acf of squared values to examine goodness of fit. We see that just the GARCH model coefficients are not all statistically significant since zero falls within the the confidence interval of b1. The acf also shows no obvious pattern of significant values. 

```{r ex2-garch}
```

```{r ex2-garch_acf, echo = FALSE, fig.width=6, fig.height=4.5, fig.cap = "Autocorrelation Plots from GARCH model"}
```

We then use this model to forcast ahead 36 steps and plot it with the timeseries. 

```{r ex2-forcast, echo = 1, fig.width = 5, fig.height = 3.75, fig.cap = "Time-series plot with 36 step forcast"}
```

**********

\pagebreak

# Part 3

## Forecast the Web Search Activity for global Warming

**Imagine that you group is part of a data science team in an appreal company. One of its recent products is Global-Warming T-shirts. The marketing director expects that the demand for the t-shirts tends to increase when global warming issues are reported in the news. As such, the director asks your group to forecast the level of interest in global warming in the news. The dataset given to your group captures the relative web search activity for the phrase, "global warming" over time. For the purpose of this exercise, ignore the units reported in the data as they are unimportant and irrelevant. Your task is to produce the weekly forecast for the *next 3 months* for the relative web search activity for global warming. For the purpose of this exercise, treat it as a *12-step ahead forecast*.**

**The dataset for this exercise is provided in `globalWarming.csv`. Use only models and techniques covered in the course (up to lecture 13). Note that one of the modeling issues you may have to consider is whether or not to use the entire series provided in the data set. Your choice will have to be clearly explained and supported with empirical evidence. As in other parts of the lab, the general instructions in the *Instruction Section* apply.**

**********

\pagebreak

# Part 4

## Forecast Inflation-Adjusted Gas Price

**During 2013 amid high gas prices, the Associated Press (AP) published an article about the U.S. inflation-adjusted price of gasoline and U.S. oil production. The article claims that there is "*evidence of no statistical correlation*" between oil production and gas prices. The data was not made publicly available, but comparable data was created using data from the Energy Information Administration. The workspace and data frame `gasOil.Rdata` contains the U.S. oil production (in millions of barrels of oil) and the inflation-adjusted average gas prices (in dollars) over the date range the article indicates.**

**In support of their conclusion, the AP reported a single p-value. You have two tasks for this exericse, and both tasks need the use of the data set `gasOil.Rdata`.**

**Your first task is to recreate the analysis that the AP likely used to reach their conclusion. Thoroughly discuss all of the errors the AP made in their analysis and conclusion.**

**Your second task is to create a more statistically-sound model that can be used to predict/forecast inflation-adjusted gas prices. Use your model to forecast the inflation-adjusted gas prices from 2012 to 2016.**

**********

---
title: "**W271**-2 -- Spring 2016 -- **Lab 3**"
author: "***Juanjo Carin, Kevin Davis, Ashley Levato, Minghu Song***"
date: "*April 22, 2016*"
output:
   pdf_document:
     fig_caption: yes
     fig_height: 3
     fig_width: 4
     toc: yes
     toc_depth: 2
numbersections: false
geometry: margin=1in
options: width=30
fontsize: 10pt
linkcolor: blue
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[LO,LE]{Carin, Davis, Levato, Song}
- \fancyhead[CE,CO]{W271 -- Lab 3}
- \fancyhead[RE,RO]{\leftmark}
- \fancyfoot[LO,LE]{UC Berkeley -- MIDS}
- \fancyfoot[CO,CE]{Spring semester 2016}
- \fancyfoot[RE,RO]{\thepage}
- \renewcommand{\headrulewidth}{0.5pt}
- \renewcommand{\footrulewidth}{0.5pt}
---

**********

### Instructions

* Thoroughly analyze the given dataset or data series. Detect any anomalies in each of the variables. Examine if any of the variables that may appear to be top- or bottom-coded.
* Your report needs to include a comprehensive graphical analysis
* Your analysis needs to be accompanied by detailed narrative. Just printing a bunch of graphs and econometric results will likely receive a very low score.
* Your analysis needs to show that your models are valid (in statistical sense).
* Your rationale of using certain metrics to choose models need to be provided. Explain the validity / pros / cons of the metric you use to choose your "best" model.
* Your rationale of any decisions made in your modeling needs to be explained and supported with empirical evidence.
* All the steps to arrive at your final model need to be shown and explained clearly.
* All of the assumptions of your final model need to be thoroughly tested and explained and shown to be valid. Don't just write something like, "the plot looks reasonable", or "the plot looks good", as different people interpret vague terms like "reasonable" or "good" differently.

\pagebreak

```{r, echo = FALSE, warning = FALSE}
require(knitr, quietly = TRUE)
read_chunk('code/W271_Lab3_Davis.R')
opts_chunk$set(message = FALSE, warning = FALSE)
opts_chunk$set(fig.width = 4, fig.height = 3)
# Set path to data here (don't use setwd() inside a chunk!!!)
opts_knit$set(root.dir = './/data', global.par = TRUE)
```

```{r Libraries-Functions-Constants, echo = FALSE}
```

# Part 1

## Modeling House Values

**In Part 1, you will use the data set `houseValue.csv` to build a linear regression model, which includes the possible use of the instrumental variable approach, to answer a set of questions interested by a philanthropist group. You will also need to test hypotheses using these questions.**

**The philanthropist group hires a think tank to examine the relationship between the house values and neighborhood characteristics. For instance, they are interested in the extent to which houses in neighbhorhood with desirable features command higher values. They are specifically interested in environmental features, such as proximity to water body (i.e. lake, river, or ocean) or air quality of a region.**

**The think tank has collected information from tens of thousands of neighborhoods throughout the United States. They hire your group as contractors, and you are given a small sample and selected variables of the original data set collected to conduct an initial, proof-of-concept analysis. Many variables, in their original form or transfomed forms, that can explain the house values are included in the dataset. Analyze each of these variables as well as different combinations of them very carefully and use them (or a subset of them), in its original or transformed version, to build a linear regression model and test hypotheses to address the questions. Also address potential (statistical) issues that may be casued by omitted variables.**

**********

\pagebreak

### Exploratory 

\color{Black}

Based on the information in `homeValueData_VariableDescription.txt`, the variables and their meaning are: 

+ `crimeRate_pc`: crime rate per capital, measured by number of crimes per 1000 residents in neighborhood.
+ `nonRetailBusiness`: the proportion of non-retail business acres per neighborhood.
+ `withWater`: the neighborhood within 5 miles of a water body (lake, river, etc); 1 if true and 0 otherwise.
+ `ageHouse`: proportion of house built before 1950.
+ `distanceToCity`: distances to the nearest city (measured in miles).
+ `pupilTeacherRatio`: average pupil-teacher ratio in all the schools in the neighborhood.
+ `pctLowIncome`: percentage of low income household in the neighborhood
+ `homeValue`: median price of single-family house in the neighborhood (measured in dollar).
+ `pollutionIndex`: pollution index, scaled between 0 and 100, with 0 being the best and 100 being the worst (i.e. uninhabitable).
+ `nBedRooms`: average number of bed rooms in the single family houses in the neighborhood.

First, we will load the data and conduct an exploratory analysis.

```{r ex1-load, echo = 3}
```


The data consists of 400 observations of 11 numeric variables related to the value of houses in each neighborhood and characteristics that describe the houses and the surrounding neighborhood. A numeric summary of each variable is provided below.

The data consists of `r dim(ex1df)[1]` observations (with no missing values) of `r dim(ex1df)[2]` numeric variables: the ones mentioned above (median price of single-family houses in different neighborhoods and characteristics about those houses and neighborhoods) plus an additional one, not mentioned in the `txt` file:

+ `distanceToHighway`: self-explanatory (and probably measured in miles, same as `distanceToCity`).

Based on the kurtosis, skewness (all of them far from zero to a greater or lesser extent) and the *p*-values of a normality test (all highly significant), none of the variables in the sample is normally distributed. That means they might benefit from transformation (potential transformations will be discussed as the exploratory analysis proceeds).

```{r ex1-summary_table, results = 'asis', echo=F, fig.cap = "Summary Statistics of Housing Data"}
```


```{r ex1-CrimeRate, echo=F, fig.height=3, fig.width=8, fig.cap="Histograms of the Crime Rate Variable"}
```

As shown in the 1st Figure in the following page, the crime rate variable is highly right-skewed, with most neighborhoods having a very low number of crimes per 1,000 residents, and a few having a high number. Using the log does not normalize that variable (the distribution is bimodal). 

```{r ex1-Business, echo=F, fig.height=3, fig.width=8, fig.cap="Histograms of Non-Retail Business Percentage Variable"}
```

As for the proportion of non-retail business acres per neighborhood, a high proportion of neighborhoods have non-retail business covering about 18% of their area, and most of the rest have much fewer non-retail businesses. A log transformation does not help to normalize this variable either.

<!-- ???This variable may be a proxy for if the house is more urban or rural, so perhaps retail businesses are a sign of greater economic activity and thus we would expect these houses to have higher value.  -->

```{r ex1-withWater, echo=F, fig.height=3, fig.width=4, fig.cap="Histogram of Water Variable"}
```

Most neighborhoods are not located within 5 miles to a water body. Being near a lake or a river seems highly desirable, in principle, so it's a good candidate to have an effect on home values.

\pagebreak

```{r ex1-houseAge, echo=F, fig.height=3, fig.width=8, fig.cap="Histogram of House Age"}
```

In almost 15% (`r 100*sum(ex1df$ageHouse>97.5)/length(ex1df$ageHouse)`%) of the neighborhoods, more than 97.5% of the houses were built before 1950. If we lower that percentage of "hold houses" to 75%, that occurs in more than half of the neighborhoods (`r 100*sum(ex1df$ageHouse>75)/length(ex1df$ageHouse)`%). In less than 10% of the neighborhoods (`r 100*sum(ex1df$ageHouse<25)/length(ex1df$ageHouse)`%) only 25% of the houses or less are "old"". Once again, a log transformation does not help to normalize the data. 

```{r ex1-cityDistance, echo=F, fig.height=3, fig.width=8, fig.cap="Histogram of Distance to City"}
```

The distance from a neighborhood to the nearest city has a right-tailed distribution, with most neighborhoods within 10 miles of nearby cities and a minority of houses with greater than 25 miles to nearby cities. Log transformation of the city distance variable removed the skewness of the distribution and produced a more approximately normal distribution, although still non-normal.

\pagebreak

```{r ex1-highwayDistance, echo=F, fig.height=3, fig.width=4, fig.cap="Histogram of Distance to Highway"}
```

The distance to highway values fall in to 8 bins. This suggests that these values may actually represent factors, or perhaps that the values were heavily rounded. The majority of houses were within 10 miles of a highway, with a large group of houses located 24 miles from the nearest highway. 

```{r ex1-classSize, echo=F, fig.height=3, fig.width=4, fig.cap="Histogram of Distance to Highway"}
```

The pupil to teacher ratio histogram shows a large portion of neighborhoods have average pupil-to-teacher ratios between 17.5 and 23, with a gap between 17.5 and 16, and a small minority that have ratios below 16. 

\pagebreak

```{r ex1-lowIncome, echo=F, fig.height=3, fig.width=8, fig.cap="Histogram of Pupil to Teacher Ratio"}
```

The percentage of low income housing in a neighborhood displayed a right-skewed distribution, with most values falling around the mean of `r mean(ex1df$pctLowIncome)` and then a long tail stretching to the maximum of nearly 50% low income housing. Log transformation produced a more normal looking distribution.

```{r ex1-homeValue, echo=F, fig.height=3, fig.width=8, fig.cap="Histogram of Percentage Low Income Housing"}
```

The housing value plot was also right-skewed, with most values around the mean of `r round(mean(ex1df$pctLowIncome),2)` and a right tail extending to around $1,200,000. Again, log transformation seems appropriate and produces a more normal looking distribution. 

\pagebreak

```{r ex1-pollution, echo=F, fig.height=3, fig.width=8, fig.cap="Histogram of Housing Value"}
```

The pollution index scores have a slightly-right tailed appearing distribution, with thin tails and evidence of multimodality. Log transformation of the pollution index reduced the right-skewness while still showing evidence of multimodality and thinner tails than a normal distribution.

```{r ex1-beds, echo=F, fig.height=3, fig.width=4, fig.cap="Histogram of Pollution Index"}
```

The number of beds in houses had a distribution appearing roughly normal, but with considerably longer tails.

After visually inspecting the distribution of each individual variable, we are also interested in how the variables relate, especially to the outcome variable of our model, homeValue. To gain an understanding of how each potential input variable relates to the outcome variable, we make a scatter plot and summarize the results of a univariate regression for each input variable against the output variable.

```{r ex1-houseScatters, echo=F, fig.height=10.5, fig.width=8, fig.cap="Scatter Plot of Home Value Against the Variables of Interest"}
```

\pagebreak

```{r ex1-univarRegressions, echo=F, results='asis'}
```

The scatterplots and regression summaries show that each of the variables, when not controlling for any other variables, is a better predictor of log(median house price) than the mean. This fact may complicate our effort to select one of these variables as an instrument, as being un-related to the outcome variable is one condition of the exclusion restriction for an instrumental variable approach. However, this does not necessarily preclude all the variables from being used as an instrument, as some variables may not be significant when controlling for other variables. 

Examining the scatter plot, it is noteworthy that many of the input variables seem to have a negative relationship with the outcome variable. To check for potential issues with multicollinarity, we can test the values in the correlation matrix for the dataset to see if they are over a threshold. 

```{r ex1-houseCorrelations, echo=T}
```

Analysis of the correlation matrix shows that there are no variable pairs with a correlation coefficient above .9. While a good check against obvious issues of multicolliniarity, this does not preclude the possibility that one variable is highly correlated with linear combinations of the other variables. 

Since we are tasked with determining the impact of environmental variables on the value of homes, we also want to understand how those variables relate to the other variables in the dataset. 

```{r ex1-polluteScatters, echo=F, fig.height=10.5, fig.width=8, fig.cap="Scatter Plot of Pollution Index Against the Variables of Interest"}
```


```{r ex1-waterScatters, echo=F, fig.height=10.5, fig.width=8, fig.cap="Scatter Plot of Water Presence/Absence Against the Variables of Interest"}
```

\pagebreak


The scatter plots of the variables in the dataset against the pollution index reveal that there are many strong negative and positive correlations. This provides evidence that estimating the effects of pollution on home values requires controlling for a number of potential confounding variables. The scatter plot of adjacency to water with the variables reveals generally weaker relationships, which is not surprising given that the variable is a factor and that the occurrence of water is less influenced by social and demographic factors than pollution. 

### Model selection

Before beginning the empirical process of model selection, we will stipulate that the variables for number of bedrooms and percentage of low income housing should be included in *any* regression model with median home value because they have a well established relationship with the outcome variable. 

Taking our variables of interest and the two variables included for theoretical reasons, the baseline model is:
$$ log(homeValue) = \beta_0 + \beta_1 log(pollutionIndex) + \beta_2 withWater + \beta_3 nBedRooms + \epsilon$$

```{r ex1-baseModel, echo=F, results = 'asis'}
```

```{r ex1-baseModelPlot, echo=F, fig.height=5, fig.width=6, fig.cap="Diagnoistic Plots for Base Regression Model"}
```

\pagebreak

Looking at the results of the base regression, we see that all coefficients are significant and the R^2 value is `r summary.lm(baseModel)$r.squared`, a very basic confirmation that these 4 variables are indeed variables of interest. Looking at the diagnostic plots, we see in the residuals versus fitted plot that the band of values seems to widen out from left to right and then narrow, suggesting that the use of **heteroskadasticity-robust standard errors** is appropriate. The residuals vs leverage plot suggests that some points may represent outliers, although they do not exert leverage on the model that is outside of the norm. The presence of outliers is not surprising in a dataset like this, as we know the values of homes in some neighborhoods are simply much higher or lower due to conditions that may not be perfectly captured by the dataset. **Since the presence of outliers is simply a reality in datasets relating to housing, we do not argue in favor of any outlier removal procedure.**

Starting with our base model, we are interested in selecting a final model that 1) is a good fit of our sample data 2) can generalize to the larger dataset 3) provides insight into the effect of environmental factors on the value of homes. One approach we examine for selecting a final model is adding additional parameters while penalizing the model for increasing complexity using a measure like the AIC/BIC.

```{r ex1-stepWise, echo=T}
```

The model chosen by step-wise addition of variables and AIC penalization has 4 additional parameters. One obvious drawback to this approach is that, due to multiple comparisons, it tends to under-state the confidence intervals and thus the resulting p-values for the parameters are too low. Indeed, in the resulting model, the final parameter's coefficient is not statistically different from zero using heteroskedasticity-robust standard errors. 

Since we are estimating a model using only a portion of the data, we should also be concerned with the out of sample fit for our proposed model. Here, we can test the accuracy of predictions made with increasingly complex models using a sub-sample of withheld data. 

```{r ex1-outSampleFit, echo=T}
```

The results of out-of-sample fitting are summarized in the table below:

```{r ex1-forecastTable, echo=F, results='asis'}
```

Using the criterion of the RMES or MAE, the model chosen would include 4 additional parameters, while the AIC would favor either the model with 3 or 4 additional parameters, and the BIC would favor the model with 3 additional parameters. 

Taking the results of step-wise addition and out-of-sample forecasting together, one could persuasively argue for both the model with 3 additional parameters and the model with 4.  Given the similarity of the out of sample fit and the preference for models with less complexity, **the model we select for predicting home values is the base model with three additional paramaters: pupil to teacher ratio, log(distance to city), and log(crime rate per capita) **. This model has the form:
$$log(homeValue) = \beta_0 + \beta_1 log(pollutionIndex) + \beta_2 withWater + \beta_3 nBedRooms +$$
$$\beta_4 pupilTeacherRatio + \beta_5 log(distanceToCity) + \beta_6 log(crimeRate_pc) + \epsilon$$ 

\pagebreak

```{r ex1-model3, echo=F, results='asis'}
```

According to the model summary, there is in fact an effect of environmental factors on home values. Compared to our original base model, controlling for the additional variables has resulted is a large (negative) increase in the coefficient for pollution index and a small decrease in the coefficient for absence/presence of water. For pollution index value, an increase of 1 in the log value of pollution index score results in a decrease of $`r round(coeftest(model3)[2],3)` (`r round(coeftest(model3)[2] + 1.96 * coeftest(model3)[10],3)`$, $`r round(coeftest(model3)[2] - 1.96 * coeftest(model3)[10],3)`$), p = $`r round(coeftest(model3)[26], 3)`$ in the log home value. For a house with median value, going from the 25th percentile on the pollution index to the median value would result in a decrease in home value of  \$$`r format(round(exp(median(s_df$log_homeValue) + quantile(s_df$log_pollutionIndex, 0.25, names=F) * -.211)- exp(median(s_df$log_homeValue) + quantile(s_df$log_pollutionIndex, 0.5, names=F) * -.211), 2), scientific=F)`$. For locations adjacent to water, going from not being adjacent to water to being adjacent results in an increase in log home value of $`r round(coeftest(model3)[3],3)`$ ($`r round(coeftest(model3)[3] + 1.96 * coeftest(model3)[11],3)`$, $`r round(coeftest(model3)[3] - 1.96 * coeftest(model3)[11],3)`$), p = $`r round(coeftest(model3)[27], 3)`$. For a house with median value, being adjacent to water would result in an increase of home value of \$$`r format(round(exp(median(s_df$log_homeValue) + .113) - exp(median(s_df$log_homeValue)) , 2), scientific=F)`$.

\pagebreak

```{r ex1-model3Plot, echo=F, fig.height=5, fig.width=6, fig.cap="Diagnoistic Plots for Selected Regression Model"}
```

The residual vs fitted plot for our selected model shows a distinct tunneling of values from left to right, suggesting heteroskedasticity and the need to use heteroskedasticity-robust standard errors. 

<!-- The primary issue in estimating home value using this dataset is the likelihood of omitted variable bias. The type of information needed to fully characterize the 'desirability' of a neighboorhood, and the causal mechanisms that translate from 'desirability' to home values are not fully captured by this dataset. For instance, we could imagine that the variable of interest, pollutionIndex, has a causal path to homeValue via intermediary variables. Perhaps total green space is an important variable for influencing people's beliefs about the environmental quality of a neighborhood, and people 'estimate' the pollutionIndex of a place based on the degree of green space. Then the true effect of pollution on home values is mediated through green space. In this case, our estimates of the coefficient for pollutionIndex would be biased because we are not controlling for green space. Given the likely complexity of the casual pathway between neighborhood desirability and home value, It is highly likely that there are variables relating to the 'diserabilityr' of a neighborhood that have an effect on the value of homes that are not captured by this dataset.  -->

The primary issue in estimating home value using this dataset is the likelihood of omitted variable bias. For instance, we could imagine that the variable of interest, pollutionIndex, has a causal path to homeValue via intermediary variables. Perhaps total green space is an important variable for influencing people's beliefs about the environmental quality of a neighborhood, and people 'estimate' the pollutionIndex of a place based on the degree of green space. Then the true effect of pollution on home values is mediated through green space. In this case, our estimates of the coefficient for pollutionIndex would be biased because we are not controlling for green space. Given the likely complexity of the casual pathway between neighborhood desirability and home value, it is highly likely that there are variables relating to the 'desirability' of a neighborhood that have an effect on the value of homes that are not captured by this dataset.

### Using IV to estimate coefficient for pollution index

Given that we are interested in how environmental factors influence home prices, we may worry that the coefficient for pollution index is biased due to considerations of omitted variables outlined above. We argue that the the proportion of non-retail businesses acres is a source of random variation with regards to home values (controlling for covariates), and thus represents a possible instrument for pollution index. 

In order for non-retail business acreage to be a suitable IV for pollution index score, two conditions must hold:
* Non-retail business acreage must be correlated with pollution index
* Non-retail business acreage must be uncorrelated with the error term in the regression model. 

Given that non-retail businesses include industrial operations like manufacturing, which are sources of pollution, it is not surprising that the two variables are positively correlated (See scatterplot above). For the IV approach to be causal, we need to argue persuasively that non-retail business percentage is not endogenous to the model, that is to say that home values do not depend directly on non-retail business acreage, conditional on the covariates in the model. Here, we argue that the main proxy of non-retail business would be urban/rural, given that rural areas tend to have more non-retail businesses, both in terms of total number of business and the proportion of acreage occupied by those businesses, and since we are controlling for distance to city, then, on average, non-retail business percentage is not a determinant of house valuation. This assumption is generally not testable, and can be seen as more or less persuasive depending on the IV proposed. In this case, one can imagine enough situations where the closeness to certain kinds of non-retail business (heavy manufacturing, open-pit mining) would have an impact on the valuation of a home that I wouldn't find non-retail business percentage to be a completely persuasive instrument for pollution index. 
 
Performing a two-step regression process using non-retail business percentage as an instrument for pollution index score produces the coefficients presented in the table below:

```{r ex1-IVRegTable, echo=F, results='asis'}
```

The coefficient for pollution index using an instrumental variable approach is considerably smaller than when using straightforward OLS regression. Depending on how you interpret the strength of the instrument, this result could be used to argue in favor of the hypothesis that people are generally not aware of the pollution index of an area and it has a small effect on home valuation, while omitted variables may give the appearance of greater importance for pollution index scores. However, I think there are strong arguments to be made against this particular instrument, and when combined with the large standard errors of the estimate, **we can be fairly confident that there is a negative effect of pollution index on home values, but the magnitude of the effect remains uncertain.** 

One hypothesis that may be of particular interest to the think-tank is whether or not the effect of pollution is the same for neighborhoods with a lot of low income housing and neighborhoods with very little low income housing. Given the bi-model distribution of low income housing noted in the exploratory analysis, this dataset seems well suited to testing this hypothesis. Formally, we are interested in the interaction term ($\beta_7 = log(PollutionIdex) \cdot log(Percentage Low Income Housing)$) and the hypothesis:

$$H_0: \beta_{7}= 0 $$
$$H_A: \beta_{7} \neq 0$$

Adding the interaction term $\beta_{7}$ to our preferred model yields:

```{r ex1-interaction, echo=F, results='asis'}
```

The coefficient for the interaction effect represents a practically and significantly difference in the effect of pollution for neighborhoods with little low income housing and neighborhoods with a lot of low income housing ($\beta_7$ = $`r round(coeftest(inter_model)[9], 3)`$, s.e. = $`r round(coeftest(inter_model)[18], 3)`$, p = $`r round(coeftest(inter_model)[36], 3)`$). For neighborhoods with a high proportion of low income housing, pollution has a considerably larger negative effect on home values, representing about a 5% decrease relative to neighborhoods with little low income housing. 

**********
\pagebreak

# Part 2

## Modeling and Forecasting a Real-World Macroeconomic / Financial time series

**Build a time-series model for the series in `lab3_series02.csv`, which is extracted from a real-world macroeconomic/financial time series, and use it to perform a 36-step ahead forecast. The periodicity of the series is purposely not provided. Possible models include AR, MA, ARMA, ARIMA, Seasonal ARIMA, GARCH, ARIMA-GARCH, or Seasonal ARIMA-GARCH models.**

**********

\pagebreak

# Part 3

## Forecast the Web Search Activity for global Warming

**Imagine that you group is part of a data science team in an appreal company. One of its recent products is Global-Warming T-shirts. The marketing director expects that the demand for the t-shirts tends to increase when global warming issues are reported in the news. As such, the director asks your group to forecast the level of interest in global warming in the news. The dataset given to your group captures the relative web search activity for the phrase, "global warming" over time. For the purpose of this exercise, ignore the units reported in the data as they are unimportant and irrelevant. Your task is to produce the weekly forecast for the *next 3 months* for the relative web search activity for global warming. For the purpose of this exercise, treat it as a *12-step ahead forecast*.**

**The dataset for this exercise is provided in `globalWarming.csv`. Use only models and techniques covered in the course (up to lecture 13). Note that one of the modeling issues you may have to consider is whether or not to use the entire series provided in the data set. Your choice will have to be clearly explained and supported with empirical evidence. As in other parts of the lab, the general instructions in the *Instruction Section* apply.**

**********

\pagebreak

# Part 4

## Forecast Inflation-Adjusted Gas Price

**During 2013 amid high gas prices, the Associated Press (AP) published an article about the U.S. inflation-adjusted price of gasoline and U.S. oil production. The article claims that there is "*evidence of no statistical correlation*" between oil production and gas prices. The data was not made publicly available, but comparable data was created using data from the Energy Information Administration. The workspace and data frame `gasOil.Rdata` contains the U.S. oil production (in millions of barrels of oil) and the inflation-adjusted average gas prices (in dollars) over the date range the article indicates.**

**In support of their conclusion, the AP reported a single p-value. You have two tasks for this exericse, and both tasks need the use of the data set `gasOil.Rdata`.**

**Your first task is to recreate the analysis that the AP likely used to reach their conclusion. Thoroughly discuss all of the errors the AP made in their analysis and conclusion.**

**Your second task is to create a more statistically-sound model that can be used to predict/forecast inflation-adjusted gas prices. Use your model to forecast the inflation-adjusted gas prices from 2012 to 2016.**

**********

---
title: "**W271**-2 -- Spring 2016 -- **Lab 1**"
author: "***Juanjo Carin, Kevin Davis, Ashley Levato, Minghu Song***"
date: "*January 14, 2016*"
output:
   pdf_document:
      toc: true
      fig_caption: yes
      toc_depth: 2
numbersections: false
geometry: margin=1in
fontsize: 10pt
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[LO,LE]{Carin, Davis, Levato, Song}
- \fancyfoot[LO,LE]{UC Berkeley -- MIDS -- W271 -- Spring semester 2016}
- \fancyfoot[CO,CE]{}
- \fancyfoot[RE,RO]{\thepage}
- \renewcommand{\headrulewidth}{0.5pt}
- \renewcommand{\footrulewidth}{0.5pt}

---

**********

\pagebreak

# Part I: Marginal, Joint, and Conditional Probabilities

## Question 1

**In a team of data scientists, 36 are expert in machine learning, 28 are expert in statistics, and 18 are awesome. 22 are expert in both machine learning and statistics, 12 are expert in machine learning and are awesome, 9 are expert in statistics and are awesome, and 48 are expert in either machine learning or statistics or are awesome. Suppose you are in a cocktail party with this group of data scientists and you have an equal probability of meeting any one of them.**

1. **What is the probability of meeting a data scientist who is an expert in both machine learning and statistics and is awesome?**

Let $\mathbf{N \geq 48}$ be the **size of the team of data scientists**, and $M, S, A$ the event that a data scientist is either a machine learning expert, a statistics expert, or awesome.

$\mathbf{\textcolor{red}{\Pr(M \cap S \cap A)}} = \Pr(M) + \Pr(S \cap A) - \Pr(M \cup (S \cap A))$

$\ \ \ \ \ \ = \Pr(M) + \Pr(S \cap A) - \Pr((M \cup S) \cap (M \cup A))$

$\ \ \ \ \ \ = \Pr(M) + \Pr(S \cap A) - ((\Pr(M \cup S) + \Pr(M \cup A) - \Pr((M \cup S) \cup (M \cup A))))$

$\ \ \ \ \ \ = \Pr(M) + \Pr(S \cap A) - (\Pr(M) - \Pr(S) + \Pr(M \cap S)) - (\Pr(M) - \Pr(A) + \Pr(M \cap A)) + \Pr(M \cup S \cup A)$

$\ \ \ \ \ \ = \Pr(M \cap S) + \Pr(M \cap A) + \Pr(M \cap A) - \Pr(M) - \Pr(S) - \Pr(A) + \Pr(M \cup S \cup A)$

$\ \ \ \ \ \ = \cfrac{22}{N} + \cfrac{12}{N} + \cfrac{9}{N} - \cfrac{36}{N} - \cfrac{28}{N} - \cfrac{18}{N} + \cfrac{48}{N}  = \cfrac{(22+12+9)-(36-28-18)+48}{N} = \cfrac{43-82+48}{N}$

$\ \ \ \ \ \ = \mathbf{\textcolor{red}{\cfrac{9}{N}}} \leq \cfrac{9}{48} = `r formatC(9/48, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`$

2. **Suppose you meet a data scientist who is an expert in machine learning. Given this information, what is the probability that s/he is not awesome?**

$\mathbf{\textcolor{red}{\Pr(A^c|M)}} = 1 - \Pr(A|M) = 1 - \cfrac{\Pr(A \cap M)}{\Pr(M)} = 1 - \cfrac{\cfrac{12}{N}}{\cfrac{36}{N}} = 1 - \cfrac{12}{36} = 1 - \cfrac{1}{3} = \mathbf{\textcolor{red}{\cfrac{2}{3} = `r formatC(2/3, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`}}$

3. **Suppose the you meet a data scientist who is awesome. Given this information, what is the probability that s/he is an expert in either machine learning or statistics?**

$\mathbf{\textcolor{red}{\Pr(M \cup S | A)}} = \cfrac{\Pr((M \cup S) \cap A)}{\Pr(A)} = \cfrac{\Pr((M \cap A) \cup (S \cap A))}{\Pr(A)}$

$\ \ \ \ \ \ = \cfrac{\Pr(M \cap A) + \Pr(S \cap A) - \Pr((M \cap A) \cap (S \cap A)}{\Pr(S)} = \cfrac{\Pr(M \cap A) + \Pr(S \cap A) - \Pr(M \cap S \cap A)}{\Pr(S)}$

$\ \ \ \ \ \ = \cfrac{\cfrac{12}{N} + \cfrac{9}{N} - \cfrac{9}{N}}{\cfrac{18}{N}} = \cfrac{12+9-9}{18} = \cfrac{12}{18}$

$\ \ \ \ \ \ = \mathbf{\textcolor{red}{\cfrac{2}{3} = `r formatC(2/3, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`}}$


## Question 2

**Suppose for events $A$ and $B$, $\Pr(A) = p \leq \frac{1}{2}, \Pr(B) = q$, where $\frac{1}{4} < q < \frac{1}{2}$. These are the only information we have about the events.**

1. **What are the maximum and minimum possible values for $\Pr(A \cup B)$?**

$$\Pr(A \cup B) = \Pr(A) + \Pr(B) - \Pr(A \cap B)$$

The maximum value of $\Pr(A \cup B)$ occurs when $A \cap B$ is the smallest set possible. In this case, since $\Pr(A) \leq \frac{1}{2}$ and $\Pr(B) < \frac{1}{2}$, it would be $A \cap B = \varnothing$ (if $A$ and $B$ were disjoint sets, which might be the case), so:

$\ \ \ \ \ \ \Pr(A \cup B) = \Pr(A) + \Pr(B) - \Pr(A \cap B) = \Pr(A) + \Pr(B) - \Pr(\varnothing) = p + q - 0 = p + q$

which could have a maximum value close to $1$ (i.e., $A \cup B \approx \Omega$), in case $\Pr(A) = \frac{1}{2}$ and $\Pr(B) \approx \frac{1}{2}$.

The minimum value of $\Pr(A \cup B)$ occurs when $A \cup B$ is the largest set possible, $A$ or $B$. In this case, since $\Pr(A)$ does not have a lower bound, that would happen when $A \subseteq B$ and (consequently) $A \cap B = A$, which would lead to:

$\ \ \ \ \ \ \Pr(A \cup B) = \Pr(A) + \Pr(B) - \Pr(A) = \Pr(B) = q$

whose minimum value is greater than $\frac{1}{4}$.

In summary,

$\mathbf{\textcolor{red}{\ \ \ \ \ \ \cfrac{1}{4} < \Pr(A \cup B) < 1}}$


2. **What are the maximum and minimum possible values for $\Pr(A | B)$?**

$$\Pr(A|B) = \cfrac{\Pr(A \cap B)}{\Pr(B)}$$

If $B \subseteq A$ (which would imply that the lower bound for $p$ is also $\frac{1}{4})$, then:

$\ \ \ \ \ \ \Pr(A|B) = \cfrac{\Pr(B)}{\Pr(B)} = 1$

As seen in the previous part, since $\Pr(A) \leq \frac{1}{2}$ and $Pr(B) < \frac{1}{2}$, it might occur that $A \cap B = \varnothing$, and hence:

$\ \ \ \ \ \ \Pr(A|B) = \cfrac{\Pr(\varnothing)}{\Pr(B)} = \cfrac{0}{q} = 0$

irrespective of the value of $q$.

In summary,

$\mathbf{\textcolor{red}{\ \ \ \ \ \ 0 \leq \Pr(A | B) \leq 1}}$

**********

\pagebreak

# Part II: Random Variables, Expectation, Conditional Exp.

## Question 3

**Suppose the life span of a particular server is a continuous random variable, $t$, with a uniform probability distribution between $0$ and $k$ year, where $k \leq 10$ is a positive integer.**

**The server comes with a contract that guarantees a full or partial refund, depending on how long it lasts. Specifically, if the server fails in the first year, it gives a full refund denoted by $\theta$. If it lasts more than $1$ year but fails before $\frac{k}{2}$ years, the manufacturer will pay $x = \$A(k - t)^{1/2}$, where $A$ is some positive constant equal to $2$ if $t \leq \frac{k}{2}$ . If it lasts between $\frac{k}{2}$ and $\frac{3k}{4}$ years, it pays $\frac{\theta}{10}$.**

1. **Given that the server lasts for $\frac{k}{4}$ years without failing, what is the probability that it will last another year?**

$\mathbf{\textcolor{red}{\Pr\left(t \leq 1+\cfrac{k}{4} \bigg| t \geq \cfrac{k}{4} \right)}} = \cfrac{\Pr \left(\left( t \leq 1+\cfrac{k}{4} \right) \bigcap \left( t \geq \cfrac{k}{4} \right) \right)}{\Pr \left(t \geq \cfrac{k}{4}\right)} = \cfrac{\Pr \left(\cfrac{k}{4} \leq t \leq 1+\cfrac{k}{4} \right )}{Pr \left(t \geq \cfrac{k}{4}\right)}$

$\ \ \ \ \ \ = \cfrac{\Pr \left(t \leq 1 +\cfrac{k}{4}\right) - \Pr \left(t \leq \cfrac{k}{4}\right)}{1-\Pr \left(t \leq \cfrac{k}{4}\right)} = \cfrac{\cfrac{1}{k}\left(1+\cfrac{k}{4} - \cfrac{k}{4} \right )}{1-\cfrac{1}{k}\cfrac{k}{4}} = \cfrac{\cfrac{1}{k}}{\cfrac{3}{4}}$

$\ \ \ \ \ \ = \mathbf{\textcolor{red}{\cfrac{4}{3k}}}$

2. **Compute the expected payout from the contract, $E(x)$.**

I tried 3 different approaches, all leading to the same result.

The first one is based on <https://www.probabilitycourse.com/chapter4/4_3_1_mixed.php>. Before computing the expected value of $X$, we need to compute its *cdf*.

$t$ (the life span of the server) is a continuous variable with the following *pdf*:

$$f_t(t) = \begin{cases}
\cfrac{1}{k} & 0\leq t\leq\ k \\
0 & \text{otherwise}
\end{cases}$$

But $X$ (the payout or refund) is not continuous:

$$X = g(t) = \begin{cases}
\theta & 0\leq t\leq\ 1 \\
A\sqrt{k-t} & 1\leq t\leq\ \cfrac{k}{2} \\
\cfrac{\theta}{10} & \cfrac{k}{2}\leq t\leq\ \cfrac{3k}{4} \\
0 & \text{otherwise}
\end{cases}$$

(The value of $X$ for $t=1$ is $A\sqrt{k-1}$, which is not necessarily equal to $\theta$; the value of $X$ for $t=k/2$ is $A\sqrt{k/2}$, which again is not necessarily equal to $\theta/10$.)

First we compute the probability that $X$ takes its possible discrete values:

$\Pr(X = 0) = \Pr\left(t \geq \cfrac{3k}{4}\right) = 1 - F_t\left(\cfrac{3k}{4}\right) = 1 - \cfrac{1}{k}\cfrac{3k}{4} = 1 - \cfrac{3}{4} = \cfrac{1}{4}$

$\Pr\left(X = \cfrac{\theta}{10}\right) = \Pr\left(\cfrac{k}{2} \leq t \leq \cfrac{3k}{4}\right) = F_t\left(\cfrac{3k}{4}\right) - F_t\left(\cfrac{k}{2}\right) = \cfrac{1}{k}\cfrac{3k}{4} - \cfrac{1}{k}\cfrac{k}{2} = \cfrac{3}{4} - \cfrac{1}{2} = \cfrac{3-2}{4} = \cfrac{1}{4}$

$\Pr(X = \theta) = \Pr(0 \leq t \leq 1) = F_t(1) - F_t(0) = \cfrac{1}{k} - \cfrac{0}{k} = \cfrac{1}{k}$

For $A\sqrt{\cfrac{k}{2}} \leq X \leq A\sqrt{k-1}$, we can compute the *cdf* of $X$ as follows:

$F_X(x) = \Pr(X \leq x)= \Pr\left(A\sqrt{k-t} \leq x\right)$

$\ \ \ \ \ \ = \Pr\left(k-t \leq \left(\frac{x}{A}\right)^2\right) = \Pr\left(t \geq k - \left(\frac{x}{A}\right)^2 \right) = 1 - \Pr\left(t \leq k - \left(\frac{x}{A}\right)^2 \right)$

$\ \ \ \ \ \ = 1 - \displaystyle\int_{t=0}^{k - \left(\frac{x}{A}\right)^2}\cfrac{1}{k}dt = 1 - \cfrac{1}{k}\left(k - \left(\frac{x}{A}\right)^2\right) = \cfrac{x^2}{kA^2}$

So the overall expression for $F_X(x)$ is:

$$F_X(x) = \begin{cases}
0 & x < 0 \\
\cfrac{1}{4} & 0 \leq x < \cfrac{\theta}{10} \\
\cfrac{1}{2} & \cfrac{\theta}{10}\leq x \leq\ A\sqrt{\cfrac{k}{2}} \\
\cfrac{x^2}{kA^2} & A\sqrt{\cfrac{k}{2}} \leq x \leq\ A\sqrt{k-1} \\
1-\cfrac{1}{k} & A\sqrt{k-1} \leq x < \theta \\
1 & x \geq \theta
\end{cases}$$

Let's check that:

$$\int_{x=A\sqrt{k/2}}^{A\sqrt{k-1}}\frac{dF_X(x)}{dx}dx + \sum_{x_k}\Pr(X=x_k) =1$$

$\displaystyle\int_{x=A\sqrt{k/2}}^{A\sqrt{k-1}}\cfrac{2x}{kA^2}dx + \Pr(X=0) + \Pr\left(X=\cfrac{\theta}{10}\right) + \Pr(X=\theta) = \left[ \cfrac{x^2}{kA^2}\right]_{x=A\sqrt{k/2}}^{A\sqrt{k-1}} + \cfrac{1}{4} + \cfrac{1}{4} + \cfrac{1}{k}$

$\ \ \ \ \ \ = \cfrac{(k-1)-\cfrac{k}{2}}{k} + \cfrac{1}{2} + \cfrac{1}{k} = \cfrac{\cfrac{k}{2}-1}{k} + \cfrac{1}{2} + \cfrac{1}{k} = \cfrac{1}{2} - \cfrac{1}{k} + \cfrac{1}{2} + \cfrac{1}{k} = 1$

Now we can compute the expected value of $X$ as:

$\mathbf{\textcolor{red}{E(X)}} = \displaystyle\int_{x=A\sqrt{k/2}}^{A\sqrt{k-1}}x\frac{dF_X(x)}{dx}dx + \displaystyle\sum_{x_k}x_k\Pr(X=x_k)$

$\ \ \ \ \ \ = \displaystyle\int_{x=A\sqrt{k/2}}^{A\sqrt{k-1}}\cfrac{2x^2}{kA^2}dx + 0\cdot\Pr(X=0) + \cfrac{\theta}{10}\cdot \Pr\left(X=\cfrac{\theta}{10}\right) + \theta\cdot\Pr(X=\theta)$

$\ \ \ \ \ \ = \left[ \cfrac{2x^2}{3kA^2}\right]_{x=A\sqrt{k/2}}^{A\sqrt{k-1}} + 0\cdot\cfrac{1}{4} + \cfrac{\theta}{10}\cdot\cfrac{1}{4} + \theta\cdot\cfrac{1}{k} = \mathbf{\textcolor{red}{\cfrac{2A}{3k}\left((k-1)^{\frac{3}{2}}-\left(\cfrac{k}{2}\right)^{\frac{3}{2}}\right) + \cfrac{\theta}{40} + \cfrac{\theta}{k}}}$

```{r, echo=FALSE, message = FALSE, warning = FALSE, fig.cap = "Approximate aspect of $F_X(x)$ (for $A=2, k=8, \\theta=30$)", fig.width = 4, fig.height = 4}
F <- function(x) {
    ifelse(x<0, 0, 
           ifelse(x < 3, 1/4, 
                  ifelse(x < 4, 1/2, 
                         ifelse(x < 2*sqrt(7), x^2/32, 
                                ifelse(x < 30, 1-1/8, 1)))))
    }
x=seq(-5, 35, by = 0.01)
plot(x, F(x), type='l', xaxt='n', yaxt='n')
axis(side = 2, at = seq(0, 1, .25))
axis(side = 1, at = seq(0, 30, 5))
# library(ggplot2)
# ggplot(data.frame(x), aes(x)) + stat_function(fun=eq, geom = 'step')
```


Another way would be just applying the *law of the unconscious statistician* (<https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician>):

$$E\left[g(t)\right] = \int g(t)f_t(t)dt$$

$\mathbf{\textcolor{red}{E(X)}} = \displaystyle\int_{t=0}^kg(t)f_t(t)dt$

$\ \ \ \ \ \ = \displaystyle\int_{t=0}^1\cfrac{g(x)}{k}dt + \displaystyle\int_{t=1}^{k/2}\cfrac{g(x)}{k}dt + \displaystyle\int_{t=k/2}^{3k/4}\cfrac{g(x)}{k}dt + \displaystyle\int_{t=3k/4}^k\cfrac{g(x)}{k}dt$

$\ \ \ \ \ \ = \cfrac{\theta}{k}\left[t\right]_{t=0}^1 + \cfrac{A}{k}\left[\left(-\cfrac{2}{3}\right)(k-t)^{\frac{3}{2}}\right]_{t=1}^{k/2} + \cfrac{\theta/10}{k}\left[t\right]_{t=k/2}^{3k/4} + \cfrac{0}{k}\left[t\right]_{t=3k/4}^k$

$\ \ \ \ \ \ = \cfrac{\theta}{k} + \cfrac{2A}{3k}\left((k-1)^{\frac{3}{2}}-\left(\cfrac{k}{2}\right)^{\frac{3}{2}}\right) + \cfrac{\theta}{10k}\cfrac{k}{4} + 0 = \mathbf{\textcolor{red}{\cfrac{2A}{3k}\left((k-1)^{\frac{3}{2}}-\left(\cfrac{k}{2}\right)^{\frac{3}{2}}\right) + \cfrac{\theta}{40} + \cfrac{\theta}{k}}}$

The last approach is based on <http://homepage.stat.uiowa.edu/~nshyamal/22S175/DI.pdf>. A *mixed distribution* can be decomposed into a continuous distribution and a discrete one:

$$\mathbf{F(x) = F_C(x) + F_D(x)}$$

each with weights $p_C$ and $p_D$, respectively.

$p_D$ is the sum of the heights of the jumps:

$F(0)-F(0^-) = \cfrac{1}{4}$

$F\left(\cfrac{\theta}{10}\right) - F\left(\cfrac{\theta^-}{10}\right) = \cfrac{1}{4}$

$F(\theta) - F(\theta^-) = \cfrac{1}{k}$

$\ \ \ \ \ \ \Rightarrow p_D = \cfrac{1}{2} + \cfrac{1}{k} = \cfrac{k+2}{2k} \Rightarrow \mathbf{p_C = 1 - p_D} = \cfrac{k-2}{2k}$

$$\Pr(X_D=x) = \begin{cases}
\cfrac{\cfrac{1}{4}}{\cfrac{k+2}{2k}} = \cfrac{1}{4}\cfrac{2k}{k+2} = \cfrac{k}{2(k+2)} & x=0 \\
\cfrac{\cfrac{1}{4}}{\cfrac{k+2}{2k}} = \cfrac{1}{4}\cfrac{2k}{k+2} = \cfrac{k}{2(k+2)} & x=\cfrac{\theta}{10} \\
\cfrac{\cfrac{1}{k}}{\cfrac{k+2}{2k}} = \cfrac{1}{k}\cfrac{2k}{k+2} = \cfrac{2}{k+2} & x=\theta
\end{cases}$$

$$\mathbf{F_C(x) = \frac{F(x) + p_D \cdot F_D(x)}{p_C}}$$

$$\Rightarrow F_C(X_C=x) = \begin{cases}
0 & x < A\sqrt{\cfrac{k}{2}} \\
\cfrac{2x^2-kA^2}{A^2(k-2)} & A\sqrt{\cfrac{k}{2}} \leq x < A\sqrt{k-1} \\
1 & x \geq A\sqrt{k-1}
\end{cases}$$

$$\Rightarrow f_C(X_C=x) = \begin{cases}
0 & x < A\sqrt{\cfrac{k}{2}} \\
\cfrac{4x}{A^2(k-2)} & A\sqrt{\cfrac{k}{2}} \leq x < A\sqrt{k-1} \\
0 & x \geq A\sqrt{k-1}
\end{cases}$$

$E(X_D) = \displaystyle\sum_{x_k} x_k\Pr(X_D=x_k) = 0\cdot\cfrac{k}{2(k+2)} + \cfrac{\theta}{10}\cdot\cfrac{k}{2(k+2)} + \theta\cdot\cfrac{2}{k+2}$

$\ \ \ \ \ \ = \cfrac{k\theta}{20(k+2)} + \cfrac{2\theta}{k+2} = \cfrac{(k+40)\theta}{20(k+2)}$

$E(X_C) = \displaystyle\int_{x=-\infty}^\infty xf_C(x)dx = \displaystyle\int_{x=A\sqrt{k/2}}^{A\sqrt{k-1}} \cfrac{4x^2}{A^2(k-2)}dx = \left[\cfrac{4x^3}{3A^2(k-2)}\right]_{x=A\sqrt{k/2}}^{A\sqrt{k-1}} = \cfrac{4A}{3(k-2)}\left((k-1)^{\frac{3}{2}} - \left(\cfrac{k}{2}\right)^{\frac{3}{2}}\right)$

$$\mathbf{E(X) = p_D \cdot E(X_D) + p_C \cdot E(X_C)}$$

$\mathbf{\textcolor{red}{E(X)}} = \cfrac{k+2}{2k}\cdot\cfrac{(k+40)\theta}{20(k+2)}+\cfrac{k-2}{2k}\cdot\cfrac{4A}{3(k-2)}\left((k-1)^{\frac{3}{2}} - \left(\cfrac{k}{2}\right)^{\frac{3}{2}}\right)$

$\ \ \ \ \ \ = \mathbf{\textcolor{red}{\cfrac{2A}{3k}\left((k-1)^{\frac{3}{2}}-\left(\cfrac{k}{2}\right)^{\frac{3}{2}}\right) + \cfrac{\theta}{40} + \cfrac{\theta}{k}}}$

3. **Compute the variance of the payout from the contract.**

The 3rd link in (2) mentions the expression for the variance of a mixed random variable:

$$\mathbf{\textcolor{red}{Var(X) = \left(p_D \cdot Var(X_D) + p_C \cdot Var(X_C) \right) + p_D \cdot p_C \cdot \left(E(X_D) - E(X_C) \right)^2}}$$

We already computed $p_D, p_C, E(X_D)$, and $E(X_C)$. We'll compute now the variances of both components of $X$, omitting the final expression of $Var(X)$ (which is really complex and with a lot of terms):

$Var(X_D) = \displaystyle\sum_{x_k} \left(x_k-E(X_D)\right)^2 \Pr(x_k)$

$\ \ \ \ \ \ = \left(0 - \cfrac{(k+40)\theta}{20(k+2)} \right)^2 \cfrac{k}{2(k+2)} + \left(\cfrac{\theta}{10} - \cfrac{(k+40)\theta}{20(k+2)} \right)^2 \cfrac{k}{2(k+2)} + \left(\theta - \cfrac{(k+40)\theta}{20(k+2)} \right)^2 \cfrac{2}{k+2}$

$\ \ \ \ \ \ = \cfrac{(k+40)^2\theta^2}{400(k+2)^2} \cdot \cfrac{k}{2(k+2)} + \left(\cfrac{2\theta(k+2)-\theta(k+40)}{20(k+2)}\right)^2 \cdot \cfrac{k}{2(k+2)} + \left(\cfrac{20\theta(k+2)-\theta(k+40)}{20(k+2)}\right)^2 \cdot \cfrac{2}{k+2}$

$\ \ \ \ \ \ = \cfrac{k\theta^2(k+40)^2}{800(k+2)} + \cfrac{4k\theta^2(k+2)^2+k\theta^2(k+40)^2-4k\theta^2(k+2)(k+40)}{800(k+2)^2}$

$\ \ \ \ \ \ \ \ \ + \cfrac{1600\theta^2(k+2)^^2+4\theta^2(k+40)^2-80\theta^2(k+2)(k+40)}{800(k+2)^3}$

$\ \ \ \ \ \ = \cfrac{2\theta^2(k+2)(k+40)^2 + 4\theta^2(k+400)(k+2)^2-4\theta^2(k+20)(k+2)(k+40)}{800(k+2)^3}$

$\ \ \ \ \ \ = \cfrac{\theta^2(k+40)((k+40) - 2(k+20))}{400(k+2)^2} + \cfrac{\theta^2(k+400)}{200(k+2)}$

$\ \ \ \ \ \ = \cfrac{\theta^2(k+400)}{200(k+2)} - \cfrac{k\theta^2(k+40)}{400(k+2)^2} = \cfrac{\theta^2((k+400)(k+2) - k (k+40))}{400(k+2)^2}$

$\ \ \ \ \ \ = \cfrac{\theta^2(k^2+2k+400k+800-k^2-40k)}{400(k+2)^2} = \cfrac{\theta^2(362k+800)}{400(k+2)^2} = \cfrac{\theta^2(181k+400)}{200(k+2)^2}$

$Var(X_C) = \displaystyle\int_{x=-\infty}^\infty x^2 f_C(x)dx - \left(E(X_C)\right)^2$

$\ \ \ \ \ \ = \displaystyle\int_{x=A\sqrt{k/2}}^{A\sqrt{k-1}} \cfrac{4x^3}{A^2(k-2)}dx - \left(E(X_C)\right)^2 = \left[ \cfrac{x^4}{A^2(k-2)}\right]_{x=A\sqrt{k/2}}^{A\sqrt{k-1}} - \left(E(X_C)\right)^2$

$\ \ \ \ \ \ = \cfrac{A^4(k-1)^2 - A^4 \cfrac{k^2}{4}}{A^2(k-2)} - \left(E(X_C)\right)^2 = \cfrac{A^2(k^2 + 1 - 2k - \cfrac{k^2}{4}}{k-2} - \left(E(X_C)\right)^2$

$\ \ \ \ \ \ = \cfrac{A^2(3k^2 + 4 - 8k)}{4(k-2)} - \cfrac{16A^2}{9(k-2)^2} \left((k-1)^3 + \cfrac{k}{2} - 2 \left(\cfrac{k}{2}\right)^{\frac{3}{2}} (k-1)^{\frac{3}{2}}\right)$

> To get the full expression for $Var(X)$ we made use of <http://www.wolframalpha.com>:

$\mathbf{\textcolor{red}{Var(X) = \cfrac{\theta^2(181k+400)}{400k(k+2)}-\cfrac{A^2(k(k(37k-32\sqrt{2k(k-1)}-66)+32\sqrt{2k(k-1)}+44)+8)}{72(k-2)k}}}$

$\ \ \ \ \ \ \mathbf{\textcolor{red}{+ \cfrac{9\theta(k-2)^2(k-1)^3(k+40)-80A(k+2)}{720(k-2)(k-1)^3k^2}}}$

## Question 4

**Continuous random variables $X$ and $Y$ have a joint distribution with probability density function $f(x, y) = 2e^{-x}e^{-2y}$ for $0<x<\infty, 0<y<\infty$ and $0$ otherwise.**

1. **Compute $\Pr(X > a, Y < b)$, where $a, b$ are positive constants and $a < b$.**

$\Pr(X > a, Y < b) = \displaystyle\int_{x=a}^\infty \displaystyle\int_{y=0}^bf(x,y)dxdy$

$\ \ \ \ \ \ = \displaystyle\int_{x=a}^\infty \displaystyle\int_{y=0}^b2e^{-x}e^{-2y}dxdy = 2\left(\displaystyle\int_{x=a}^\infty e^{-x}dx \right)\left( \displaystyle\int_{y=0}^b e^{-2y}dy\right )$

$\ \ \ \ \ \ = 2 \left[-e^{-x} \right]_{x=a}^\infty \left[-\cfrac{e^{-2y}}{2}\right]_{y=0}^b = \left[ e^{-x} \right]_{x=a}^\infty \left[ e^{-2y} \right]_{y=0}^b = \left( 0-e^{-a}\right) \left(e^{-2b}-1 \right)$

$\ \ \ \ \ \ = \mathbf{\textcolor{red}{e^{-a}\left(1-e^{-2b} \right) = e^{-a}-e^{-a-2b}}}$

2. **Compute $\Pr(X < Y)$.**

$\Pr(X < Y) = \displaystyle\int_{x=0}^y \displaystyle\int_{y=0}^\infty f(x,y)dxdy$

$\ \ \ \ \ \ = \displaystyle\int_{x=0}^y \displaystyle\int_{y=0}^\infty 2e^{-x}e^{-2y}dxdy = 2 \displaystyle\int_{y=0}^\infty \left( \displaystyle\int_{x=0}^y e^{-x}dx \right) e^{-2y}dy$

$\ \ \ \ \ \ = 2 \displaystyle\int_{y=o}^\infty \left[-e^{-x} \right]_{x=0}^y e^{-2y}dy = 2 \displaystyle\int_{y=0}^\infty \left(1-e^{-y}\right)e^{-2y}dy = 2 \displaystyle\int_{y=0}^\infty \left( e^{-2y} - e^{-3y} \right)dy$

$\ \ \ \ \ \ = 2 \left[ -\cfrac{e^{-2y}}{2}+\cfrac{e^{-3y}}{3}\right]_{y=0}^\infty = 2 \left[ 0-\left( -\cfrac{1}{2}+\cfrac{1}{3} \right)\right] = 2 \left( \cfrac{1}{2} - \cfrac{1}{3}\right) = 1 - \cfrac{2}{3}$

$\ \ \ \ \ \ = \mathbf{\textcolor{red}{\cfrac{1}{3}}}$

3. **Compute $\Pr(X < a)$.**

$\Pr(X < a) = \displaystyle\int_{x=0}^a \displaystyle\int_{y=0}^\infty f(x,y)dxdy$

$\ \ \ \ \ \ = \displaystyle\int_{x=0}^a \displaystyle\int_{y=0}^\infty 2e^{-x}e^{-2y}dxdy = 2\left(\displaystyle\int_{x=0}^a e^{-x}dx \right)\left( \displaystyle\int_{y=0}^\infty e^{-2y}dy \right)$

$\ \ \ \ \ \ = 2 \left[-e^{-x} \right]_{x=0}^a \left[-\cfrac{e^{-2y}}{2}\right]_{y=0}^\infty = \left[ e^{-x} \right]_{x=0}^a \left[ e^{-2y} \right]_{y=0}^\infty= \left(e^{-a}-1\right) \left(0-1 \right)$

$\ \ \ \ \ \ = \mathbf{\textcolor{red}{1-e^{-a}}}$


\pagebreak

## Question 5

**Let $X$ be a random variable and $x$ be a real number. A linear function of the squared deviation from $x$ is another random variable, $Y = a+b(X - x)^2$, where $a$ and $b$ are some positive constant.**

1. **Find the value of $x$ that minimizes $E(Y)$. Show that your result is really the minimum.**

The *Law of the unconscious statistician* states that:

$$E\left[g(X)\right] = \int g(x)f_X(x)dx$$

So, if we call $\mu=E(X)$ and $\sigma^2=Var(X)$ (and knowing that $Var(X)=E\left(X^2\right) - \left(E(X)\right)^2=E\left(X^2\right)-\mu^2$):

$E(Y) = \displaystyle\int \left(a+b(X-x)^2 \right) f(X)dX$

$\ \ \ \ \ \ = \displaystyle\int \left( a + b(X^2 +x^2 -2xX\right)f(X)dX$

$\ \ \ \ \ \ = (a+bx^2)\displaystyle\int f(X)dX -2bx \displaystyle\int Xf(X)dX + b \displaystyle\int X^2f(X)dX$

$\ \ \ \ \ \ = (a+bx^2)1 -2bx\mu +b(\sigma^2-\mu^2)$

$\ \ \ \ \ \ = bx^2-2b\mu x + \left( a+b(\sigma^2+\mu^2)\right)$

Hence:

$\cfrac{dE(Y)}{dx} = 2bx-2b\mu = 2b(x-\mu) = 2b\left(x-E(X)\right)$

And consequently:

$\cfrac{dE(Y)}{dx} = 0 \Rightarrow \mathbf{\textcolor{red}{x=E(X)}}$


2. **Find the value of $E(Y)$ for the choice of $x$ you found in (1)?**

We just have to substitute in the last expression of $E(Y)

$\mathbf{\textcolor{red}{E(Y)}} = \displaystyle\int \left(a+b(X-\mu)^2 \right) f(X)dX$

$\ \ \ \ \ \ = b\mu^2-2b\mu^2+ \left( a+b(\sigma^2+\mu^2)\right) = -b\mu^2+a+b\sigma^2+b\mu^2$

$\ \ \ \ \ \ = \mathbf{\textcolor{red}{a+b\sigma^2}}$

3. **Suppose $Y = ax+b(X-x)^2$. Find the values of $x$ that minimizes $E(Y)$. Show that your result is really the minimum.**

$E(Y) = \displaystyle\int \left(ax+b(X-x)^2 \right) f(X)dX$

$\ \ \ \ \ \ = \displaystyle\int \left( ax + b(X^2 +x^2 -2xX\right)f(X)dX$

$\ \ \ \ \ \ = (ax+bx^2)\displaystyle\int f(X)dX -2bx \displaystyle\int Xf(X)dX + b \displaystyle\int X^2f(X)dX$

$\ \ \ \ \ \ = (ax+bx^2)1 -2bx\mu +b(\sigma^2-\mu^2)$

$\ \ \ \ \ \ = bx^2 + (a-2b\mu) x + b(\sigma^2+\mu^2)$

Hence:

$\cfrac{dE(Y)}{dx} = 2bx + (a-2b\mu) = 2b(x-\mu) + a = 2b\left(x-E(X)\right) + a$

And consequently:

$\cfrac{dE(Y)}{dx} = 0 \Rightarrow \mathbf{\textcolor{red}{x=E(X) - \cfrac{a}{2b}}}$


## Question 6

**Suppose $X$ and $Y$ are independent continuous random variables, where both of which are uniformly distributed between $0$ and $1$. Let random variable $Z = X + Y$.**

1. **Choose a value of $z$ between $0$ and $2$, and draw a graph depicting the region of the $X-Y$ plane for which $Z$ is less than $z$.**

First let's plot the thre variables:

```{r, echo=FALSE, message = FALSE, warning = FALSE, fig.cap = "Contour plot and 3D plot of $X, Y, Z$", fig.width = 6, fig.height = 4.5}
library(plot3D)
x <- y <- seq(0, 1, by = 0.01)
grid <- mesh(x, y)
z <- with(grid, x + y)
par(mfrow = c(1, 2))
contour(x, y, z, asp=1)
persp3D(z = z, x = x, y = y, theta = 0)
```

As shown above (especially in the contour plot on the left), $0 \leq Z \leq 1$, and for a given value of $z$ (let's use $z = 0.8$) the region of the $X-Y$ plane for which $Z<z$ will be a triangle with vertices $(0,0), (z,0)$, and $(0,z)$.

\pagebreak

```{r, echo=FALSE, message = FALSE, warning = FALSE, fig.cap = "Region of the $X-Y$ plane for which $Z<z$ where $z=0.8$", fig.width = 6, fig.height = 4.5}
N <- 100e3 # number of simulations
X <- runif(N, min=0, max=1)
Y <- runif(N, min=0, max=1)
Z <- X+Y

z = 0.8

plot(X[Z<z], Y[Z<z], col='red', asp = 1)
polygon(x=c(0,z,0), y=c(0,0,z))
# abline(a=z, b=-1); abline(h=0); abline(v=0)
```

2. **Derive the probability density function, $f(z)$.**

There is a theorem that states:

> Let $X$ and $Y$ be two independent random variables with density functions $f_X(x)$ and $f_Y(y)$. Then the sum $Z = X + Y$ is a random variable with density function $f_Z(z)$, where $f_Z$ is the convolution of $f_X$ and $f_Y$.

$$(f*g)(z) = \int_{x=-\infty}^{+\infty}f_Y(z-x)f_X(x)dx = \int_{y=-\infty}^{+\infty}f_X(z-y)f_Y(y)dy$$

In our case:

$$f_X(x) = f_Y(y) = \begin{cases}
1 & 0\leq x\leq\ 1 \\
0 & Otherwise
\end{cases}$$

So: 

$f_Z(z) = \displaystyle\int_{x=0}^1 f_Y(z-x)dx$

$f_Y(z-x) \neq 0 \Leftrightarrow 0 \leq z-x \leq 1 \Leftrightarrow z-1 \leq x \leq z$

So for $0 \leq z \leq 1$

$\ \ \ \ \ \ f_Z(z) = \displaystyle\int_{x=0}^z dx = z$

While for $1 \leq z \leq 2$

$\ \ \ \ \ \ f_Z(z) = \displaystyle\int_{x=z-1}^1 dx = 1-(z-1)=2-z$

In summary:

$$\mathbf{\textcolor{red}{f_Z(z) = \begin{cases}
z & 0\leq z\leq 1 \\
2 - z & 1\textless z\leq 2 \\
0 & Otherwise
\end{cases}}}$$

```{r, echo=FALSE, message = FALSE, warning = FALSE, fig.cap = "Histogram and (approximate) density distribution plot of $Z$", fig.width = 6, fig.height = 4.5}
par(mfrow = c(1, 2))
hist(Z, freq = FALSE)
plot(density(Z), main = 'Density distribution of Z')
```


\pagebreak

## Question 7

**In a casino, you pay the following game. A pair fair, ordinary 6-faced dices is rolled. If the sum of the dice is $2$, $3$, or $12$, the house wins. If it is $7$ or $11$, you win. If it is any other number $x$, the house rolls the dice again until the sum is either $7$ or $x$. If it is $7$, the house wins. If it is $x$, you win. A game ends if one of the two players wins. Let $Y$ be the number of rolls needed until the game ends.**

1. **Is the expected number of rolls given that you win more than, equal to, or less than the expected number of rolls given that house wins (in a game)? The steps to arrive at your answer numerically need to be clearly shown.**

<http://www.dehn.wustl.edu/~blake/courses/WU-Ed6021-2012-Summer/handouts/Expected_Value.pdf>

First we check how many possible games there are, and the probabilities for each outcome:

```{r, echo=FALSE, message = FALSE, warning = FALSE}
games <- sapply(seq(1,6), function(x) x+seq(1,6))
colnames(games) <- sapply(seq(1,6), function(x) paste("**Die 1=", x, '**', sep=""))
rownames(games) <- sapply(seq(1,6), function(x) paste("**Die 2=", x, '**', sep=""))
library(knitr)
kable(games, align = 'c', caption = 'Possible games')
# library(pander)
# panderOptions('table.split.table', Inf)
# set.caption("Possible games")
# pander(games, style = 'rmarkdown')
```

From the table above we can derive the following:

$$\Pr(X=x) =\begin{cases}
\cfrac{x-1}{36} & x \in \{2,\dots,7\} \\
\cfrac{13-x}{36} & x \in \{8,\dots,12\} \\
\end{cases}$$

$\ \ \ \ \ \ \Pr(X=2)=\Pr(X=12)=\cfrac{1}{36}$

$\ \ \ \ \ \ \Pr(X=3)=\Pr(X=11)=\cfrac{1}{18}$

$\ \ \ \ \ \ \Pr(X=4)=\Pr(X=10)=\cfrac{1}{12}$

$\ \ \ \ \ \ \Pr(X=5)=\Pr(X=9)=\cfrac{1}{9}$

$\ \ \ \ \ \ \Pr(X=6)=\Pr(X=8)=\cfrac{5}{36}$

$\ \ \ \ \ \ \Pr(X=7)=\cfrac{1}{6}$

Let $Z$ be the event that I win (so $\bar{Z}$ is the event that house wins).

$\Pr(Z, 2 \text{ on roll } 1) = 0, \Pr(\bar{Z}, 2 \text{ on roll } 1) = \cfrac{1}{36}$

$\Pr(Z, 3 \text{ on roll } 1) = 0, \Pr(\bar{Z}, 3 \text{ on roll } 1) = \cfrac{1}{18}$

$\Pr(Z, 7 \text{ on roll } 1) = \cfrac{1}{6}, \Pr(\bar{Z}, 7 \text{ on roll } 1) = 0$

$\Pr(Z, 11 \text{ on roll } 1) = \cfrac{1}{18}, \Pr(\bar{Z}, 11 \text{ on roll } 1) = 0$

$\Pr(Z, 12 \text{ on roll } 1) = 0, \Pr(\bar{Z}, 12 \text{ on roll } 1) = \cfrac{1}{36}$

When we roll a $4, 5, 6, 8, 9$, or $10$, we need to roll again, and the final result will depend on what we obtain in subsequent rolls (the same value, $7$, or another):

$\Pr(Z, 4 \text{ on roll } 1) = \Pr(4)\Pr(4) + \Pr(4)\Pr(\text{neither 4 nor 7})\Pr(4) + \dots$

$\Pr(\text{neither 4 nor 7}) = 1 - \cfrac{3+6}{36}=1-\cfrac{1}{4}=\cfrac{3}{4}$

$\Pr(Z, 4 \text{ on roll } 1) = \left(\cfrac{1}{12}\right)^2\left(1+\left(\cfrac{3}{4}\right)+\left(\cfrac{3}{4}\right)^2+\dots\right) = \left(\cfrac{1}{12}\right)^2\displaystyle\sum_{y=0}^\infty\left(\cfrac{3}{4}\right)^y=\left(\cfrac{1}{12}\right)^2\cfrac{1}{1-\cfrac{3}{4}}=\cfrac{4}{12^2}=\cfrac{1}{36}$

$\Pr(\bar{Z}, 4 \text{ on roll } 1) = \Pr(4)\Pr(7) + \Pr(4)\Pr(\text{neither 4 nor 7})\Pr(7) + \dots = \cfrac{1}{12}\cfrac{1}{6}\displaystyle\sum_{y=0}^\infty\left(\cfrac{3}{4}\right)^y=\cfrac{1}{12}\cfrac{1}{6}4=\cfrac{1}{18}$

The same applies to $x=10$.

$\Pr(Z, 5 \text{ on roll } 1) = \Pr(5)\Pr(5) + \Pr(5)\Pr(\text{neither 5 nor 7})\Pr(5) + \dots$

$\Pr(\text{neither 5 nor 7}) = 1 - \cfrac{4+6}{36}=\cfrac{26}{36}=\cfrac{13}{18}$

$\Pr(Z, 5 \text{ on roll } 1) = \left(\cfrac{1}{9}\right)^2\cfrac{1}{1-\cfrac{13}{18}}=\left(\cfrac{1}{9}\right)^2 \cfrac{18}{5}=\cfrac{2}{45}$

$\Pr(\bar{Z}, 5 \text{ on roll } 1) = \Pr(5)\Pr(7) + \Pr(5)\Pr(\text{neither 5 nor 7})\Pr(7) + \dots = \cfrac{1}{9}\cfrac{1}{6}\displaystyle\sum_{y=0}^\infty\left(\cfrac{13}{18}\right)^y=\cfrac{1}{9}\cfrac{1}{6}\cfrac{18}{5}=\cfrac{1}{15}$

The same applies to $x=9$.

$\Pr(Z, 6 \text{ on roll } 1) = \Pr(6)\Pr(6) + \Pr(6)\Pr(\text{neither 6 nor 7})\Pr(6) + \dots$

$\Pr(\text{neither 6 nor 7}) = 1 - \cfrac{5+6}{36}=\cfrac{25}{36}$

$\Pr(Z, 6 \text{ on roll } 1) = \left(\cfrac{5}{36}\right)^2\cfrac{1}{1-\cfrac{25}{36}}=\left(\cfrac{5}{36}\right)^2 \cfrac{36}{11}=\cfrac{25}{396}$

$\Pr(\bar{Z}, 6 \text{ on roll } 1) = \Pr(6)\Pr(7) + \Pr(6)\Pr(\text{neither 6 nor 7})\Pr(7) + \dots = \cfrac{5}{36}\cfrac{1}{6}\displaystyle\sum_{y=0}^\infty\left(\cfrac{25}{36}\right)^y=\cfrac{5}{36}\cfrac{1}{6}\cfrac{36}{11}=\cfrac{5}{66}$

The same applies to $x=8$.

$\Pr(Z) = \displaystyle\sum_{x=2}^12 \Pr(Z, \text{x on roll 1}) = 0 + 0 + \cfrac{1}{36} + \cfrac{2}{45} + \cfrac{25}{396} + \cfrac{1}{6} + \cfrac{25}{396} + \cfrac{2}{45} + \cfrac{1}{36} + \cfrac{1}{18} + 0$

$\ \ \ \ \ \ = \cfrac{55+88+125+330+125+88+55+110}{1980} = \cfrac{244}{495} \approx `r formatC(244/495, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`\Rightarrow \Pr(\bar{Z}) = \cfrac{251}{495} \approx `r formatC(251/495, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`$

What we are asked for is:

$$E[Y | Z] = \sum_{y=1}^\infty y \Pr(y | Z)$$

In order to compute $\Pr(y | Z) = \cfrac{\Pr(y,Z)}{\Pr(Z)}$ we need to consider the two main sets of cases: $Y=1$ and $Y \geq 2$:

$\Pr(Y=1|Z) = \cfrac{\Pr(Y=1,Z)}{\Pr(Z)} = \cfrac{\Pr(Z, X=7)+\Pr(Z,X=11)}{\Pr(Z)} = \cfrac{\cfrac{1}{6} + \cfrac{1}{18}}{\cfrac{244}{495}} = \cfrac{55}{122}$

For $y \geq 2$:

$\Pr(Y=y|Z) = \cfrac{1}{\Pr(Z)} \displaystyle\sum_{x \in \{4,5,6,8,9,10\}} \Pr(x,y,Z) = \cfrac{495}{244}\left(\left(\cfrac{1}{12}\right)^2\left(\cfrac{3}{4}\right)^{y-2} + \left(\cfrac{1}{9}\right)^2\left(\cfrac{13}{18}\right)^{y-2} + \dots\right)$

$\ \ \ \ \ \ = 2\cfrac{495}{244}\left(\left(\cfrac{1}{12}\right)^2\left(\cfrac{3}{4}\right)^{y-2} + \left(\cfrac{1}{9}\right)^2\left(\cfrac{13}{18}\right)^{y-2} + \left(\cfrac{5}{36}\right)^2\left(\cfrac{25}{36}\right)^{y-2} \right)$

It can be proved that:

$$\displaystyle\sum_{y=2}^\infty y\cdot p^{y-2} = \cfrac{2-p}{(1-p)^2}$$

So:

$\mathbf{\textcolor{red}{E[Y|Z]}}=1\cdot\cfrac{55}{122}+\cfrac{495}{122}\left(\left(\cfrac{1}{12}\right)^2\cfrac{2-\cfrac{3}{4}}{\left(1-\cfrac{3}{4}\right)^2} + \left(\cfrac{1}{9}\right)^2\cfrac{2-\cfrac{13}{18}}{\left(1-\cfrac{13}{18}\right)^2} + \left(\cfrac{5}{36}\right)^2\cfrac{2-\cfrac{25}{36}}{\left(1-\cfrac{25}{36}\right)^2} \right)$

$\ \ \ \ \ \ = \cfrac{55}{122}+\cfrac{495}{122}\left(\cfrac{1}{144}\cfrac{5}{4}\cfrac{16}{1}+\cfrac{1}{81}\cfrac{23}{18}\cfrac{18^2}{25}+\cfrac{25}{36^2}\cfrac{47}{36}\cfrac{36^2}{121}\right) = \cfrac{55}{122}+\cfrac{495}{122}\left(\cfrac{5}{36}+\cfrac{46}{225}+\cfrac{1175}{4356}\right) = \cfrac{9858}{3355} \approx \mathbf{\textcolor{red}{`r formatC(9858/3355, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`}}$

$$E[Y | \bar{Z}] = \sum_{y=1}^\infty y \Pr(y | \bar{Z})$$

$\Pr(y | \bar{Z}) = \cfrac{\Pr(y,\bar{Z})}{\Pr(\bar{Z})}$

$\Pr(Y=1|\bar{Z}) = \cfrac{\Pr(Y=1,\bar{Z})}{\Pr(\bar{Z})} = \cfrac{\Pr(\bar{Z}, X=2)+\Pr(\bar{Z},X=3) + \Pr(\bar{Z},X=12)}{\Pr(Z)} = \cfrac{\cfrac{1+2+1}{36}}{\cfrac{251}{495}} = \cfrac{55}{251}$

For $y \geq 2$:

$\Pr(Y=y|\bar{Z}) = \cfrac{1}{\Pr(\bar{Z})} \displaystyle\sum_{x \in \{4,5,6,8,9,10\}} \Pr(x,y,\bar{Z}) = \cfrac{495}{251}\left(\cfrac{1}{12}\cfrac{1}{6}\left(\cfrac{3}{4}\right)^{y-2} + \cfrac{1}{9}\cfrac{1}{6}\left(\cfrac{13}{18}\right)^{y-2} + \dots\right)$

$\ \ \ \ \ \ = 2\cfrac{495}{251}\left(\cfrac{1}{12}\cfrac{1}{6}\left(\cfrac{3}{4}\right)^{y-2} + \cfrac{1}{9}\cfrac{1}{6}\left(\cfrac{13}{18}\right)^{y-2} + \cfrac{5}{36}\cfrac{1}{6}\left(\cfrac{25}{36}\right)^{y-2} \right)$

$\mathbf{\textcolor{red}{E[Y|\bar{Z}]}}=1\cdot\cfrac{55}{251}+\cfrac{990}{251}\left(\cfrac{1}{12}\cfrac{1}{6}\cfrac{2-\cfrac{3}{4}}{\left(1-\cfrac{3}{4}\right)^2} + \cfrac{1}{9}\cfrac{1}{6}\cfrac{2-\cfrac{13}{18}}{\left(1-\cfrac{13}{18}\right)^2} + \cfrac{5}{36}\cfrac{1}{6}\cfrac{2-\cfrac{25}{36}}{\left(1-\cfrac{25}{36}\right)^2} \right)$

$\ \ \ \ \ \ = \cfrac{55}{251}+\cfrac{990}{251}\left(\cfrac{1}{12}\cfrac{1}{6}\cfrac{5}{4}\cfrac{16}{1}+\cfrac{1}{9}\cfrac{1}{6}\cfrac{23}{18}\cfrac{18^2}{25}+\cfrac{5}{36}\cfrac{1}{6}\cfrac{47}{36}\cfrac{36^2}{121}\right) = \cfrac{55}{251}+\cfrac{990}{251}\left(\cfrac{5}{18}+\cfrac{23}{75}+\cfrac{235}{726}\right) = \cfrac{52473}{13805} \approx \mathbf{\textcolor{red}{`r formatC(52473/13805, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`}}$

> Again, we used <http://www.wolframalpha.com> (or the R function `fractions {MASS}`) to simplify expressions.

$$\mathbf{\textcolor{red}{E[Y|Z]<E[Y|\bar{Z}]}}$$

Let's prove it with a simulation:

```{r, echo=TRUE, message = FALSE, warning = FALSE}
N <- 100e3 # number of simulations
Y <- rep(0, N) # value of Y (number of rolls for every simulation)
Z <- rep(0, N) # value of Z (0: house wins; 1: I win)
for (i in 1:N) {
    y <- 0 # initialize number of rolls
    x <- sample(seq(6), 1) + sample(seq(6), 1) # simulate two dice
    y <- y+1 # update number of rolls
    first_x <- x # store x (will be used if it's 4, 5, 6, 8, 9, 10)
    # House wins if 2, 3, 12
    # I win if 7, 11
    # Otherwise, keep that value of x (and insert in z)
    z <- ifelse(x==2 | x==3 | x==12, 0, 
                ifelse(x==7 | x==11, 1, first_x))
    # While z > 1 (i.e., if nobody wins)
    while (z>1) {
        x <- sample(seq(6), 1) + sample(seq(6), 1) # simulate other roll
        y <- y+1 # update number of rolls
        # House wins if 7
        # I win if same value of 1st roll
        z <- ifelse(x==7, 0, ifelse(x==first_x , 1, first_x))
    }
    # When finished, store values for that simulation
    Y[i] <- y
    Z[i] <- z
}

mean(Z) # E[Z] (=Pr(Z=1) since Z is binary); should be close to 0.493 (244/495)

mean(Y[Z==0]) # E[Y|Z]; should be close to 3.8010
mean(Y[Z==1]) # E[Y|not Z]; should be close to 2.9383
```


2. **Suppose it takes $\$20$ to pay, and the payoff is $\$100$, $\$80$, $\$60$, $\$40$, $\$0$ if you win in the 1st, 2nd, 3rd, 4th, 5th round, respectively. That is, if you win in the 1st round, you are paid $\$100$ (so your net profit is $\$80$), if you win in the 2nd round, you are paid $\$80$, etc. Derive the expected payoff function of a game.**

Let $W$ be the payoff:

$\mathbf{\textcolor{red}{E[W] = \displaystyle\sum_{\substack{w \in \{0,40,\\60,80,100\}}} w \Pr(w) = \sum_{\substack{w \in \{0,40,\\60,80,100\}}} w \Pr(y) = \sum_{\substack{w \in \{0,40,\\60,80,100\}}} w \sum_{Z=0}^1 w \Pr(y,Z) = \sum_{\substack{w \in \{0,40,\\60,80,100\}}} w \sum_{Z=0}^1 w \Pr(y|Z)\Pr(Z)}}$

We can focus on the event $Z=1$ since $\bar{Z} \Rightarrow W=0$:

$E[W] = \mathbf{\textcolor{red}{\displaystyle\sum_{\substack{w \in \{0,40,\\60,80,100\}}} w \Pr(y|Z=1) \Pr(Z=1) = \Pr(Z=1) \sum_{\substack{w \in \{40,60,\\80,100\}}} w \Pr(y|Z=1)}}$

$\ \ \ \ \ \ = \Pr(Z=1) \displaystyle\sum_{y=1}^4 w(y) \Pr(y|Z=1)$

$\ \ \ \ \ \ = \cfrac{244}{495} \left(100 \cdot \Pr(Y=1|Z=1) + \sum_{y=2}^4 w(y) \cfrac{495}{122}\left(\left(\cfrac{1}{12}\right)^2\left(\cfrac{3}{4}\right)^{y-2}\right)\right)$


$\ \ \ \ \ \ = \cfrac{244}{495} \left(100 \cdot \cfrac{55}{122} + \displaystyle\sum_{y=2}^4 w(y) \cfrac{495}{122}\left(\left(\cfrac{1}{12}\right)^2\left(\cfrac{3}{4}\right)^{y-2}+ \left(\cfrac{1}{9}\right)^2\left(\cfrac{13}{18}\right)^{y-2} + \left(\cfrac{5}{36}\right)^2\left(\cfrac{25}{36}\right)^{y-2}\right)\right)$

$\ \ \ \ \ \ = 100 \cdot \cfrac{110}{495} + 80 \cdot 2\left(\left(\cfrac{1}{12}\right)^2+ \left(\cfrac{1}{9}\right)^2 + \left(\cfrac{5}{36}\right)^2\right)$

$\ \ \ \ \ \ \ \ \ + 60 \cdot 2\left(\left(\cfrac{1}{12}\right)^2\left(\cfrac{3}{4}\right)+ \left(\cfrac{1}{9}\right)^2\left(\cfrac{13}{18}\right) + \left(\cfrac{5}{36}\right)^2\left(\cfrac{25}{36}\right)\right)$

$\ \ \ \ \ \ \ \ \ + 40 \cdot 2\left(\left(\cfrac{1}{12}\right)^2\left(\cfrac{3}{4}\right)^2+ \left(\cfrac{1}{9}\right)^2\left(\cfrac{13}{18}\right)^2 + \left(\cfrac{5}{36}\right)^2\left(\cfrac{25}{36}\right)^2\right)$

$\ \ \ \ \ \ = 100 \cdot \cfrac{110}{495} + \cfrac{1}{144}\left(160 \cdot 1 + 120 \cdot \cfrac{3}{4} + 80 \cdot \cfrac{9}{16}\right)$

$\ \ \ \ \ \ \ \ \ + \cfrac{1}{81}\left(160 \cdot 1 + 120 \cdot \cfrac{13}{18} + 80 \cdot \cfrac{25}{36}\right)$

$\ \ \ \ \ \ \ \ \ + \cfrac{25}{1296}\left(160 \cdot 1 + 120 \cdot \cfrac{25}{36} + 80 \cdot \cfrac{625}{1296}\right)$

$\ \ \ \ \ \ = \mathbf{\textcolor{red}{\cfrac{160658}{4829} \approx `r formatC((100*110/495+1/144*(160+120*3/4+80*9/16)+1/81*(160+120*13/18+80*169/324)+25/1296*(160+120*25/36+80*625/1296)), format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`}}$


Again, let's confirm this result with a simulation:

```{r, echo=TRUE, message = FALSE, warning = FALSE}
N <- 100e3 # number of simulations
Y <- rep(0, N) # value of Y (number of rolls for every simulation)
Z <- rep(0, N) # value of Z (0: house wins; 1: I win)
P <- rep(0, N) # payoff
for (i in 1:N) {
    y <- 0 # initialize number of rolls
    x <- sample(seq(6), 1) + sample(seq(6), 1) # simulate two dice
    y <- y+1 # update number of rolls
    first_x <- x # store x (will be used if it's 4, 5, 6, 8, 9, 10)
    # House wins if 2, 3, 12
    # I win if 7, 11
    # Otherwise, keep that value of x (and insert in z)
    z <- ifelse(x==2 | x==3 | x==12, 0, 
                ifelse(x==7 | x==11, 1, first_x))
    p <- ifelse(z==1, 100, 0) # if I win 1st time, payoff is 100
    # While z > 1 (i.e., if nobody wins)
    j <- 2 # keep count of rolls
    while (z>1) {
        x <- sample(seq(6), 1) + sample(seq(6), 1) # simulate other roll
        y <- y+1 # update number of rolls
        # House wins if 7
        # I win if same value of 1st roll
        z <- ifelse(x==7, 0, ifelse(x==first_x , 1, first_x))
        # Compute payoff if I win (with less than 5 rolls)
        p <- ifelse(z==1 & j<5, 100-20*(j-1), 0)
        j <- j+1 # update count of rolls
    }
    # When finished, store values for that simulation
    Y[i] <- y
    Z[i] <- z
    P[i] <- p
}

mean(P) # E[P]; should be close to 33.2694
```

To calculate the net profit we would just have to subtract $\$20$ to every game:

$E[\text{net profit}] = E[W-20] = E[W]-20 = `r formatC((100*110/495+1/144*(160+120*3/4+80*9/16)+1/81*(160+120*13/18+80*169/324)+25/1296*(160+120*25/36+80*625/1296)-20), format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`$

(It's positive, so it does not seem likely that the house would choose payoffs so high: usually they are selected so $E[\text{net profit}] < 0$, so the house always wins on the long term.)



**********

\pagebreak

# Part III: Statistical Estimation and Statistical Inference

**In classical statistics, parameters are unknown constants whereas estimators are functions of samples and are random variables. The questions in this section are designed to clarify the relationship between parameters and estimators, and explore the properties that different estimators may have.**

## Question 8

**Let $Y1, \dots, Yn$ be $n$ random variables, such that any two of them are uncorrelated, and all share the same mean $\mu$ and variance $\sigma^2$. Let $Y$ be the average $Y_i$, which is also a random variable.**

**Define the class of linear estimators of $\mu$ by**

$$\mathbf{W = \sum_{i=1}^n a_iY_i}$$

**where the $a_i$ are constants.**

1. **What restriction on the $a_i$ is needed for $W$ to be an unbiased estimator of $\mu$?**

For $W$ to be an unbiased estimator of $\mu$, the following condition has to be met:

$E[W]=\mu$

$E[W] = E\left[\displaystyle\sum_{i=1}^n a_iY_i\right] = \displaystyle\sum_{i=1}^n a_iE[Y_i] = \displaystyle\sum_{i=1}^n a_i\mu = \mu \displaystyle\sum_{i=1}^n a_i$

So the condition is $\mathbf{textcolor{red}{\sum_{i=1}^n a_i=1}}$ (for example---but not necessarily---, $a_i=\cfrac{1}{n} \ \forall i$).

2. **Find $Var(W)$.**

$\mathbf{\textcolor{red}{Var(W)}} = Var\left(\displaystyle\sum_{i=1}^n a_iY_i\right) = \displaystyle\sum_{i=1}^n a_i^2Var(Y_i) = \displaystyle\sum_{i=1}^n a_i^2\sigma^2 = \mathbf{\textcolor{red}{\sigma^2\displaystyle\sum_{i=1}^n a_i^2}}$

3. **Given a set of numbes $a_1,\dots,a_n$, the following inequality holds:**

    $$\mathbf{\frac{1}{n}\left(\sum_{i=1}^na_i\right)^2 \leq \sum_{i=1}^na_i^2}$$

    **Use this inequality, along with the previous parts of this question, tho show that $Var(W) \geq Var(\bar{Y})$ whenever $W$ is unbiased. We say that $\bar{Y}$ is the best linear unbiased estimator (BLUE).**

As proved in (1), if $W$ is unbiased, $\displaystyle\sum_{i=1}^n a_i = 1$, and hence:

$\displaystyle\sum_{i=1}^na_i^2 \geq \cfrac{1}{n}$

And following the result of (2), we have:

$\mathbf{\textcolor{red}{Var(W) = \sigma^2 \displaystyle\sum_{i=1}^n a_i^2 \geq \sigma^2 = Var(\bar{Y})}}$

## Question 9

**Let $\bar{Y}$ denote the average of $n$ independent draws from a population distribution with mean $\mu$ and variance $\sigma^2$. Consider two alternative estimators of $\mu$: $W_1 = \frac{n-1}{n}\bar{Y}$ and $W_2=k\bar{Y}$, where $0<k<1$.**

1. **Compute the biases of both $W_1$ and $W_2$. Which estimator is consistent?**

$\mathbf{\textcolor{red}{Bias(W_1)}} = E(W_1)-\mu = E\left(\cfrac{n-1}{n}\bar{Y}\right)-\mu=\cfrac{n-1}{n}E(\bar{Y})-\mu=\left(\cfrac{n-1}{n}-1\right)\mu=\mathbf{\textcolor{red}{-\cfrac{1}{n}}}$

$\mathbf{\textcolor{red}{Bias(W_2)}} = E(W_2)-\mu = E(k\bar{Y})-\mu=kE(\bar{Y})-\mu=(k-1)\mu=\mathbf{\textcolor{red}{-\mu(1-k)}}$

An estimator $W$ is consistent if $\forall \varepsilon>0$

$$\lim_{n \to \infty}\Pr(|Bias(W)|>\varepsilon) \to 0$$

**\textcolor{red}{$\mathbf{W_1}$ meets this condition (it's consistent), but $\mathbf{W_2}$ does not.}**

2. **Compute $Var(W_1)$ and $Var(W_2)$. Which estimator has lower variance?**

$\mathbf{\textcolor{red}{Var(W_1)}} = Var\left(\cfrac{n-1}{n}\bar{Y}\right) = \left(\cfrac{n-1}{n}\right)^2Var(\bar{Y}) = \left(\cfrac{n-1}{n}\right)^2\cfrac{\sigma^2}{n} = \mathbf{\textcolor{red}{\cfrac{(n-1)^2\sigma^2}{n^3}}}$

$\mathbf{\textcolor{red}{Var(W_2)}} = Var\left(k\bar{Y}\right) = k^2Var(\bar{Y}) = \mathbf{\textcolor{red}{k^2\cfrac{\sigma^2}{n}}}$

Each variance dependes on the values of $n$ and $k$, respectively. Let's see how they should compare to each other for $Var(W_1)$ to be lower than $Var(W_2)$ (and vice versa):

$Var(W_1) < Var(W_2) \Leftrightarrow \left(\cfrac{n-1}{n}\right)^2<k^2 \Rightarrow \cfrac{n-1}{n} < k \Rightarrow n(1-k)<1 \Rightarrow n < \cfrac{1}{1-k}$

```{r, echo=FALSE, message = FALSE, warning = FALSE, fig.cap = "Maximum value of $n$ as function of $k$ for $Var(W_1)$ to be lower than $Var(W_2)$", fig.width = 4, fig.height = 4}
k <- seq(0.001, .999, .001)
n <- floor(1/(1-k))
plot(k[n>=2], n[n>=2])
```

So the closer $k$ is to 1, the largest $n$ can be for $Var(W_1)$ to be greater than $Var(W_2)$. But $n$ has to be an integer greater than $2$, which means that:

$2 \leq n < \cfrac{1}{1-k} \Rightarrow 2(1-k) < 1 \Rightarrow 2-2k<1 \Rightarrow 1<2k \Rightarrow k>\cfrac{1}{2}$

I.e., **\textcolor{red}{for $\mathbf{k>\cfrac{1}{2}, Var(W_1)<Var(W_2)}$ as long as $\mathbf{n<\cfrac{1}{1-k}}$. For $\mathbf{k<\cfrac{1}{2}, Var(W_1)>Var(W_2)}$ regardless of the value of $\mathbf{n}$. If $\mathbf{k=\cfrac{1}{2}, Var(W_1)=Var(W_2)}$ if $\mathbf{n=2}$ and $\mathbf{Var(W_1)>Var(W_2)}$ for any $\mathbf{n>2}$}**.


## Question 10

**Given a random sample $Y_1, Y_2, \dots, Y_n$ from some distribution $F(\cdot )$ with mean $\mu$ and variance $\sigma^2$, where both $\mu$ and $\sigma^2$ are unknown parameters.**

**Let $\bar{Y}$ be the average of the sample. Consider the following estimator for $\sigma^2$:**

$$\widehat{\sigma^2} = \frac{1}{n}\sum_{i=1}^n \left(Y - \bar{Y} \right)^2$$

1. **Show that $E(\bar{Y}) = E(Y_i) \ \forall i \in 1,2,\dots,n$**

2. **Show that $Var(\bar{Y}) = \frac{1}{n}Var(Y_i) \ \forall i \in 1,2,\dots,n$**

3. **Compute the expectation of $\widehat{\sigma^2}$ in terms of $n$ and $\sigma^2$. In your derivation, make sure make use of the *i.i.d.* property and identify where you use it.**

4. **Is this an unbiased estimator for $\sigma^2$?**

5. **If not, what function of $\widehat{\sigma^2}$ produce an unbiased estimator?**


## Question 11

**Wooldridge's textbook: Appendix C, Question 4*i*, *ii*, *iii*.**


## Question 12

**Wooldridge's textbook: Appendix C, Question 6.**


## Question 13

**Wooldridge's textbook: Appendix C, Question 8**

**For a given player, the outcome of a particular shot can be modeled as a Bernoulli variable: if $Y_i$ is the outcome of shot i, then $Y_i$ = 1 if the shot is made, and $Y_i$ = 0 if the shot is missed. Let $\theta$ denote the probability of making any particular three-point shot attempt. The natural estimator of $\theta$ is $\bar{Y} = FGM/FGA$.**

i. **Estimate $\theta$ for Mark Price.**

$$\mathbf{\textcolor{red}{\hat{\theta}_{\text{Mark Price}}}} = \bar{Y}_{\text{Mark Price}} = \cfrac{FGM_{\text{Mark Price}}}{FGA_{\text{Mark Price}}} = \mathbf{\textcolor{red}{\cfrac{188}{429} \approx `r formatC(188/429, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`}}$$

ii. **Find the standard deviation of the estimator $\bar{Y}$ in terms of $\theta$ and the number of shot attempts $n$.**

For each trial (which is a Bernoulli process):

$$Var(Y_i) = \theta(1-\theta)$$

Thus for the sample mean we have:

$$Var(\bar{Y}) = \cfrac{\theta(1-\theta)}{n}$$

$$sd(\bar{Y}) = \sqrt{\cfrac{\theta(1-\theta)}{n}}$$

iii. **Test $H_0: \theta = .5$ against $H_1: \theta < .5$ for Mark Price. Use a 1% significance level.**
    
$$t=\cfrac{\bar{Y}-\theta_0}{se(\bar{Y})} = \cfrac{(\theta-\theta_0)\sqrt{n}}{\sqrt{\theta(1-\theta)}}$$

```{r, echo=TRUE, message = FALSE, warning = FALSE}
n <- 429
theta <- 188/n
theta_0 <- 0.5
(se <- sqrt(theta*(1-theta)/n))
(t <- (theta-theta_0)/se)
```

For $\theta_0=0.5, n=429$, and $\theta \approx `r formatC(188/429, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`$ we have:

$$t=`r formatC(t, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`$$

```{r, echo=FALSE, message = FALSE, warning = FALSE}
(alpha <- pt(t, n-1))
```

Since we have a one-sided alternative:

$$\Pr(t<`r formatC(t, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`)=F_t(`r formatC(t, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`, `r n-1`) =`r formatC(alpha, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`$$

And since the probability we have obtained is lower than 1%, **\textcolor{red}{at that significance level we reject the null hypothesis that $\theta_0 = 0.5$}** (do note that had our altenative hypothesis been double-sided, $H_1: \theta_0 \neq 0.5$, we would have had to consider both tails, leading to a probability of $`r formatC(2*alpha, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`$).


1. **Define Type I error.**

Rejecting the null hypothesis when it is true.

2. **What is the probability of Type I error of this test?**

$\mathbf{\textcolor{red}{1\%}}$

3. **Define Type II error.**

Failing to reject the null hypothesis when it is false.

4. **What is the probability of Type II error when using this decision rule, assuming the "true" population proportion is $\theta^*= 0.45$.**

```{r, echo=TRUE, message = FALSE, warning = FALSE}
true_theta <- 0.45
sig <- 0.01
(t_critical <- qt(sig, n-1))
(theta_critical <- t_critical*se + theta_0)
(t <- (true_theta-theta_critical)/se)
(beta <- pt(t, n-1))*100
```

At the 1% significance level, $t_{crit} = F_t^{-1}(`r sig`, `r n-1`) = `r formatC(t_critical, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`$, which corresponding $\theta$ value can be calculated from:

$$t_{crit} = \cfrac{\theta_{crit}-\theta_0}{se}$$

Therefore, $\theta_{crit} = t_{crit}\cdot se + \theta_0 = `r formatC(t_critical, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)` \cdot `r formatC(se, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)` + `r theta_0` = `r formatC(theta_critical, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`$. And

$$\beta = \Pr(\text{failing to reject }H_0|\bar{H_0}) = \Pr\left(\cfrac{\theta^*\theta_{crit}}{se}\right) = F_t\left(\cfrac{\theta^*\theta_{crit}}{se}, n-1\right) = `r formatC(beta, format = "f", digits = 4, big.mark=",", drop0trailing = FALSE)`$$

**\textcolor{red}{The probability of type II error is about $\mathbf{`r formatC(beta, format = "f", digits = 2, big.mark=",", drop0trailing = FALSE)`}$.}**

5. **Define the power of the test (in general terms).**

The power of a test is the probability that we correctly reject the null hypothesis when it is false. Hence its probability is 1 minus the probability of type II error.

6. **Calculate the power of this test, again assuming the "true" population proportion is $\theta^*= 0.45$.**

$\mathbf{\textcolor{red}{Power = 1 - \beta \approx 0.40}}$


---
title: "**W271**-2 -- Spring 2016 -- **HW 4**"
author: "***Juanjo Carin, Kevin Davis, Ashley Levato, Minghu Song***"
date: "*February 24, 2016*"
output:
   pdf_document:
     fig_caption: yes
     toc: yes
numbersections: false
geometry: margin=1in
options: width=30
fontsize: 10pt
linkcolor: blue
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[LO,LE]{Carin, Davis, Levato, Song}
- \fancyhead[CE,CO]{W271 -- HW2 -- \leftmark}
- \fancyhead[RE,RO]{\rightmark}
- \fancyfoot[LO,LE]{UC Berkeley -- MIDS}
- \fancyfoot[CO,CE]{Spring semester 2016}
- \fancyfoot[RE,RO]{\thepage}
- \renewcommand{\headrulewidth}{0.5pt}
- \renewcommand{\footrulewidth}{0.5pt}
---
  
**********



# Data

**The file `athletics.RData` contains a two-year panel of data on 59 universities. Some variables relate to admissions, while others related to atheletic performance. You will use this dataset to investigate whether athletic success causes more students to apply to a university.**

**This data was made available by Wooldridge, and collected by Patrick Tulloch, then an economics student at MSU. It may have been further modified to test your proficiency. Sources are as follows:**

- **Peterson’s Guide to Four Year Colleges, 1994 and 1995 (24th and 25th editions). Princeton University Press. Princeton, NJ.**
- **The Official 1995 College Basketball Records Book, 1994, NCAA.**
- **1995 Information Please Sports Almanac (6th edition). Houghton Mifflin. New York, NY.**



**********

\pagebreak

# Exercises

```{r, echo = FALSE}
require(knitr, quietly = TRUE)
read_chunk('code/W271_HW4_Carin-Davis-Levato-Song.R')
opts_chunk$set(message = FALSE, warning = FALSE)
opts_chunk$set(fig.width = 4, fig.height = 3)
# Set path to data here (don't use setwd() inside a chunk!!!)
opts_knit$set(root.dir = './data')
```

```{r Libraries-Functions-Constants, echo = FALSE}
```



## Question 1

**Examine and summarize the dataset. Note that the actual data is found in the data object, while descriptions can be found in the `desc` object. How many observations and variables are there?**

**Examine the variables of key interest: `apps` represents the number of applications for admission. `bowl`, `btitle`, and `finfour` are indicators of athletic success. The three athletic performance variables are all lagged by one year. Intuitively, this is because we expect a school’s athletic success in the previous year to affect how many applications it recieves in the current year.**
  
```{r Question1-1, echo = -c(1:6)}
```

\pagebreak

```{r Question1-1bis, echo = TRUE}
```

There are `r dim(data)[1]` observations of `r dim(data)[2]` variables (which correspond to `r dim(data)[1]/2` schools over two years, `r unique(data$year)[1]` and `r unique(data$year)[2]`.

There are `r sum(unlist(lapply(data, function(x) sum(is.na(x)))))` `NA`s in the whole dataset, distributed as shown below:

```{r Question1-2, echo = -c(1:2)}
```

None of the variables of interest contains missing values (so we don't have to omit any observation).

For the rest of this assignment we'll focus on the 4 variables of interest mentioned above (and some others built from those ones), plus the names of the schools and the year (19922 or 1993).

> There's no need to keep `lapps` since it's just the log of `apps` and we will only use its change from 1992 to 1993 (so we will have to apply a transformation anyway: $\log(apps.1993)-\log(apps.1992)$ instead of $lapps.1993 - lapps.1992$; the results may be slightly different---and more precise---because of the rounding decimal error at storing `lapps`).

```{r Question1-3, echo = TRUE}
```

\pagebreak

```{r Question1-4, echo = FALSE}
```

\label{table-Q1}

```{r Question1-5, echo = FALSE, fig.width = 6, fig.height = 4.5, fig.cap = "Histogram of applications for admission per year"}
```

\label{hist-Q1}

As shown above, in Figure 1, the sample distribution of `apps` is right-skewed (i.e., its skewness is positive skewed) and platykurtic (its excess kurtosis is negative, so it has thinner tails than the normal distribution). Both things happen each year, as well as to the average number of applications

The figures in the next three pages show the number of observations depending on the three athletic success indicators for each of the two years under study, as well as the mean number of applications for admission is different whether if the school won the bowl game, the men's conference championship, or the final four in the previous year, but that difference is never significant (see how the confidence intervals always overlap in Figures 2, 4, and 6).

```{r Question1-6, echo = FALSE, fig.cap = "Number of schools depending on bowl game in the previous year"}
```

\label{barchart-Q1-bowl-1}

```{r Question1-7, echo = FALSE, fig.cap = "Bar chart of the mean number of applications depending on bowl game in the previous year"}
```

\label{barchart-Q1-bowl-2}

```{r Question1-8, echo = FALSE, fig.cap = "Number of schools depending on men's conference championship in the previous year"}
```

\label{barchart-Q1-btitle-1}

```{r Question1-9, echo = FALSE, fig.cap = "Bar chart of the mean number of applications depending on men's conference championship in the previous year"}
```

\label{barchart-Q1-btitle-2}

```{r Question1-10, echo = FALSE, fig.cap = "Number of schools depending on men's final four in the previous year"}
```

\label{barchart-Q1-finfour-1}

```{r Question1-11, echo = FALSE, fig.cap = "Bar chart of the mean number of applications depending on men's final four in the previous year"}
```

\label{barchart-Q1-finfour-2}



**********

\pagebreak

## Question 2

**Note that the dataset is in long format, with a separate row for each year for each school. To prepare for a difference-in-difference analysis, transfer the dataset to wide-format. Each school should have a single row of data, with separate variables for 1992 and 1993. For example, you should have an `apps.1992` variable and an `apps.1993` variable to record the number of applications in either year.**

```{r Question2-1, echo = -c(1:9)}
```

\pagebreak

**Create a new variable, `clapps` to represent the change in the log of the number of applications from 1992 to 1993. Examine this variable and its distribution.**

```{r Question2-2, echo = TRUE}
```

**Which schools had the greatest increase and the greatest decrease in number of log applications?**

```{r Question2-3, echo = -4}
```

\label{table-Q2-1}

```{r Question2-4, echo = -c(4:7)}
```

\label{table-Q2-2}

How does the sample distribution of this change in the log of applications for admission per year look like? As it happened if the log is not applied (see Figure 1), much more normal than the sample distribution of the log of applications each year:

```{r Question2-5, echo = FALSE, fig.width = 6, fig.height = 4.5, fig.cap = "Histogram of log of applications for admission per year"}
```

\label{hist-Q2}



**********

\pagebreak

## Question 3

**Similarly to above, create three variables, `cperf`, `cbball`, and `cbowl` to represent the changes in the three athletic success variables. Since these variables are lagged by one year, you are actually computing the change in athletic success from 1991 to 1992.**

```{r Question3-1, echo = -c(1:4)}
```

**Which of these variables has the highest variance?**

First, let's see how many of the `r dim(data3)[1]` schools in the sample won each title each year:

```{r Question3-2, echo = TRUE}
```

Of course, this does not tell us anything about the variance: the same `r sum(data3$bowl.1992)` schools that won the bowl game in 1991 could have won it again in 1992.

```{r Question3-3, echo = -1}
```

As shown above, **`r names(v[which(v == max(v))])`** is the variable with the highest variance.



**********

\pagebreak

## Question 4

**We are interested in a population model,**

$$\mathbf{lapps_i = \delta_0 + \beta_0I_{1993} + \beta_1bowl_i + \beta_2btitle_i + \beta_3finfour_i + a_i + u_{it}}$$

**Here, $I_{1993}$ is an indicator variable for the year 1993. $a_i$ is the time-constant effect of school $i$. $u_{it}$ is the idiosyncratic effect of school $i$ at time $t$. The athletic success indicators are all lagged by one year as discussed above.**

**At this point, we assume that (1) all data points are independent random draws from this population model (2) there is no perfect multicollinearity (3) $E(a_i) = E(u_{it}) = 0$.**

**You will estimate the first-difference equation,**

$$\mathbf{clapps_i = \beta_0 + \beta_1cbowl+i + \beta_2cbtitle_i + \beta_3cfinfour_i + a_i + cu_i}$$

**where $cu_i = u_{i1993} - u_{i1992}$ is the change in the idiosyncratic term from 1992 to 1993.**

First of all, we'll change the names of `cperf`, `cfinfour`, and `cbball` (as defined in [Question 3](#question-3)) to follow the notation in the formula above.

```{r Question4, echo = -c(1:2)}
```

a) **What additional assumption is needed for this population model to be causal? Write this in mathematical notation and also explain it intuitively in English.**

Causality is about whether manipulations to the independent variable influence the dependent variable but not the error term. For a model to be causal, we need to be able to introduce a manipulation in $x$, $dx$, that (we expect) will cause a change in $y$, $dy$, while the error term $u$ (that includes both the idiosyncratic error and the individual time-constant or fixed effect) needs to stay unchanged as we manipulate $x$. I.e., as long as

$$\frac{\partial u}{\partial x} = 0$$

we can claim that the effect of $x$ is

$$\frac{\partial y}{\partial x} = \beta_1.$$


b) **What additional assumption is needed for OLS to consistently estimate the first-difference model? Write this in mathematical notation and also explain it intuitively in English. Comment on whether this assumption is plausible in this setting.**

The assumptions for OLS to consistently estimate the OLS parameters are:

1. (the model is) linear in parameters
2. random sampling
3. no perfect collinearity (among the independent variables)
3. zero mean (of the errors) and zero correlation (with any of the independent variables)

The assumption that we have not made yet is the second part of the fourth one: that the errors (or unobserved factors) are uncorrelated with the (3) independent variables (the **exogeneity** assumption, together with $E(a_i) = E(u_{it}) = 0$). In a difference-in-difference model this means:

$$Cov(\Delta u, \Delta x_j) = Cov(cu, cx_j) = 0 \ \forall j=1,2,\dots,k$$

I.e., no individual-specific factors are changing over time, depending on the change of the independent variables. This may not be plausible in this setting: for example, winning one of those competitions may increase the income of a school, and that additional income might be spent on better professors or facilities, that would make the school more attractive and most likely for potential students to apply for admission. Or the other way around: an individual-specific factor that may cause the change in athletic success as well as more applications for admissions could have changed; for example, the school may have hired a highly reputed basketball coach, which can increase the number of applications for admission as well as the chances to win the title.



**********

\pagebreak

## Question 5

**Estimate the first-difference model given above. Using the best practices descibed in class, interpret the slope coefficients and comment on their statistical significance and practical significance.**

```{r Question5-1, echo = 5, results = 'asis'}
```

\label{table-Q5}

The standard errors shown in the Table above are **heteroskedasticity-robust**. This is always a good practice . . . and it's mandatory in this case because the homoskedasticity assumption does not hold, as the diagnostic plots in Figure 9 (see next page) suggest.

> The normality assumption---keep in mind that our sample size is quite small: `r dim(data3)[1]` observations---and especially the zero conditional mean assumption are also broken.

\pagebreak

```{r Question5-2, echo = FALSE, fig.width = 5, fig.height = 3.75, fig.cap = "Diagnostic plots of the regression model"}
```

\label{regPlots-Q5}

Since we are using the (change in) log of our dependent variable and the independent variables are binary, each coefficient can be understood as the estimated average effect of a change in those independent variables on the percentage change of the independent variable (`apps`; provided that the coefficient is not too high).

+ The intercept here is the average difference between 1993 and 1992: roughly `r frmt(coeftest(model1, vcov = vcovHC)[1, 1]*100, digits = 2)` percentage points more in the number of applications for admission. That effect (of the change in year) is neither statistically nor practically significant. 

\small

> > Let's prove that simplifying it a bit: considering all cases, not only the ones corresponding to the baseline case (no change in any of the athletic success indicators):

> > ```{r Question5-3, echo = TRUE}
> > ```

\normalsize

+ The effect of the change in `bowl` (the only one statistically significant, $p = `r frmt(coeftest(model1, vcov = vcovHC)[2, 4])`$) means that winning the bowl game after not having won it the previous year increases the number of applications for admission the following year, on average, by roughly `r frmt(coeftest(model1, vcov = vcovHC)[2, 1]*100, digits = 2)` percentage points. This is also practically significant: almost `r round(mean(data3$apps.1992) * coeftest(model1, vcov = vcovHC)[2, 1] / 25) * 25` more applications on the average school.

+ The effect of the change in `btitle` means that winning the men's conference championship after not having won it the previous year increases the number of applications for admission the following year, on average, by roughly `r frmt(coeftest(model1, vcov = vcovHC)[3, 1]*100, digits = 2)` percentage points. This effect is practically significant (about `r round(mean(data3$apps.1992) * coeftest(model1, vcov = vcovHC)[3, 1])` more applications on the average school), but not statistically significant.

+ The effect of the change in `finfour` is the largest one in absolute value (though not statistically significant), but it's negative (and hence counterintuitive): it means that winning the men's final four after not having won it the previous year decreases the number of applications for admission the following year, on average, by roughly `r frmt(abs(coeftest(model1, vcov = vcovHC)[4, 1])*100, digits = 2)` percentage points. This would have a high practical significance: about `r round(mean(data3$apps.1992) * abs(coeftest(model1, vcov = vcovHC)[4, 1]))` fewer applications on the average school. Similarly---this same reasoning could be applied to the other variables of interest---, not winning the men's final four after having won it the previous year would increase the number of applications for admission the following year, on average, by roughly the same quantity, `r frmt(abs(coeftest(model1, vcov = vcovHC)[4, 1])*100, digits = 2)` percentage points.

We should note that those two last variables (`cbtitle` and `cfinfour`) usually take a value of zero (i.e., `btitle` and `finfour` do not vary too much between 1992 and 1993: only $`r frmt(var((data3$cbtitle))*100, digits =2)`\%$ and $`r frmt(var((data3$cfinfour))*100, digits =2)`\%$ of the observations, respectively, change from one year to the next; see the answer to [Question 3](#question-3)) so it would be harder to find a statistically significant effect (if it existed).

```{r}
data3$cbtitle; sum(abs(data3$cbtitle))
data3$cfinfour; sum(abs(data3$cfinfour))
```

\small

> > Of course, if we don't use the difference-in-difference model (and include the time-constant effect of school $i$, i.e., `school`) the values of the 4 coefficients are the same (but less precise---wider confidence intervals---; in this case not even `bowl` is significant):

> > ```{r Question5-4, echo = TRUE}
> > ```

\normalsize



**********

\pagebreak

## Question 6

**Test the joint signifance of the three indicator variables. This is the test of the overall model. What impact does the result have on your conclusions?**
  
```{r Question6-1, echo = -c(1:3)}
```

The *F* statistic for overall significance of the regression model is $`r frmt(waldtest(model1, vcov = vcovHC)$F[2])` \ (`r -waldtest(model1, vcov = vcovHC)$Df[2]`, `r waldtest(model1, vcov = vcovHC)$Res.Df[1]`)$ (as already shown in Table 4). Its *p* value is `r frmt(waldtest(model1, vcov = vcovHC)[[4]][2])`, so it's not significant at any acceptable level. Hence, the model does not help to explain the variance in the change in the log of applications; in other words, **athletic success seems unrelated to more students applying to a university**.

Had we not used heteroskedasticity-robust standard errors, the *F* statistic would have been significant at the `r cut(waldtest(model1)[[4]][2], breaks = c(0, 0.001, 0.01, 0.05, 0.1, 1), labels = c('0.001', '0.001', '0.05', '0.1', '1'))` level.

```{r Question6-2, echo = TRUE}
```

But as we demonstrated in [Question 5](#question-5), based on Figure 9, not using robust standard errors would not allow us to assume *t* and *F* distributions of the *t* and *F* statistics, so that signifcance level is meaningless in this case.

**********

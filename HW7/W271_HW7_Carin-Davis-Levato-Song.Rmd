---
title: "**W271**-2 -- Spring 2016 -- **HW 7**"
author: "***Juanjo Carin, Kevin Davis, Ashley Levato, Minghu Song***"
date: "*March 30, 2016*"
output:
   pdf_document:
     fig_caption: yes
     toc: yes
numbersections: false
geometry: margin=1in
options: width=30
fontsize: 10pt
linkcolor: blue
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[LO,LE]{Carin, Davis, Levato, Song}
- \fancyhead[CE,CO]{W271 -- HW 7 -- \leftmark}
- \fancyhead[RE,RO]{\rightmark}
- \fancyfoot[LO,LE]{UC Berkeley -- MIDS}
- \fancyfoot[CO,CE]{Spring semester 2016}
- \fancyfoot[RE,RO]{\thepage}
- \renewcommand{\headrulewidth}{0.5pt}
- \renewcommand{\footrulewidth}{0.5pt}
---

**********

\pagebreak

# Exercises

```{r, echo = FALSE, warning = FALSE}
require(knitr, quietly = TRUE)
read_chunk('code/W271_HW7_Carin-Davis-Levato-Song.R')
opts_chunk$set(message = FALSE, warning = FALSE)
opts_chunk$set(fig.width = 4, fig.height = 3)
# Set path to data here (don't use setwd() inside a chunk!!!)
opts_knit$set(root.dir = './data')
```

```{r Libraries-Functions-Constants, echo = FALSE}
```

## Question 1

1.1. **Load `hw07_series1.csv`.**

```{r Question1-1, echo = -c(1:3)}
```


**********


1.2. **Describe the basic structure of the data and provide summary statistics of the series.**

```{r Question1-2, echo = -c(1:2)}
```

The data correspond to `r dim(hw07)[1]` observations of a single variable. No information about the time scale is given, so we'll just use an index between 1 and `r dim(hw07)[1]` (with `frequency = 1`). The main descriptive statistics are shonw in the Table above.


**********

\pagebreak


1.3. **Plot histogram and time-series plot of the series. Describe the patterns exhibited in histogram and time-series plot. For time series analysis, is it sufficient to use only histogram to describe a series?**

```{r Question1-3-1, echo = 3, fig.width=6, fig.height=4.5, fig.cap = "Histogram of the data"}
```

Let's get some descriptive statistics about the distribution of the data, that complement the histogram above:

```{r Question1-3-2, echo = FALSE}
```

The series has an *excess kurtosis* (kurtosis minus 3, the value for a normal distribution) which is negative, which indicates a platykurtic distribution (thinner tails than a normal one). But that excess kurtosis is close to zero, and not statistically significant (the parameter `kurt.2SE` would have to be greater than 1; see the [documentation of `stat.desc`](http://127.0.0.1:13524/library/pastecs/html/stat.desc.html) for more information). Likewise, the *skewness* is positive, which indicates a right-skewed distribution (with a heavy right tail), but again is not far from zero and not statistically significantly different than it. A Shapiro-Wilk test is also non-significant ($p = `r more_desc_stat[which(rownames(more_desc_stat) == 'normtest.p')]`$), so we cannot reject the hypothesis that the distribution of the series is normal (though its size---`r dim(hw07)[1]`---is a bit reduced).

For time series analysis (unlike cross-sectional data analysis), the histogram alone is not enough to describe a series because it tells us nothing about the dynamics of it; e.g., this one in particular lets us know that there was one time period where the value of the time series was below 10, but not when that happened (in the 4th time period, as shown in the next Figure).

```{r Question1-3-3, echo = FALSE, fig.width=6, fig.height=4.5, fig.cap = "Time-series plot of the data"}
```

The time-series plot has been plotted together with the mean value and a 9-point symmetric moving average, that exhibits an increasing trend during the first 30 time periods or so, then a small decline for about 10 time periods, another increasing trend for a few time periods, and finally a decreasing trend during the last 30 time periods or so (this one not so linear; the decrease is steeper at the beginning, mainly due to the effect of a pike in observations 44 and 45); the smoother yields no values for the first and last 4 observations, but it seems the trend becomes positive again at the end of the observation period. Overall, we can say the time series is persistent.


**********

\pagebreak


1.4. **Plot the ACF and PACF of the series. Describe the patterns exhibited in the ACF and PACF.**

```{r Question1-4, echo = FALSE, fig.width=6, fig.height=4.5, fig.cap = "Autocorrelation and partial autocorrelation graphs"}
```

The correlogram has a wave-like shape that resembles that of a shrinking cosine function (typical of an AR(2) process). It decreases relatively slowly (the first autocorrelation not statistically significantly different from zero corresponds to the 9th lag). As for the PACF, it drops off relatively abruptly at lag 2 (another indication of an AR(2) process).


**********


1.5. **Estimate the series using the `ar()` function.**

```{r Question1-5, echo = c(2:4, 6:9)}
```


**********


1.6. **Report the estimated AR parameters, the order of the model, and standard errors.**

Order of the model:

```{r Question1-6-1, echo = -1}
```

Estimated AR parameters:

```{r Question1-6-2}
```

Other parameters of the estimated AR model:

```{r Question1-6-3}
```

As shown in the output of the code above, models of other orders (especially the one of order 3) have similar AIC values. The last parameter shown above is the asymptotic covariance matrix of the coefficient estimates, so the square root of the elements in the diagonal of that matrix are the standard errors. Together with them, we can also estimate the confidence interval of the coefficient estimates:

```{r Question1-6-4, echo = 1}
```

Both coefficients are significant (the CI does not include zero in both cases).

> It should be noticed that `arima()` returns slightly different parameter estimates and SEs (which now are not exactly the same for both lags), but the differences with `ar()` are pretty small.

> ```{r Question1-6-5, echo = c(1:2)}
> ```


**********
**********

\pagebreak



## Question 2

2.1. **Simulate a time series of lenght 100 for the following model. Name the series $x$.**

$$\mathbf{x_t = \frac{5}{6} x_{t-1} - \frac{1}{6} x_{t-2} + \omega_t}$$

This is an AR(2) model with coefficients $5/6 = `r frmt(5/6)`$ and $-1/6 = `r frmt(-1/6)`$ respectively.

```{r Question2-1, echo = c(4:5), fig.width = 6, fig.height = 6, fig.cap = "Histogram and time-series plot of the simulated time series"}
```

> (The Figure above have the same legends than Figures 1 and 2, Question 1.)


**********

\pagebreak


2.2. **Plot the correlogram and partial correlogram for the simulated series. Comments on the plots.**

```{r Question2-2, echo = FALSE, fig.width=6, fig.height=4.5, fig.cap = "Autocorrelation and partial autocorrelation graphs"}
```

As in Question 1, the correlogram has a wave-like shape (that slightly resembles that of a shrinking cosine function). It decreases relatively quickly (though this changes in other simulations---if other seed is used).

The PACF drops off relatively abruptly at lag 1 or 2, depending on the simulation. For this particular simulation the partial autocorrelation for lag 2 is on the verge of being significantly different from zero; in other simulations it is not significantly different from zero. This is partly due to the limited size of the simulated series: if $\rho_k = 0$, its 95% confidence interval is $-\frac{1}{n} \pm \frac{2}{\sqrt{n}}$; for $n=100$, that is $[`r frmt(-1/100 - 2/sqrt(100))`, `r frmt(-1/100 + 2/sqrt(100))`]$... which includes the "true" model coefficient for lag 2 ($-1/6 = `r frmt(-1/6)`$).


**********

\pagebreak


2.3. **Estimate an AR model for this simulated series. Report the estimated AR parameters, standard errors, and the order of the AR model.**

An AR model of order 2 is estimated, with coefficients quite close to the "true" ones (`r frmt(5/6)` and `r frmt(-1/6)`). The result depends on the series that we simulated (i.e., on the seed that we used to generate the simulated series): for many other seeds an AR(1) model is estimated.

```{r Question2-3, echo = c(4:10)}
```


**********


2.4. **Construct a 95% confidence intervals for the parameter estimates of the estimated model. Do the "true" model parameters fall within the confidence intervals? Explain the 95% confidence intervals in this context.**

In the answer to the previous question we already estimated the SE of the parameter estimates.

```{r Question2-4, echo = 3}
```

Based on Table 5 above (see the last two columns), the "true" model parameters (`r frmt(5/6,4)` and `r frmt(-1/6,4)`) fall within the confidence intervals.

As we explained in **2.3**, what happens in many of the simulations (when using a different seed) is that the best fitted model is an AR(1) so there is no estimate for the coefficient for lag 2). In any case, if we repeat the simulation over and over, the "true" model parameters will fall within the CIs about 95% of the time (at least the coefficient of lag 1; and also the coefficient of lag 2 if we "force" our fitted model to be AR(2)[^note]).

[^note]: \ \ \ That would have to be done using the `arima()` function rather than `ar()`: `arima(x, order = c(2,0,0))`.

> Again, we may notice that `arima()` returns slightly different parameter estimates and SEs (which now are not exactly the same for both lags), but the differences with `ar()` are pretty small (and hence the "true" model parameters fall within the confidence intervals).

> ```{r Question2-4-2, echo = c(1:2)}
> ```

**********

\pagebreak


2.5. **Is the estimated model stationary or non-stationary?**

For an AR model ($x_t$ depends only on white noise and previous values of it) to be stationary, *all* the roots of its *characteristic equation* $\Phi(B) = 0$ must exceed unity in absolute value (i.e., they must be outside the unit circle). In this case, $\Phi(B) = 1 - `r frmt(x.arfit$ar[1], 4)`B - `r frmt(x.arfit$ar[2], 4)`B^2$ (rather than $\Phi(B) = 1 - `r frmt(5/6, 4)`B + `r frmt(1/6, 4)`B^2$, because we are asked about the estimated model, not the "true" one), so let's check the roots of the characteristic equation:

```{r Question2-5}
```

All the roots are outside the unit circle, so the estimated model (as well as the "true" model) is stationary.

> Actually, this check is not even necessary: had the estimated model not been stationary, the `ar()` function would have returned an error.

**********

\pagebreak


2.6. **Plot the correlogram of the residuals of the estimated model. Comment on the plot.**

```{r Question2-6, echo = FALSE, fig.cap = "Autocorrelation graph of the residuals of the estimated model"}
```

None of the autocorrelations (with the exception of $\rho_0 = 1$, of course) is statistically significantly different from zero, which indicates that the residuals could be white noise (at least we have no evidence against that hypothesis). A Q-Q plot indicates that the distributions of the residuals is close to normal, which strengthens the idea that the residuals might be (gaussian) white noise, and indicates that the estimated model (an AR(2) stochastic process) is---as expected, because we know how the data were generated---a good fit for the series.

```{r Question2-6-2, echo = FALSE, fig.cap = "Normal Q-Q plot of the residuals of the estimated model"}
```

**********
**********
